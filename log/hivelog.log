Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:19:01 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:19:01 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:19:01 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
18/05/29 10:19:01 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:19:02 INFO manager.SqlManager: Executing SQL statement: select * from sign_project where  (1 = 0) 
18/05/29 10:19:02 INFO manager.SqlManager: Executing SQL statement: select * from sign_project where  (1 = 0) 
18/05/29 10:19:02 INFO manager.SqlManager: Executing SQL statement: select * from sign_project where  (1 = 0) 
18/05/29 10:19:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/7e874794d7eae4d3faa240dde57822e8/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:19:05 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/7e874794d7eae4d3faa240dde57822e8/QueryResult.jar
18/05/29 10:19:07 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/sign_project/2018-05-28 deleted.
18/05/29 10:19:07 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:19:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:19:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:19:07 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:19:09 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:19:09 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:19:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6750
18/05/29 10:19:10 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6750
18/05/29 10:19:10 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6750/
18/05/29 10:19:10 INFO mapreduce.Job: Running job: job_1525741534203_6750
18/05/29 10:19:18 INFO mapreduce.Job: Job job_1525741534203_6750 running in uber mode : false
18/05/29 10:19:18 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:19:25 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:19:25 INFO mapreduce.Job: Job job_1525741534203_6750 completed successfully
18/05/29 10:19:25 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153051
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2788
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9678
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4839
		Total vcore-milliseconds taken by all map tasks=4839
		Total megabyte-milliseconds taken by all map tasks=9910272
	Map-Reduce Framework
		Map input records=20
		Map output records=20
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=24
		CPU time spent (ms)=1680
		Physical memory (bytes) snapshot=259338240
		Virtual memory (bytes) snapshot=2942468096
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2788
18/05/29 10:19:25 INFO mapreduce.ImportJobBase: Transferred 2.7227 KB in 18.5149 seconds (150.5812 bytes/sec)
18/05/29 10:19:25 INFO mapreduce.ImportJobBase: Retrieved 20 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.547 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.831 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.715 seconds
OK
Time taken: 0.673 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:19:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:19:58 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:19:59 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
18/05/29 10:19:59 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:19:59 INFO manager.SqlManager: Executing SQL statement: select * from sign_evidence where create_time <= '2018-05-28 23:59:59' and create_time >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:19:59 INFO manager.SqlManager: Executing SQL statement: select * from sign_evidence where create_time <= '2018-05-28 23:59:59' and create_time >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:20:00 INFO manager.SqlManager: Executing SQL statement: select * from sign_evidence where create_time <= '2018-05-28 23:59:59' and create_time >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:20:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/f9ccbfdae6cc60fa4b04acb1de4dd787/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:20:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/f9ccbfdae6cc60fa4b04acb1de4dd787/QueryResult.jar
18/05/29 10:20:04 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/sign_evidence/2018-05-28 deleted.
18/05/29 10:20:04 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:20:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:20:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:20:04 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:20:07 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:20:07 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:20:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6751
18/05/29 10:20:07 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6751
18/05/29 10:20:07 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6751/
18/05/29 10:20:07 INFO mapreduce.Job: Running job: job_1525741534203_6751
18/05/29 10:20:14 INFO mapreduce.Job: Job job_1525741534203_6751 running in uber mode : false
18/05/29 10:20:14 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:20:32 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:20:32 INFO mapreduce.Job: Job job_1525741534203_6751 completed successfully
18/05/29 10:20:32 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153231
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=219341791
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=29118
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=14559
		Total vcore-milliseconds taken by all map tasks=14559
		Total megabyte-milliseconds taken by all map tasks=29816832
	Map-Reduce Framework
		Map input records=90652
		Map output records=90652
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=127
		CPU time spent (ms)=15850
		Physical memory (bytes) snapshot=789065728
		Virtual memory (bytes) snapshot=2938724352
		Total committed heap usage (bytes)=985137152
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=219341791
18/05/29 10:20:32 INFO mapreduce.ImportJobBase: Transferred 209.1806 MB in 27.4494 seconds (7.6206 MB/sec)
18/05/29 10:20:32 INFO mapreduce.ImportJobBase: Retrieved 90652 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.5 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.838 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.595 seconds
OK
Time taken: 0.593 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:21:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:21:05 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:21:05 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:21:05 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:21:05 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:21:06 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:21:06 INFO manager.SqlManager: Executing SQL statement: select * from CONFIG_APP where  (1 = 0) 
18/05/29 10:21:06 INFO manager.SqlManager: Executing SQL statement: select * from CONFIG_APP where  (1 = 0) 
18/05/29 10:21:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/97edb9dfdae1c51dca1fa406b22665bf/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:21:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/97edb9dfdae1c51dca1fa406b22665bf/QueryResult.jar
18/05/29 10:21:11 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/CONFIG_APP/2018-05-28 deleted.
18/05/29 10:21:11 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:21:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:21:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:21:12 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:21:14 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:21:14 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:21:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6752
18/05/29 10:21:15 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6752
18/05/29 10:21:15 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6752/
18/05/29 10:21:15 INFO mapreduce.Job: Running job: job_1525741534203_6752
18/05/29 10:21:23 INFO mapreduce.Job: Job job_1525741534203_6752 running in uber mode : false
18/05/29 10:21:23 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:21:30 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:21:30 INFO mapreduce.Job: Job job_1525741534203_6752 completed successfully
18/05/29 10:21:30 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153036
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=23656
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9840
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4920
		Total vcore-milliseconds taken by all map tasks=4920
		Total megabyte-milliseconds taken by all map tasks=10076160
	Map-Reduce Framework
		Map input records=129
		Map output records=129
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=43
		CPU time spent (ms)=1770
		Physical memory (bytes) snapshot=258023424
		Virtual memory (bytes) snapshot=2945130496
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=23656
18/05/29 10:21:30 INFO mapreduce.ImportJobBase: Transferred 23.1016 KB in 18.8632 seconds (1.2247 KB/sec)
18/05/29 10:21:30 INFO mapreduce.ImportJobBase: Retrieved 129 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.641 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.869 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.566 seconds
OK
Time taken: 0.527 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:22:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:22:03 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:22:04 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:22:04 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:22:04 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:22:05 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:22:05 INFO manager.SqlManager: Executing SQL statement: select * from SYS_OFFICE where  (1 = 0) 
18/05/29 10:22:05 INFO manager.SqlManager: Executing SQL statement: select * from SYS_OFFICE where  (1 = 0) 
18/05/29 10:22:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/1087c3bf120bacfc7afe594a81bb0e9f/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:22:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1087c3bf120bacfc7afe594a81bb0e9f/QueryResult.jar
18/05/29 10:22:10 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/SYS_OFFICE/2018-05-28 deleted.
18/05/29 10:22:10 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:22:10 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:22:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:22:10 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:22:12 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:22:12 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:22:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6753
18/05/29 10:22:13 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6753
18/05/29 10:22:13 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6753/
18/05/29 10:22:13 INFO mapreduce.Job: Running job: job_1525741534203_6753
18/05/29 10:22:21 INFO mapreduce.Job: Job job_1525741534203_6753 running in uber mode : false
18/05/29 10:22:21 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:22:27 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:22:28 INFO mapreduce.Job: Job job_1525741534203_6753 completed successfully
18/05/29 10:22:29 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153036
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7602
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9710
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4855
		Total vcore-milliseconds taken by all map tasks=4855
		Total megabyte-milliseconds taken by all map tasks=9943040
	Map-Reduce Framework
		Map input records=47
		Map output records=47
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=44
		CPU time spent (ms)=1640
		Physical memory (bytes) snapshot=261009408
		Virtual memory (bytes) snapshot=2928054272
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7602
18/05/29 10:22:29 INFO mapreduce.ImportJobBase: Transferred 7.4238 KB in 18.7685 seconds (405.0398 bytes/sec)
18/05/29 10:22:29 INFO mapreduce.ImportJobBase: Retrieved 47 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.523 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.83 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.538 seconds
OK
Time taken: 0.587 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:23:02 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:23:02 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:23:02 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:23:02 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:23:02 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:23:03 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:23:03 INFO manager.SqlManager: Executing SQL statement: select * from SYS_USER where  (1 = 0) 
18/05/29 10:23:03 INFO manager.SqlManager: Executing SQL statement: select * from SYS_USER where  (1 = 0) 
18/05/29 10:23:03 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/41e42b42b57137e1a0708cfa6b302685/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:23:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/41e42b42b57137e1a0708cfa6b302685/QueryResult.jar
18/05/29 10:23:08 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/SYS_USER/2018-05-28 deleted.
18/05/29 10:23:08 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:23:08 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:23:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:23:08 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:23:10 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:23:10 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:23:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6754
18/05/29 10:23:11 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6754
18/05/29 10:23:11 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6754/
18/05/29 10:23:11 INFO mapreduce.Job: Running job: job_1525741534203_6754
18/05/29 10:23:18 INFO mapreduce.Job: Job job_1525741534203_6754 running in uber mode : false
18/05/29 10:23:18 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:23:25 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:23:25 INFO mapreduce.Job: Job job_1525741534203_6754 completed successfully
18/05/29 10:23:26 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153028
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=36238
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8964
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4482
		Total vcore-milliseconds taken by all map tasks=4482
		Total megabyte-milliseconds taken by all map tasks=9179136
	Map-Reduce Framework
		Map input records=121
		Map output records=121
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=53
		CPU time spent (ms)=1960
		Physical memory (bytes) snapshot=263413760
		Virtual memory (bytes) snapshot=2947084288
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=36238
18/05/29 10:23:26 INFO mapreduce.ImportJobBase: Transferred 35.3887 KB in 17.3473 seconds (2.04 KB/sec)
18/05/29 10:23:26 INFO mapreduce.ImportJobBase: Retrieved 121 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.57 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.963 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.739 seconds
OK
Time taken: 0.518 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:23:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:23:59 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:23:59 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:23:59 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:23:59 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:24:00 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:24:00 INFO manager.SqlManager: Executing SQL statement: select * from WORK_CERT_INFO where  (1 = 0) 
18/05/29 10:24:00 INFO manager.SqlManager: Executing SQL statement: select * from WORK_CERT_INFO where  (1 = 0) 
18/05/29 10:24:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/b303f61b8741f59bc6a117d676150ec8/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:24:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/b303f61b8741f59bc6a117d676150ec8/QueryResult.jar
18/05/29 10:24:06 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/WORK_CERT_INFO/2018-05-28 deleted.
18/05/29 10:24:06 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:24:06 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:24:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:24:06 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:24:08 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:24:08 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:24:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6755
18/05/29 10:24:09 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6755
18/05/29 10:24:09 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6755/
18/05/29 10:24:09 INFO mapreduce.Job: Running job: job_1525741534203_6755
18/05/29 10:24:17 INFO mapreduce.Job: Job job_1525741534203_6755 running in uber mode : false
18/05/29 10:24:17 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:32:13 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:32:14 INFO mapreduce.Job: Job job_1525741534203_6755 completed successfully
18/05/29 10:32:14 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153052
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2606789223
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=948532
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=474266
		Total vcore-milliseconds taken by all map tasks=474266
		Total megabyte-milliseconds taken by all map tasks=971296768
	Map-Reduce Framework
		Map input records=928916
		Map output records=928916
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=2252
		CPU time spent (ms)=167720
		Physical memory (bytes) snapshot=203112448
		Virtual memory (bytes) snapshot=2942504960
		Total committed heap usage (bytes)=385875968
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2606789223
18/05/29 10:32:14 INFO mapreduce.ImportJobBase: Transferred 2.4278 GB in 488.4407 seconds (5.0897 MB/sec)
18/05/29 10:32:14 INFO mapreduce.ImportJobBase: Retrieved 928916 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.349 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.962 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.702 seconds
OK
Time taken: 0.569 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:32:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:32:47 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:32:47 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:32:48 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:32:48 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:32:48 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:32:48 INFO manager.SqlManager: Executing SQL statement: select * from WORK_COMPANY where  (1 = 0) 
18/05/29 10:32:48 INFO manager.SqlManager: Executing SQL statement: select * from WORK_COMPANY where  (1 = 0) 
18/05/29 10:32:48 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/1f696d4143b93fa66d642ddf0628bf4d/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:32:52 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1f696d4143b93fa66d642ddf0628bf4d/QueryResult.jar
18/05/29 10:32:53 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/WORK_COMPANY/2018-05-28 deleted.
18/05/29 10:32:53 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:32:54 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:32:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:32:54 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:32:56 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:32:56 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:32:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6756
18/05/29 10:32:56 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6756
18/05/29 10:32:56 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6756/
18/05/29 10:32:56 INFO mapreduce.Job: Running job: job_1525741534203_6756
18/05/29 10:33:05 INFO mapreduce.Job: Job job_1525741534203_6756 running in uber mode : false
18/05/29 10:33:05 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:33:20 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:33:20 INFO mapreduce.Job: Job job_1525741534203_6756 completed successfully
18/05/29 10:33:20 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153044
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=89684192
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=26992
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=13496
		Total vcore-milliseconds taken by all map tasks=13496
		Total megabyte-milliseconds taken by all map tasks=27639808
	Map-Reduce Framework
		Map input records=374054
		Map output records=374054
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=170
		CPU time spent (ms)=15720
		Physical memory (bytes) snapshot=787427328
		Virtual memory (bytes) snapshot=2929098752
		Total committed heap usage (bytes)=763363328
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=89684192
18/05/29 10:33:20 INFO mapreduce.ImportJobBase: Transferred 85.5295 MB in 26.6433 seconds (3.2102 MB/sec)
18/05/29 10:33:20 INFO mapreduce.ImportJobBase: Retrieved 374054 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.546 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.029 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.505 seconds
OK
Time taken: 0.551 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:33:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:33:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:33:54 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:33:54 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:33:54 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:33:54 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:33:55 INFO manager.SqlManager: Executing SQL statement: select * from WORK_COMPANY_HIS where  (1 = 0) 
18/05/29 10:33:55 INFO manager.SqlManager: Executing SQL statement: select * from WORK_COMPANY_HIS where  (1 = 0) 
18/05/29 10:33:55 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/48da6d9dd2fcc7f6a10b92d2b9716fb1/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:33:58 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/48da6d9dd2fcc7f6a10b92d2b9716fb1/QueryResult.jar
18/05/29 10:34:00 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/WORK_COMPANY_HIS/2018-05-28 deleted.
18/05/29 10:34:00 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:34:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:34:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:34:00 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:34:02 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:34:02 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:34:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6757
18/05/29 10:34:03 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6757
18/05/29 10:34:03 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6757/
18/05/29 10:34:03 INFO mapreduce.Job: Running job: job_1525741534203_6757
18/05/29 10:34:11 INFO mapreduce.Job: Job job_1525741534203_6757 running in uber mode : false
18/05/29 10:34:11 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:34:38 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:34:38 INFO mapreduce.Job: Job job_1525741534203_6757 completed successfully
18/05/29 10:34:39 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153060
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=216726169
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=47626
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=23813
		Total vcore-milliseconds taken by all map tasks=23813
		Total megabyte-milliseconds taken by all map tasks=48769024
	Map-Reduce Framework
		Map input records=922008
		Map output records=922008
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=250
		CPU time spent (ms)=27700
		Physical memory (bytes) snapshot=534683648
		Virtual memory (bytes) snapshot=2948026368
		Total committed heap usage (bytes)=577765376
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=216726169
18/05/29 10:34:39 INFO mapreduce.ImportJobBase: Transferred 206.6862 MB in 38.4923 seconds (5.3695 MB/sec)
18/05/29 10:34:39 INFO mapreduce.ImportJobBase: Retrieved 922008 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.495 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.084 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.445 seconds
OK
Time taken: 0.662 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:35:11 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:35:11 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:35:12 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:35:12 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:35:12 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:35:13 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:35:13 INFO manager.SqlManager: Executing SQL statement: select * from WORK_USER where  (1 = 0) 
18/05/29 10:35:13 INFO manager.SqlManager: Executing SQL statement: select * from WORK_USER where  (1 = 0) 
18/05/29 10:35:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9911c208e1cece60dfe1e56bc466fdaf/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:35:16 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9911c208e1cece60dfe1e56bc466fdaf/QueryResult.jar
18/05/29 10:35:17 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/WORK_USER/2018-05-28 deleted.
18/05/29 10:35:17 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:35:17 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:35:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:35:18 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:35:20 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:35:20 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:35:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6758
18/05/29 10:35:20 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6758
18/05/29 10:35:20 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6758/
18/05/29 10:35:20 INFO mapreduce.Job: Running job: job_1525741534203_6758
18/05/29 10:35:29 INFO mapreduce.Job: Job job_1525741534203_6758 running in uber mode : false
18/05/29 10:35:29 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:35:44 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:35:44 INFO mapreduce.Job: Job job_1525741534203_6758 completed successfully
18/05/29 10:35:44 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153032
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=67691789
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=25202
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=12601
		Total vcore-milliseconds taken by all map tasks=12601
		Total megabyte-milliseconds taken by all map tasks=25806848
	Map-Reduce Framework
		Map input records=672896
		Map output records=672896
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=172
		CPU time spent (ms)=12540
		Physical memory (bytes) snapshot=1075654656
		Virtual memory (bytes) snapshot=2944901120
		Total committed heap usage (bytes)=1033895936
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=67691789
18/05/29 10:35:44 INFO mapreduce.ImportJobBase: Transferred 64.5559 MB in 26.5678 seconds (2.4299 MB/sec)
18/05/29 10:35:44 INFO mapreduce.ImportJobBase: Retrieved 672896 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.75 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.899 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.481 seconds
OK
Time taken: 0.527 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:36:17 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:36:17 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:36:17 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:36:17 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:36:17 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:36:18 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:36:18 INFO manager.SqlManager: Executing SQL statement: select * from WORK_USER_HIS where  (1 = 0) 
18/05/29 10:36:18 INFO manager.SqlManager: Executing SQL statement: select * from WORK_USER_HIS where  (1 = 0) 
18/05/29 10:36:18 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/d861dd5fecd840cf47efe7dddcfae542/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:36:21 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/d861dd5fecd840cf47efe7dddcfae542/QueryResult.jar
18/05/29 10:36:23 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/WORK_USER_HIS/2018-05-28 deleted.
18/05/29 10:36:23 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:36:23 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:36:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:36:23 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:36:25 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:952)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:690)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:879)
18/05/29 10:36:25 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:36:26 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:36:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6759
18/05/29 10:36:26 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6759
18/05/29 10:36:26 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6759/
18/05/29 10:36:26 INFO mapreduce.Job: Running job: job_1525741534203_6759
18/05/29 10:36:33 INFO mapreduce.Job: Job job_1525741534203_6759 running in uber mode : false
18/05/29 10:36:33 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:36:50 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:36:50 INFO mapreduce.Job: Job job_1525741534203_6759 completed successfully
18/05/29 10:36:50 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153048
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=84678908
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=26634
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=13317
		Total vcore-milliseconds taken by all map tasks=13317
		Total megabyte-milliseconds taken by all map tasks=27273216
	Map-Reduce Framework
		Map input records=923479
		Map output records=923479
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=162
		CPU time spent (ms)=13530
		Physical memory (bytes) snapshot=853721088
		Virtual memory (bytes) snapshot=2944221184
		Total committed heap usage (bytes)=948961280
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=84678908
18/05/29 10:36:50 INFO mapreduce.ImportJobBase: Transferred 80.7561 MB in 26.5945 seconds (3.0366 MB/sec)
18/05/29 10:36:50 INFO mapreduce.ImportJobBase: Retrieved 923479 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 8.657 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.936 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.572 seconds
OK
Time taken: 0.613 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:37:25 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:37:25 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:37:25 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:37:25 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:37:25 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:37:26 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:37:26 INFO manager.SqlManager: Executing SQL statement: select * from WORK_DEAL_INFO where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:37:26 INFO manager.SqlManager: Executing SQL statement: select * from WORK_DEAL_INFO where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:37:26 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9ca45db697f0137585592d0294d6c75c/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:37:31 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9ca45db697f0137585592d0294d6c75c/QueryResult.jar
18/05/29 10:37:33 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/WORK_DEAL_INFO/2018-05-28 deleted.
18/05/29 10:37:33 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:37:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:37:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:37:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:37:37 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:37:37 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:37:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6760
18/05/29 10:37:38 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6760
18/05/29 10:37:38 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6760/
18/05/29 10:37:38 INFO mapreduce.Job: Running job: job_1525741534203_6760
18/05/29 10:37:46 INFO mapreduce.Job: Job job_1525741534203_6760 running in uber mode : false
18/05/29 10:37:46 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:38:24 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:38:25 INFO mapreduce.Job: Job job_1525741534203_6760 completed successfully
18/05/29 10:38:26 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153364
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=363633409
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=71762
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=35881
		Total vcore-milliseconds taken by all map tasks=35881
		Total megabyte-milliseconds taken by all map tasks=73484288
	Map-Reduce Framework
		Map input records=595228
		Map output records=595228
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=256
		CPU time spent (ms)=38580
		Physical memory (bytes) snapshot=787746816
		Virtual memory (bytes) snapshot=2929246208
		Total committed heap usage (bytes)=517472256
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=363633409
18/05/29 10:38:26 INFO mapreduce.ImportJobBase: Transferred 346.7878 MB in 52.8252 seconds (6.5648 MB/sec)
18/05/29 10:38:26 INFO mapreduce.ImportJobBase: Retrieved 595228 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.521 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.988 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.597 seconds
OK
Time taken: 0.714 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:38:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:38:59 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:39:00 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:39:00 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:39:00 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:39:00 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:39:00 INFO manager.SqlManager: Executing SQL statement: select * from IXIN_DATA where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:39:01 INFO manager.SqlManager: Executing SQL statement: select * from IXIN_DATA where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:39:01 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9cbd54a39af40c5b34c32d03dfc91694/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:39:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9cbd54a39af40c5b34c32d03dfc91694/QueryResult.jar
18/05/29 10:39:05 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/IXIN_DATA/2018-05-28 deleted.
18/05/29 10:39:05 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:39:05 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:39:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:39:05 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:39:07 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:39:07 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:39:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6761
18/05/29 10:39:08 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6761
18/05/29 10:39:08 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6761/
18/05/29 10:39:08 INFO mapreduce.Job: Running job: job_1525741534203_6761
18/05/29 10:39:15 INFO mapreduce.Job: Job job_1525741534203_6761 running in uber mode : false
18/05/29 10:39:15 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:43:24 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:43:24 INFO mapreduce.Job: Job job_1525741534203_6761 completed successfully
18/05/29 10:43:24 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153344
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=3501971006
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=491560
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=245780
		Total vcore-milliseconds taken by all map tasks=245780
		Total megabyte-milliseconds taken by all map tasks=503357440
	Map-Reduce Framework
		Map input records=16305363
		Map output records=16305363
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=804
		CPU time spent (ms)=223380
		Physical memory (bytes) snapshot=1135104000
		Virtual memory (bytes) snapshot=2944557056
		Total committed heap usage (bytes)=615514112
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=3501971006
18/05/29 10:43:24 INFO mapreduce.ImportJobBase: Transferred 3.2615 GB in 259.1717 seconds (12.8862 MB/sec)
18/05/29 10:43:24 INFO mapreduce.ImportJobBase: Retrieved 16305363 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.823 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.094 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.46 seconds
OK
Time taken: 0.603 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:43:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:43:58 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:43:58 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:43:58 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:43:58 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:43:59 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:43:59 INFO manager.SqlManager: Executing SQL statement: select * from S_CLIENT where  (1 = 0) 
18/05/29 10:43:59 INFO manager.SqlManager: Executing SQL statement: select * from S_CLIENT where  (1 = 0) 
18/05/29 10:43:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9b5a952cd225ffb5b3eeb6ea92b8ca17/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:44:02 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9b5a952cd225ffb5b3eeb6ea92b8ca17/QueryResult.jar
18/05/29 10:44:04 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_CLIENT/2018-05-28 deleted.
18/05/29 10:44:04 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:44:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:44:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:44:04 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:44:06 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:44:06 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:44:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6762
18/05/29 10:44:07 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6762
18/05/29 10:44:07 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6762/
18/05/29 10:44:07 INFO mapreduce.Job: Running job: job_1525741534203_6762
18/05/29 10:44:15 INFO mapreduce.Job: Job job_1525741534203_6762 running in uber mode : false
18/05/29 10:44:15 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:44:21 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:44:21 INFO mapreduce.Job: Job job_1525741534203_6762 completed successfully
18/05/29 10:44:21 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153028
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=661
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8266
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4133
		Total vcore-milliseconds taken by all map tasks=4133
		Total megabyte-milliseconds taken by all map tasks=8464384
	Map-Reduce Framework
		Map input records=4
		Map output records=4
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=40
		CPU time spent (ms)=1640
		Physical memory (bytes) snapshot=258584576
		Virtual memory (bytes) snapshot=2943033344
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=661
18/05/29 10:44:21 INFO mapreduce.ImportJobBase: Transferred 661 bytes in 17.4178 seconds (37.9497 bytes/sec)
18/05/29 10:44:21 INFO mapreduce.ImportJobBase: Retrieved 4 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.623 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.876 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.724 seconds
OK
Time taken: 0.513 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:44:55 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:44:55 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:44:55 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:44:55 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:44:55 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:44:56 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:44:56 INFO manager.SqlManager: Executing SQL statement: select * from S_PROJECTCONFIG where  (1 = 0) 
18/05/29 10:44:56 INFO manager.SqlManager: Executing SQL statement: select * from S_PROJECTCONFIG where  (1 = 0) 
18/05/29 10:44:56 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/1504c716c6e7864ce21c4d90aae5925d/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:44:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1504c716c6e7864ce21c4d90aae5925d/QueryResult.jar
18/05/29 10:45:01 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_PROJECTCONFIG/2018-05-28 deleted.
18/05/29 10:45:01 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:45:01 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:45:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:45:01 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:45:03 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:45:03 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:45:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6763
18/05/29 10:45:03 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6763
18/05/29 10:45:03 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6763/
18/05/29 10:45:03 INFO mapreduce.Job: Running job: job_1525741534203_6763
18/05/29 10:45:12 INFO mapreduce.Job: Job job_1525741534203_6763 running in uber mode : false
18/05/29 10:45:12 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:45:19 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:45:19 INFO mapreduce.Job: Job job_1525741534203_6763 completed successfully
18/05/29 10:45:19 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153056
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=182
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9944
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4972
		Total vcore-milliseconds taken by all map tasks=4972
		Total megabyte-milliseconds taken by all map tasks=10182656
	Map-Reduce Framework
		Map input records=3
		Map output records=3
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=42
		CPU time spent (ms)=1610
		Physical memory (bytes) snapshot=259162112
		Virtual memory (bytes) snapshot=2944454656
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=182
18/05/29 10:45:19 INFO mapreduce.ImportJobBase: Transferred 182 bytes in 18.3709 seconds (9.9069 bytes/sec)
18/05/29 10:45:19 INFO mapreduce.ImportJobBase: Retrieved 3 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.515 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.902 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.434 seconds
OK
Time taken: 0.641 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:45:52 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:45:52 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:45:52 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:45:52 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:45:52 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:45:53 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:45:53 INFO manager.SqlManager: Executing SQL statement: select * from S_EVIDENCE where to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:45:53 INFO manager.SqlManager: Executing SQL statement: select * from S_EVIDENCE where to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:45:53 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/39e7a4e0a6110a87057af9bbd07b9b27/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:45:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/39e7a4e0a6110a87057af9bbd07b9b27/QueryResult.jar
18/05/29 10:45:58 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_EVIDENCE/2018-05-28 deleted.
18/05/29 10:45:58 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:45:58 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:45:58 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:45:58 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:46:01 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:46:01 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:46:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6764
18/05/29 10:46:02 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6764
18/05/29 10:46:02 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6764/
18/05/29 10:46:02 INFO mapreduce.Job: Running job: job_1525741534203_6764
18/05/29 10:46:10 INFO mapreduce.Job: Job job_1525741534203_6764 running in uber mode : false
18/05/29 10:46:10 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:46:38 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:46:38 INFO mapreduce.Job: Job job_1525741534203_6764 completed successfully
18/05/29 10:46:38 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153946
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=8907618
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=52200
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=26100
		Total vcore-milliseconds taken by all map tasks=26100
		Total megabyte-milliseconds taken by all map tasks=53452800
	Map-Reduce Framework
		Map input records=1645
		Map output records=1645
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=253
		CPU time spent (ms)=6070
		Physical memory (bytes) snapshot=694439936
		Virtual memory (bytes) snapshot=2944462848
		Total committed heap usage (bytes)=905969664
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=8907618
18/05/29 10:46:38 INFO mapreduce.ImportJobBase: Transferred 8.495 MB in 39.5085 seconds (220.1767 KB/sec)
18/05/29 10:46:38 INFO mapreduce.ImportJobBase: Retrieved 1645 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.567 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.916 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.706 seconds
OK
Time taken: 0.515 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:47:11 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:47:11 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:47:11 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:47:11 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:47:11 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:47:12 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:47:12 INFO manager.SqlManager: Executing SQL statement: select * from S_D_QZCS where  (1 = 0) 
18/05/29 10:47:12 INFO manager.SqlManager: Executing SQL statement: select * from S_D_QZCS where  (1 = 0) 
18/05/29 10:47:12 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/a223c3d71f1c4a56e9305560bc3ef1c2/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:47:15 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a223c3d71f1c4a56e9305560bc3ef1c2/QueryResult.jar
18/05/29 10:47:17 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_D_QZCS/2018-05-28 deleted.
18/05/29 10:47:17 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:47:17 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:47:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:47:17 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:47:19 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:47:19 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:47:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6765
18/05/29 10:47:20 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6765
18/05/29 10:47:20 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6765/
18/05/29 10:47:20 INFO mapreduce.Job: Running job: job_1525741534203_6765
18/05/29 10:47:28 INFO mapreduce.Job: Job job_1525741534203_6765 running in uber mode : false
18/05/29 10:47:28 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:47:35 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:47:35 INFO mapreduce.Job: Job job_1525741534203_6765 completed successfully
18/05/29 10:47:35 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153028
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=164
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9706
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4853
		Total vcore-milliseconds taken by all map tasks=4853
		Total megabyte-milliseconds taken by all map tasks=9938944
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=43
		CPU time spent (ms)=1630
		Physical memory (bytes) snapshot=257445888
		Virtual memory (bytes) snapshot=2940301312
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=164
18/05/29 10:47:35 INFO mapreduce.ImportJobBase: Transferred 164 bytes in 18.5122 seconds (8.859 bytes/sec)
18/05/29 10:47:35 INFO mapreduce.ImportJobBase: Retrieved 2 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.507 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.841 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.544 seconds
OK
Time taken: 0.581 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:48:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:48:08 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:48:09 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:48:09 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:48:09 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:48:09 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:48:10 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL_CERT_EC where  (1 = 0) 
18/05/29 10:48:10 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL_CERT_EC where  (1 = 0) 
18/05/29 10:48:10 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/cef5d353b9e1caf40bc8727901ebb4a1/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:48:13 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cef5d353b9e1caf40bc8727901ebb4a1/QueryResult.jar
18/05/29 10:48:14 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_ESEAL_CERT_EC/2018-05-28 deleted.
18/05/29 10:48:14 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:48:14 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:48:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:48:15 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:48:17 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:48:17 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:48:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6766
18/05/29 10:48:18 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6766
18/05/29 10:48:18 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6766/
18/05/29 10:48:18 INFO mapreduce.Job: Running job: job_1525741534203_6766
18/05/29 10:48:26 INFO mapreduce.Job: Job job_1525741534203_6766 running in uber mode : false
18/05/29 10:48:26 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:48:34 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:48:34 INFO mapreduce.Job: Job job_1525741534203_6766 completed successfully
18/05/29 10:48:34 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153056
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=673372
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=11670
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=5835
		Total vcore-milliseconds taken by all map tasks=5835
		Total megabyte-milliseconds taken by all map tasks=11950080
	Map-Reduce Framework
		Map input records=7317
		Map output records=7317
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=43
		CPU time spent (ms)=3960
		Physical memory (bytes) snapshot=283774976
		Virtual memory (bytes) snapshot=2950242304
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=673372
18/05/29 10:48:34 INFO mapreduce.ImportJobBase: Transferred 657.5898 KB in 19.6176 seconds (33.5204 KB/sec)
18/05/29 10:48:34 INFO mapreduce.ImportJobBase: Retrieved 7317 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.476 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.93 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.588 seconds
OK
Time taken: 0.57 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:49:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:49:07 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:49:08 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:49:08 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:49:08 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:49:08 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:49:08 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL_CS where  (1 = 0) 
18/05/29 10:49:08 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL_CS where  (1 = 0) 
18/05/29 10:49:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9e97e33774a414024039275a7d146314/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:49:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9e97e33774a414024039275a7d146314/QueryResult.jar
18/05/29 10:49:13 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_ESEAL_CS/2018-05-28 deleted.
18/05/29 10:49:13 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:49:13 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:49:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:49:13 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:49:15 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:49:15 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:49:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6767
18/05/29 10:49:16 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6767
18/05/29 10:49:16 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6767/
18/05/29 10:49:16 INFO mapreduce.Job: Running job: job_1525741534203_6767
18/05/29 10:49:24 INFO mapreduce.Job: Job job_1525741534203_6767 running in uber mode : false
18/05/29 10:49:24 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:49:30 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:49:30 INFO mapreduce.Job: Job job_1525741534203_6767 completed successfully
18/05/29 10:49:30 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153036
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8560
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4280
		Total vcore-milliseconds taken by all map tasks=4280
		Total megabyte-milliseconds taken by all map tasks=8765440
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=53
		CPU time spent (ms)=1450
		Physical memory (bytes) snapshot=255799296
		Virtual memory (bytes) snapshot=2937987072
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
18/05/29 10:49:30 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 17.5457 seconds (0 bytes/sec)
18/05/29 10:49:30 INFO mapreduce.ImportJobBase: Retrieved 0 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.478 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.885 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.765 seconds
OK
Time taken: 0.479 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:50:04 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:50:04 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:50:04 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:50:04 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:50:04 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:50:05 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:50:05 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL_CERT where  (1 = 0) 
18/05/29 10:50:05 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL_CERT where  (1 = 0) 
18/05/29 10:50:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/3bae026dbf46b15f660a7b46b062e2b7/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:50:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/3bae026dbf46b15f660a7b46b062e2b7/QueryResult.jar
18/05/29 10:50:10 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_ESEAL_CERT/2018-05-28 is not present, hence not deleting.
18/05/29 10:50:10 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:50:10 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:50:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:50:10 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:50:12 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:50:12 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:50:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6768
18/05/29 10:50:13 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6768
18/05/29 10:50:13 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6768/
18/05/29 10:50:13 INFO mapreduce.Job: Running job: job_1525741534203_6768
18/05/29 10:50:21 INFO mapreduce.Job: Job job_1525741534203_6768 running in uber mode : false
18/05/29 10:50:21 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:50:37 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:50:37 INFO mapreduce.Job: Job job_1525741534203_6768 completed successfully
18/05/29 10:50:37 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153191
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=8739575
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=28038
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=14019
		Total vcore-milliseconds taken by all map tasks=14019
		Total megabyte-milliseconds taken by all map tasks=28710912
	Map-Reduce Framework
		Map input records=5087
		Map output records=5087
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=124
		CPU time spent (ms)=7710
		Physical memory (bytes) snapshot=822239232
		Virtual memory (bytes) snapshot=2929774592
		Total committed heap usage (bytes)=906493952
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=8739575
18/05/29 10:50:37 INFO mapreduce.ImportJobBase: Transferred 8.3347 MB in 27.5697 seconds (309.5697 KB/sec)
18/05/29 10:50:37 INFO mapreduce.ImportJobBase: Retrieved 5087 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.59 seconds
Dropped the partition ymd=2018-05-27
OK
Time taken: 0.851 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.555 seconds
OK
Time taken: 0.528 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:51:10 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:51:10 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 10:51:10 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 10:51:10 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:51:10 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:51:11 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 10:51:11 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:51:11 INFO manager.SqlManager: Executing SQL statement: select * from S_ESEAL where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 10:51:11 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/e4b5fbe6c55f187c5e1a711ac635b309/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:51:15 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/e4b5fbe6c55f187c5e1a711ac635b309/QueryResult.jar
18/05/29 10:51:17 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/S_ESEAL/2018-05-28 deleted.
18/05/29 10:51:17 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:51:17 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:51:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:51:17 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:51:19 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:51:19 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:51:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6769
18/05/29 10:51:20 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6769
18/05/29 10:51:20 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6769/
18/05/29 10:51:20 INFO mapreduce.Job: Running job: job_1525741534203_6769
18/05/29 10:51:28 INFO mapreduce.Job: Job job_1525741534203_6769 running in uber mode : false
18/05/29 10:51:28 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:52:24 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:52:24 INFO mapreduce.Job: Job job_1525741534203_6769 completed successfully
18/05/29 10:52:25 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153630
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=69087208
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=108096
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=54048
		Total vcore-milliseconds taken by all map tasks=54048
		Total megabyte-milliseconds taken by all map tasks=110690304
	Map-Reduce Framework
		Map input records=5705
		Map output records=5705
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=314
		CPU time spent (ms)=11460
		Physical memory (bytes) snapshot=750125056
		Virtual memory (bytes) snapshot=2938195968
		Total committed heap usage (bytes)=801636352
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=69087208
18/05/29 10:52:25 INFO mapreduce.ImportJobBase: Transferred 65.8867 MB in 67.9569 seconds (992.8057 KB/sec)
18/05/29 10:52:25 INFO mapreduce.ImportJobBase: Retrieved 5705 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.488 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.127 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.458 seconds
OK
Time taken: 0.603 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:52:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:52:58 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 10:52:58 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:52:58 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:52:59 INFO manager.SqlManager: Executing SQL statement: select * from project where  (1 = 0) 
18/05/29 10:52:59 INFO manager.SqlManager: Executing SQL statement: select * from project where  (1 = 0) 
18/05/29 10:52:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/ebae1c32916293b3e6ba1b5da7a4e236/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:53:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/ebae1c32916293b3e6ba1b5da7a4e236/QueryResult.jar
18/05/29 10:53:04 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/project/2018-05-28 deleted.
18/05/29 10:53:04 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:53:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:53:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:53:05 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:53:07 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:53:07 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:53:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6770
18/05/29 10:53:07 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6770
18/05/29 10:53:07 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6770/
18/05/29 10:53:07 INFO mapreduce.Job: Running job: job_1525741534203_6770
18/05/29 10:53:15 INFO mapreduce.Job: Job job_1525741534203_6770 running in uber mode : false
18/05/29 10:53:15 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:53:22 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:53:22 INFO mapreduce.Job: Job job_1525741534203_6770 completed successfully
18/05/29 10:53:22 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=152855
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=24716
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9376
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4688
		Total vcore-milliseconds taken by all map tasks=4688
		Total megabyte-milliseconds taken by all map tasks=9601024
	Map-Reduce Framework
		Map input records=100
		Map output records=100
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=50
		CPU time spent (ms)=2120
		Physical memory (bytes) snapshot=260767744
		Virtual memory (bytes) snapshot=2942283776
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=24716
18/05/29 10:53:22 INFO mapreduce.ImportJobBase: Transferred 24.1367 KB in 17.3644 seconds (1.39 KB/sec)
18/05/29 10:53:22 INFO mapreduce.ImportJobBase: Retrieved 100 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.484 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.034 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.467 seconds
OK
Time taken: 0.604 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 10:53:55 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 10:53:55 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 10:53:55 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 10:53:55 INFO tool.CodeGenTool: Beginning code generation
18/05/29 10:53:56 INFO manager.SqlManager: Executing SQL statement: select * from customer_info where  (1 = 0) 
18/05/29 10:53:56 INFO manager.SqlManager: Executing SQL statement: select * from customer_info where  (1 = 0) 
18/05/29 10:53:56 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/1d2c256221fcab187731fec4597e0275/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 10:54:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1d2c256221fcab187731fec4597e0275/QueryResult.jar
18/05/29 10:54:02 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/customer_info/2018-05-28 deleted.
18/05/29 10:54:02 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 10:54:02 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 10:54:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 10:54:02 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 10:54:04 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 10:54:04 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 10:54:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6771
18/05/29 10:54:05 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6771
18/05/29 10:54:05 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6771/
18/05/29 10:54:05 INFO mapreduce.Job: Running job: job_1525741534203_6771
18/05/29 10:54:13 INFO mapreduce.Job: Job job_1525741534203_6771 running in uber mode : false
18/05/29 10:54:13 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 10:59:35 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 10:59:35 INFO mapreduce.Job: Job job_1525741534203_6771 completed successfully
18/05/29 10:59:35 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=152879
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=494506309
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=638458
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=319229
		Total vcore-milliseconds taken by all map tasks=319229
		Total megabyte-milliseconds taken by all map tasks=653780992
	Map-Reduce Framework
		Map input records=745158
		Map output records=745158
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=1242
		CPU time spent (ms)=75590
		Physical memory (bytes) snapshot=473862144
		Virtual memory (bytes) snapshot=2929045504
		Total committed heap usage (bytes)=386400256
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=494506309
18/05/29 10:59:35 INFO mapreduce.ImportJobBase: Transferred 471.598 MB in 332.5709 seconds (1.418 MB/sec)
18/05/29 10:59:35 INFO mapreduce.ImportJobBase: Retrieved 745158 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.412 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.914 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.568 seconds
OK
Time taken: 0.619 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 11:00:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 11:00:08 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 11:00:08 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 11:00:08 INFO tool.CodeGenTool: Beginning code generation
18/05/29 11:00:09 INFO manager.SqlManager: Executing SQL statement: select * from key_table where  (1 = 0) 
18/05/29 11:00:09 INFO manager.SqlManager: Executing SQL statement: select * from key_table where  (1 = 0) 
18/05/29 11:00:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/54baa85ac79d67d52ac44679c2a75c51/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 11:00:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/54baa85ac79d67d52ac44679c2a75c51/QueryResult.jar
18/05/29 11:00:15 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/key_table/2018-05-28 deleted.
18/05/29 11:00:15 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 11:00:15 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 11:00:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 11:00:16 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 11:00:17 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 11:00:18 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 11:00:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6772
18/05/29 11:00:18 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6772
18/05/29 11:00:18 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6772/
18/05/29 11:00:18 INFO mapreduce.Job: Running job: job_1525741534203_6772
18/05/29 11:00:25 INFO mapreduce.Job: Job job_1525741534203_6772 running in uber mode : false
18/05/29 11:00:25 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 11:27:44 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 11:27:45 INFO mapreduce.Job: Job job_1525741534203_6772 completed successfully
18/05/29 11:27:45 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=152863
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2840961339
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=3272200
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=1636100
		Total vcore-milliseconds taken by all map tasks=1636100
		Total megabyte-milliseconds taken by all map tasks=3350732800
	Map-Reduce Framework
		Map input records=476153
		Map output records=476153
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=3152
		CPU time spent (ms)=255440
		Physical memory (bytes) snapshot=339431424
		Virtual memory (bytes) snapshot=2951430144
		Total committed heap usage (bytes)=385875968
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2840961339
18/05/29 11:27:45 INFO mapreduce.ImportJobBase: Transferred 2.6459 GB in 1,649.2668 seconds (1.6428 MB/sec)
18/05/29 11:27:45 INFO mapreduce.ImportJobBase: Retrieved 476153 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.594 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.938 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.577 seconds
OK
Time taken: 0.653 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 11:28:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 11:28:18 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 11:28:18 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 11:28:18 INFO tool.CodeGenTool: Beginning code generation
18/05/29 11:28:19 INFO manager.SqlManager: Executing SQL statement: select * from CertUpdate where CONVERT(varchar(30),AddInTime,120) <= '2018-05-28 23:59:59' and CONVERT(varchar(30),AddInTime,120) >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 11:28:20 INFO manager.SqlManager: Executing SQL statement: select * from CertUpdate where CONVERT(varchar(30),AddInTime,120) <= '2018-05-28 23:59:59' and CONVERT(varchar(30),AddInTime,120) >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 11:28:20 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/4b55052566f44454e01008957e44e2bb/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 11:28:24 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/4b55052566f44454e01008957e44e2bb/QueryResult.jar
18/05/29 11:28:26 INFO tool.ImportTool: Destination directory /scca/ods_hdfs/ods_db/CertUpdate/2018-05-28 deleted.
18/05/29 11:28:26 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 11:28:26 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 11:28:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 11:28:26 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 11:28:28 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 11:28:28 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 11:28:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6773
18/05/29 11:28:29 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6773
18/05/29 11:28:29 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6773/
18/05/29 11:28:29 INFO mapreduce.Job: Running job: job_1525741534203_6773
18/05/29 11:28:36 INFO mapreduce.Job: Job job_1525741534203_6773 running in uber mode : false
18/05/29 11:28:36 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 11:58:49 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 11:58:49 INFO mapreduce.Job: Job job_1525741534203_6773 completed successfully
18/05/29 11:58:49 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153135
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=1587459817
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=3620356
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=1810178
		Total vcore-milliseconds taken by all map tasks=1810178
		Total megabyte-milliseconds taken by all map tasks=3707244544
	Map-Reduce Framework
		Map input records=714341
		Map output records=714341
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=1973
		CPU time spent (ms)=164640
		Physical memory (bytes) snapshot=185700352
		Virtual memory (bytes) snapshot=2929197056
		Total committed heap usage (bytes)=385875968
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=1587459817
18/05/29 11:58:49 INFO mapreduce.ImportJobBase: Transferred 1.4784 GB in 1,823.1681 seconds (850.3076 KB/sec)
18/05/29 11:58:49 INFO mapreduce.ImportJobBase: Retrieved 714341 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.473 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.066 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.538 seconds
OK
Time taken: 0.652 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 11:59:22 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 11:59:22 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 11:59:23 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
18/05/29 11:59:23 INFO tool.CodeGenTool: Beginning code generation
18/05/29 11:59:24 INFO manager.SqlManager: Executing SQL statement: select count(*) from sign_project where  (1 = 0) 
18/05/29 11:59:24 INFO manager.SqlManager: Executing SQL statement: select count(*) from sign_project where  (1 = 0) 
18/05/29 11:59:24 INFO manager.SqlManager: Executing SQL statement: select count(*) from sign_project where  (1 = 0) 
18/05/29 11:59:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/c0df8899a37cb03bed327dea01bc5e4a/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 11:59:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/c0df8899a37cb03bed327dea01bc5e4a/QueryResult.jar
18/05/29 11:59:28 INFO tool.ImportTool: Destination directory /scca/ods_check/sign_project deleted.
18/05/29 11:59:28 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 11:59:29 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 11:59:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 11:59:29 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 11:59:31 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 11:59:31 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 11:59:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6774
18/05/29 11:59:31 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6774
18/05/29 11:59:31 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6774/
18/05/29 11:59:31 INFO mapreduce.Job: Running job: job_1525741534203_6774
18/05/29 11:59:40 INFO mapreduce.Job: Job job_1525741534203_6774 running in uber mode : false
18/05/29 11:59:40 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 11:59:46 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 11:59:46 INFO mapreduce.Job: Job job_1525741534203_6774 completed successfully
18/05/29 11:59:46 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153031
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=3
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8704
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4352
		Total vcore-milliseconds taken by all map tasks=4352
		Total megabyte-milliseconds taken by all map tasks=8912896
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=50
		CPU time spent (ms)=1790
		Physical memory (bytes) snapshot=260902912
		Virtual memory (bytes) snapshot=2942009344
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=3
18/05/29 11:59:46 INFO mapreduce.ImportJobBase: Transferred 3 bytes in 17.5336 seconds (0.1711 bytes/sec)
18/05/29 11:59:46 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.313 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 8.066 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.sign_project
Table scca_ods_test.sign_project stats: [numFiles=1, totalSize=3]
OK
Time taken: 7.856 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:01:31 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:01:31 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:01:31 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
18/05/29 12:01:31 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:01:32 INFO manager.SqlManager: Executing SQL statement: select count(*) from sign_evidence where create_time <= '2018-05-28 23:59:59' and create_time >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:01:32 INFO manager.SqlManager: Executing SQL statement: select count(*) from sign_evidence where create_time <= '2018-05-28 23:59:59' and create_time >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:01:32 INFO manager.SqlManager: Executing SQL statement: select count(*) from sign_evidence where create_time <= '2018-05-28 23:59:59' and create_time >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:01:32 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/a210191757abbf9906efe5701a010764/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:01:35 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a210191757abbf9906efe5701a010764/QueryResult.jar
18/05/29 12:01:37 INFO tool.ImportTool: Destination directory /scca/ods_check/sign_evidence deleted.
18/05/29 12:01:37 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:01:37 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:01:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:01:37 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:01:39 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:01:39 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:01:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6776
18/05/29 12:01:40 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6776
18/05/29 12:01:40 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6776/
18/05/29 12:01:40 INFO mapreduce.Job: Running job: job_1525741534203_6776
18/05/29 12:01:47 INFO mapreduce.Job: Job job_1525741534203_6776 running in uber mode : false
18/05/29 12:01:47 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:03:09 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:03:09 INFO mapreduce.Job: Job job_1525741534203_6776 completed successfully
18/05/29 12:03:10 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153211
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=6
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=159940
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=79970
		Total vcore-milliseconds taken by all map tasks=79970
		Total megabyte-milliseconds taken by all map tasks=163778560
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=54
		CPU time spent (ms)=1960
		Physical memory (bytes) snapshot=255782912
		Virtual memory (bytes) snapshot=2928345088
		Total committed heap usage (bytes)=487063552
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=6
18/05/29 12:03:10 INFO mapreduce.ImportJobBase: Transferred 6 bytes in 92.8923 seconds (0.0646 bytes/sec)
18/05/29 12:03:10 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.349 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.421 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.sign_evidence
Table scca_ods_test.sign_evidence stats: [numFiles=1, totalSize=6]
OK
Time taken: 7.433 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:04:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:04:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:04:54 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:04:54 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:04:54 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:04:55 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:04:55 INFO manager.SqlManager: Executing SQL statement: select count(*) from CONFIG_APP where  (1 = 0) 
18/05/29 12:04:55 INFO manager.SqlManager: Executing SQL statement: select count(*) from CONFIG_APP where  (1 = 0) 
18/05/29 12:04:55 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/78b915e3f4b4f7623e3335dff34d9e31/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:04:58 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/78b915e3f4b4f7623e3335dff34d9e31/QueryResult.jar
18/05/29 12:05:00 INFO tool.ImportTool: Destination directory /scca/ods_check/CONFIG_APP deleted.
18/05/29 12:05:00 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:05:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:05:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:05:00 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:05:02 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:05:02 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:05:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6778
18/05/29 12:05:03 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6778
18/05/29 12:05:03 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6778/
18/05/29 12:05:03 INFO mapreduce.Job: Running job: job_1525741534203_6778
18/05/29 12:05:10 INFO mapreduce.Job: Job job_1525741534203_6778 running in uber mode : false
18/05/29 12:05:10 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:05:17 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:05:17 INFO mapreduce.Job: Job job_1525741534203_6778 completed successfully
18/05/29 12:05:18 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153016
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=4
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8480
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4240
		Total vcore-milliseconds taken by all map tasks=4240
		Total megabyte-milliseconds taken by all map tasks=8683520
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=44
		CPU time spent (ms)=1620
		Physical memory (bytes) snapshot=260030464
		Virtual memory (bytes) snapshot=2928050176
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=4
18/05/29 12:05:18 INFO mapreduce.ImportJobBase: Transferred 4 bytes in 17.6055 seconds (0.2272 bytes/sec)
18/05/29 12:05:18 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.419 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.557 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.config_app
Table scca_ods_test.config_app stats: [numFiles=1, totalSize=4]
OK
Time taken: 7.614 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:07:02 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:07:02 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:07:03 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:07:03 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:07:03 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:07:03 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:07:04 INFO manager.SqlManager: Executing SQL statement: select count(*) from SYS_OFFICE where  (1 = 0) 
18/05/29 12:07:04 INFO manager.SqlManager: Executing SQL statement: select count(*) from SYS_OFFICE where  (1 = 0) 
18/05/29 12:07:04 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/cf90d28868011c5d36c7e22c27695ed7/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:07:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cf90d28868011c5d36c7e22c27695ed7/QueryResult.jar
18/05/29 12:07:08 INFO tool.ImportTool: Destination directory /scca/ods_check/SYS_OFFICE deleted.
18/05/29 12:07:08 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:07:08 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:07:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:07:08 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:07:10 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:07:10 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:07:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6780
18/05/29 12:07:11 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6780
18/05/29 12:07:11 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6780/
18/05/29 12:07:11 INFO mapreduce.Job: Running job: job_1525741534203_6780
18/05/29 12:07:18 INFO mapreduce.Job: Job job_1525741534203_6780 running in uber mode : false
18/05/29 12:07:18 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:07:25 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:07:25 INFO mapreduce.Job: Job job_1525741534203_6780 completed successfully
18/05/29 12:07:26 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153016
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=3
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8660
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4330
		Total vcore-milliseconds taken by all map tasks=4330
		Total megabyte-milliseconds taken by all map tasks=8867840
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=49
		CPU time spent (ms)=1810
		Physical memory (bytes) snapshot=262856704
		Virtual memory (bytes) snapshot=2943967232
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=3
18/05/29 12:07:26 INFO mapreduce.ImportJobBase: Transferred 3 bytes in 17.5141 seconds (0.1713 bytes/sec)
18/05/29 12:07:26 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.669 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.569 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.sys_office
Table scca_ods_test.sys_office stats: [numFiles=1, totalSize=3]
OK
Time taken: 7.33 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:09:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:09:09 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:09:10 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:09:10 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:09:10 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:09:10 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:09:10 INFO manager.SqlManager: Executing SQL statement: select count(*) from SYS_USER where  (1 = 0) 
18/05/29 12:09:10 INFO manager.SqlManager: Executing SQL statement: select count(*) from SYS_USER where  (1 = 0) 
18/05/29 12:09:10 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/a9f8ba0ac15478e96689ecf0153efa72/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:09:13 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a9f8ba0ac15478e96689ecf0153efa72/QueryResult.jar
18/05/29 12:09:15 INFO tool.ImportTool: Destination directory /scca/ods_check/SYS_USER deleted.
18/05/29 12:09:15 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:09:15 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:09:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:09:15 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:09:17 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:09:17 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:09:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6782
18/05/29 12:09:18 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6782
18/05/29 12:09:18 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6782/
18/05/29 12:09:18 INFO mapreduce.Job: Running job: job_1525741534203_6782
18/05/29 12:09:26 INFO mapreduce.Job: Job job_1525741534203_6782 running in uber mode : false
18/05/29 12:09:26 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:09:33 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:09:33 INFO mapreduce.Job: Job job_1525741534203_6782 completed successfully
18/05/29 12:09:33 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153008
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=4
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9776
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4888
		Total vcore-milliseconds taken by all map tasks=4888
		Total megabyte-milliseconds taken by all map tasks=10010624
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=45
		CPU time spent (ms)=1630
		Physical memory (bytes) snapshot=257933312
		Virtual memory (bytes) snapshot=2928041984
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=4
18/05/29 12:09:33 INFO mapreduce.ImportJobBase: Transferred 4 bytes in 18.3643 seconds (0.2178 bytes/sec)
18/05/29 12:09:33 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.448 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.229 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.sys_user
Table scca_ods_test.sys_user stats: [numFiles=1, totalSize=4]
OK
Time taken: 7.39 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:11:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:11:16 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:11:17 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:11:17 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:11:17 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:11:17 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:11:18 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_CERT_INFO where  (1 = 0) 
18/05/29 12:11:18 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_CERT_INFO where  (1 = 0) 
18/05/29 12:11:18 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/a28fd828b892a7d07fc58ba5023308eb/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:11:20 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a28fd828b892a7d07fc58ba5023308eb/QueryResult.jar
18/05/29 12:11:22 INFO tool.ImportTool: Destination directory /scca/ods_check/WORK_CERT_INFO deleted.
18/05/29 12:11:22 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:11:22 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:11:22 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:11:22 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:11:24 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:11:24 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:11:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6784
18/05/29 12:11:25 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6784
18/05/29 12:11:25 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6784/
18/05/29 12:11:25 INFO mapreduce.Job: Running job: job_1525741534203_6784
18/05/29 12:11:34 INFO mapreduce.Job: Job job_1525741534203_6784 running in uber mode : false
18/05/29 12:11:34 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:11:47 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:11:47 INFO mapreduce.Job: Job job_1525741534203_6784 completed successfully
18/05/29 12:11:47 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153032
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=22568
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=11284
		Total vcore-milliseconds taken by all map tasks=11284
		Total megabyte-milliseconds taken by all map tasks=23109632
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=52
		CPU time spent (ms)=1820
		Physical memory (bytes) snapshot=257290240
		Virtual memory (bytes) snapshot=2944004096
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:11:47 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 24.7312 seconds (0.283 bytes/sec)
18/05/29 12:11:47 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.424 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.1 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.work_cert_info
Table scca_ods_test.work_cert_info stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.315 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:13:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:13:32 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:13:32 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:13:32 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:13:32 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:13:33 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:13:33 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_COMPANY where  (1 = 0) 
18/05/29 12:13:33 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_COMPANY where  (1 = 0) 
18/05/29 12:13:33 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/671acdab2bcc981afe007b902bce06ef/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:13:36 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/671acdab2bcc981afe007b902bce06ef/QueryResult.jar
18/05/29 12:13:38 INFO tool.ImportTool: Destination directory /scca/ods_check/WORK_COMPANY deleted.
18/05/29 12:13:38 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:13:38 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:13:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:13:38 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:13:40 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:13:40 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:13:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6786
18/05/29 12:13:40 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6786
18/05/29 12:13:41 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6786/
18/05/29 12:13:41 INFO mapreduce.Job: Running job: job_1525741534203_6786
18/05/29 12:13:49 INFO mapreduce.Job: Job job_1525741534203_6786 running in uber mode : false
18/05/29 12:13:49 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:13:58 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:13:59 INFO mapreduce.Job: Job job_1525741534203_6786 completed successfully
18/05/29 12:13:59 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153024
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=14530
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=7265
		Total vcore-milliseconds taken by all map tasks=7265
		Total megabyte-milliseconds taken by all map tasks=14878720
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=45
		CPU time spent (ms)=1640
		Physical memory (bytes) snapshot=261390336
		Virtual memory (bytes) snapshot=2942464000
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:13:59 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 21.3463 seconds (0.3279 bytes/sec)
18/05/29 12:13:59 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.442 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.132 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.work_company
Table scca_ods_test.work_company stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.296 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:15:43 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:15:43 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:15:43 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:15:43 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:15:43 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:15:44 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:15:44 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_COMPANY_HIS where  (1 = 0) 
18/05/29 12:15:44 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_COMPANY_HIS where  (1 = 0) 
18/05/29 12:15:44 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9113106330131d80fabfb1bd44e26049/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:15:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9113106330131d80fabfb1bd44e26049/QueryResult.jar
18/05/29 12:15:49 INFO tool.ImportTool: Destination directory /scca/ods_check/WORK_COMPANY_HIS deleted.
18/05/29 12:15:49 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:15:49 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:15:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:15:49 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:15:51 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:15:51 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:15:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6788
18/05/29 12:15:52 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6788
18/05/29 12:15:52 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6788/
18/05/29 12:15:52 INFO mapreduce.Job: Running job: job_1525741534203_6788
18/05/29 12:16:00 INFO mapreduce.Job: Job job_1525741534203_6788 running in uber mode : false
18/05/29 12:16:00 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:16:26 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:16:26 INFO mapreduce.Job: Job job_1525741534203_6788 completed successfully
18/05/29 12:16:27 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153040
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=46940
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=23470
		Total vcore-milliseconds taken by all map tasks=23470
		Total megabyte-milliseconds taken by all map tasks=48066560
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=56
		CPU time spent (ms)=1860
		Physical memory (bytes) snapshot=255709184
		Virtual memory (bytes) snapshot=2945220608
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:16:27 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 37.8883 seconds (0.1848 bytes/sec)
18/05/29 12:16:27 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 9.943 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.266 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.work_company_his
Table scca_ods_test.work_company_his stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.378 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:18:13 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:18:13 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:18:14 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:18:14 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:18:14 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:18:15 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:18:15 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_USER where  (1 = 0) 
18/05/29 12:18:15 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_USER where  (1 = 0) 
18/05/29 12:18:15 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/d18a21778fe681e3dff5614ee2db40a5/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:18:17 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/d18a21778fe681e3dff5614ee2db40a5/QueryResult.jar
18/05/29 12:18:19 INFO tool.ImportTool: Destination directory /scca/ods_check/WORK_USER deleted.
18/05/29 12:18:19 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:18:19 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:18:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:18:19 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:18:21 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:18:22 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:18:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6790
18/05/29 12:18:22 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6790
18/05/29 12:18:22 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6790/
18/05/29 12:18:22 INFO mapreduce.Job: Running job: job_1525741534203_6790
18/05/29 12:18:29 INFO mapreduce.Job: Job job_1525741534203_6790 running in uber mode : false
18/05/29 12:18:29 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:18:41 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:18:42 INFO mapreduce.Job: Job job_1525741534203_6790 completed successfully
18/05/29 12:18:42 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153012
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=18866
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=9433
		Total vcore-milliseconds taken by all map tasks=9433
		Total megabyte-milliseconds taken by all map tasks=19318784
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=53
		CPU time spent (ms)=1820
		Physical memory (bytes) snapshot=264556544
		Virtual memory (bytes) snapshot=2944692224
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:18:42 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 22.4698 seconds (0.3115 bytes/sec)
18/05/29 12:18:42 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.354 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.208 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.work_user
Table scca_ods_test.work_user stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.294 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:20:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:20:26 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:20:26 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:20:26 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:20:26 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:20:27 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:20:27 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_USER_HIS where  (1 = 0) 
18/05/29 12:20:27 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_USER_HIS where  (1 = 0) 
18/05/29 12:20:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/81506e3daed27ca55c37e154265d833f/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:20:30 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/81506e3daed27ca55c37e154265d833f/QueryResult.jar
18/05/29 12:20:32 INFO tool.ImportTool: Destination directory /scca/ods_check/WORK_USER_HIS deleted.
18/05/29 12:20:32 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:20:32 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:20:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:20:32 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:20:34 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:20:34 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:20:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6792
18/05/29 12:20:35 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6792
18/05/29 12:20:35 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6792/
18/05/29 12:20:35 INFO mapreduce.Job: Running job: job_1525741534203_6792
18/05/29 12:20:43 INFO mapreduce.Job: Job job_1525741534203_6792 running in uber mode : false
18/05/29 12:20:43 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:20:56 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:20:57 INFO mapreduce.Job: Job job_1525741534203_6792 completed successfully
18/05/29 12:20:58 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153028
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=22626
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=11313
		Total vcore-milliseconds taken by all map tasks=11313
		Total megabyte-milliseconds taken by all map tasks=23169024
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=44
		CPU time spent (ms)=1680
		Physical memory (bytes) snapshot=257757184
		Virtual memory (bytes) snapshot=2929016832
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:20:58 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 25.5225 seconds (0.2743 bytes/sec)
18/05/29 12:20:58 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.262 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.143 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.work_user_his
Table scca_ods_test.work_user_his stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.549 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:22:41 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:22:41 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:22:41 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:22:41 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:22:41 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:22:42 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:22:42 INFO manager.SqlManager: Executing SQL statement: select count(*) from IXIN_DATA where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:22:42 INFO manager.SqlManager: Executing SQL statement: select count(*) from IXIN_DATA where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:22:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/8a255920e34b80b24e30d97b727d3e40/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:22:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/8a255920e34b80b24e30d97b727d3e40/QueryResult.jar
18/05/29 12:22:46 INFO tool.ImportTool: Destination directory /scca/ods_check/IXIN_DATA deleted.
18/05/29 12:22:46 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:22:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:22:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:22:46 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:22:49 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:22:49 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6794
18/05/29 12:22:49 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6794
18/05/29 12:22:49 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6794/
18/05/29 12:22:49 INFO mapreduce.Job: Running job: job_1525741534203_6794
18/05/29 12:22:56 INFO mapreduce.Job: Job job_1525741534203_6794 running in uber mode : false
18/05/29 12:22:56 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:23:32 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:23:33 INFO mapreduce.Job: Job job_1525741534203_6794 completed successfully
18/05/29 12:23:33 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153324
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=9
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=65972
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=32986
		Total vcore-milliseconds taken by all map tasks=32986
		Total megabyte-milliseconds taken by all map tasks=67555328
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=49
		CPU time spent (ms)=1850
		Physical memory (bytes) snapshot=260087808
		Virtual memory (bytes) snapshot=2942025728
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=9
18/05/29 12:23:33 INFO mapreduce.ImportJobBase: Transferred 9 bytes in 46.624 seconds (0.193 bytes/sec)
18/05/29 12:23:33 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.452 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.281 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.ixin_data
Table scca_ods_test.ixin_data stats: [numFiles=1, totalSize=9]
OK
Time taken: 7.236 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:25:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:25:23 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:25:24 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:25:24 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:25:24 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:25:24 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:25:24 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_DEAL_INFO where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:25:24 INFO manager.SqlManager: Executing SQL statement: select count(*) from WORK_DEAL_INFO where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:25:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/39a34572fbeab2dad792039e8c90a14d/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:25:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/39a34572fbeab2dad792039e8c90a14d/QueryResult.jar
18/05/29 12:25:29 INFO tool.ImportTool: Destination directory /scca/ods_check/WORK_DEAL_INFO deleted.
18/05/29 12:25:29 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:25:29 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:25:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:25:29 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:25:31 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:25:31 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:25:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6796
18/05/29 12:25:32 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6796
18/05/29 12:25:32 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6796/
18/05/29 12:25:32 INFO mapreduce.Job: Running job: job_1525741534203_6796
18/05/29 12:25:40 INFO mapreduce.Job: Job job_1525741534203_6796 running in uber mode : false
18/05/29 12:25:40 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:25:50 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:25:50 INFO mapreduce.Job: Job job_1525741534203_6796 completed successfully
18/05/29 12:25:50 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153344
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=16666
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=8333
		Total vcore-milliseconds taken by all map tasks=8333
		Total megabyte-milliseconds taken by all map tasks=17065984
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=52
		CPU time spent (ms)=1810
		Physical memory (bytes) snapshot=257679360
		Virtual memory (bytes) snapshot=2945327104
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:25:50 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 21.5152 seconds (0.3254 bytes/sec)
18/05/29 12:25:50 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.438 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.623 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.work_deal_info
Table scca_ods_test.work_deal_info stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.38 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:27:34 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:27:34 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:27:34 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:27:34 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:27:34 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:27:35 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:27:35 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_CLIENT where  (1 = 0) 
18/05/29 12:27:35 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_CLIENT where  (1 = 0) 
18/05/29 12:27:35 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/041ca1c0b34a11ccbeefbed207dfd2b6/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:27:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/041ca1c0b34a11ccbeefbed207dfd2b6/QueryResult.jar
18/05/29 12:27:40 INFO tool.ImportTool: Destination directory /scca/ods_check/S_CLIENT deleted.
18/05/29 12:27:40 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:27:40 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:27:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:27:40 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:27:42 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:27:42 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:27:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6798
18/05/29 12:27:43 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6798
18/05/29 12:27:43 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6798/
18/05/29 12:27:43 INFO mapreduce.Job: Running job: job_1525741534203_6798
18/05/29 12:27:51 INFO mapreduce.Job: Job job_1525741534203_6798 running in uber mode : false
18/05/29 12:27:51 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:27:57 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:27:57 INFO mapreduce.Job: Job job_1525741534203_6798 completed successfully
18/05/29 12:27:57 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153008
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8332
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4166
		Total vcore-milliseconds taken by all map tasks=4166
		Total megabyte-milliseconds taken by all map tasks=8531968
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=43
		CPU time spent (ms)=1610
		Physical memory (bytes) snapshot=264335360
		Virtual memory (bytes) snapshot=2948718592
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2
18/05/29 12:27:57 INFO mapreduce.ImportJobBase: Transferred 2 bytes in 17.4444 seconds (0.1147 bytes/sec)
18/05/29 12:27:57 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.454 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.251 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_client
Table scca_ods_test.s_client stats: [numFiles=1, totalSize=2]
OK
Time taken: 7.328 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:29:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:29:40 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:29:41 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:29:41 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:29:41 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:29:43 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:29:43 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_PROJECTCONFIG where  (1 = 0) 
18/05/29 12:29:43 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_PROJECTCONFIG where  (1 = 0) 
18/05/29 12:29:43 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/0a817c19b1994f96cdfb5b1ea1ba5187/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:29:46 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/0a817c19b1994f96cdfb5b1ea1ba5187/QueryResult.jar
18/05/29 12:29:48 INFO tool.ImportTool: Destination directory /scca/ods_check/S_PROJECTCONFIG deleted.
18/05/29 12:29:48 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:29:48 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:29:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:29:48 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:29:55 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:29:55 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:29:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6800
18/05/29 12:29:56 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6800
18/05/29 12:29:56 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6800/
18/05/29 12:29:56 INFO mapreduce.Job: Running job: job_1525741534203_6800
18/05/29 12:30:04 INFO mapreduce.Job: Job job_1525741534203_6800 running in uber mode : false
18/05/29 12:30:04 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:30:19 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:30:19 INFO mapreduce.Job: Job job_1525741534203_6800 completed successfully
18/05/29 12:30:19 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153036
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=25614
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=12807
		Total vcore-milliseconds taken by all map tasks=12807
		Total megabyte-milliseconds taken by all map tasks=26228736
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=49
		CPU time spent (ms)=1930
		Physical memory (bytes) snapshot=255418368
		Virtual memory (bytes) snapshot=2943512576
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2
18/05/29 12:30:19 INFO mapreduce.ImportJobBase: Transferred 2 bytes in 31.1918 seconds (0.0641 bytes/sec)
18/05/29 12:30:19 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.726 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.161 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_projectconfig
Table scca_ods_test.s_projectconfig stats: [numFiles=1, totalSize=2]
OK
Time taken: 7.847 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:32:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:32:03 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:32:03 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:32:03 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:32:03 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:32:04 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:32:04 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_EVIDENCE where to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:32:05 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_EVIDENCE where to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_TIME,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:32:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/86dbc076a911464250f3c567e4245349/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:32:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/86dbc076a911464250f3c567e4245349/QueryResult.jar
18/05/29 12:32:09 INFO tool.ImportTool: Destination directory /scca/ods_check/S_EVIDENCE deleted.
18/05/29 12:32:09 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:32:09 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:32:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:32:09 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:32:15 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:32:15 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:32:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6802
18/05/29 12:32:16 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6802
18/05/29 12:32:16 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6802/
18/05/29 12:32:16 INFO mapreduce.Job: Running job: job_1525741534203_6802
18/05/29 12:32:23 INFO mapreduce.Job: Job job_1525741534203_6802 running in uber mode : false
18/05/29 12:32:23 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:32:30 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:32:30 INFO mapreduce.Job: Job job_1525741534203_6802 completed successfully
18/05/29 12:32:31 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153328
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=5
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9252
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4626
		Total vcore-milliseconds taken by all map tasks=4626
		Total megabyte-milliseconds taken by all map tasks=9474048
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=50
		CPU time spent (ms)=1780
		Physical memory (bytes) snapshot=263258112
		Virtual memory (bytes) snapshot=2944315392
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=5
18/05/29 12:32:31 INFO mapreduce.ImportJobBase: Transferred 5 bytes in 21.2841 seconds (0.2349 bytes/sec)
18/05/29 12:32:31 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.361 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.118 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_evidence
Table scca_ods_test.s_evidence stats: [numFiles=1, totalSize=5]
OK
Time taken: 7.435 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:34:13 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:34:13 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:34:14 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:34:14 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:34:14 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:34:14 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:34:15 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_D_QZCS where  (1 = 0) 
18/05/29 12:34:15 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_D_QZCS where  (1 = 0) 
18/05/29 12:34:15 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/dd09c6a8ec98c208556cb1af84e4e3dc/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:34:17 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/dd09c6a8ec98c208556cb1af84e4e3dc/QueryResult.jar
18/05/29 12:34:19 INFO tool.ImportTool: Destination directory /scca/ods_check/S_D_QZCS deleted.
18/05/29 12:34:19 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:34:19 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:34:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:34:19 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:34:21 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:34:21 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:34:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6804
18/05/29 12:34:22 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6804
18/05/29 12:34:22 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6804/
18/05/29 12:34:22 INFO mapreduce.Job: Running job: job_1525741534203_6804
18/05/29 12:34:29 INFO mapreduce.Job: Job job_1525741534203_6804 running in uber mode : false
18/05/29 12:34:29 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:34:36 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:34:36 INFO mapreduce.Job: Job job_1525741534203_6804 completed successfully
18/05/29 12:34:36 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153008
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8524
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4262
		Total vcore-milliseconds taken by all map tasks=4262
		Total megabyte-milliseconds taken by all map tasks=8728576
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=43
		CPU time spent (ms)=1670
		Physical memory (bytes) snapshot=256876544
		Virtual memory (bytes) snapshot=2946056192
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2
18/05/29 12:34:36 INFO mapreduce.ImportJobBase: Transferred 2 bytes in 17.4559 seconds (0.1146 bytes/sec)
18/05/29 12:34:37 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.419 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.61 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_d_qzcs
Table scca_ods_test.s_d_qzcs stats: [numFiles=1, totalSize=2]
OK
Time taken: 7.479 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:36:20 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:36:20 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:36:20 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:36:20 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:36:20 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:36:21 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:36:21 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL_CERT where  (1 = 0) 
18/05/29 12:36:21 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL_CERT where  (1 = 0) 
18/05/29 12:36:21 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/25eca355b7a13fffba91512f8f836fd8/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:36:24 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/25eca355b7a13fffba91512f8f836fd8/QueryResult.jar
18/05/29 12:36:26 INFO tool.ImportTool: Destination directory /scca/ods_check/S_ESEAL_CERT deleted.
18/05/29 12:36:26 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:36:26 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:36:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:36:26 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:36:28 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:36:28 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:36:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6806
18/05/29 12:36:29 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6806
18/05/29 12:36:29 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6806/
18/05/29 12:36:29 INFO mapreduce.Job: Running job: job_1525741534203_6806
18/05/29 12:36:36 INFO mapreduce.Job: Job job_1525741534203_6806 running in uber mode : false
18/05/29 12:36:36 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:36:43 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:36:43 INFO mapreduce.Job: Job job_1525741534203_6806 completed successfully
18/05/29 12:36:44 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153024
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=5
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8976
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4488
		Total vcore-milliseconds taken by all map tasks=4488
		Total megabyte-milliseconds taken by all map tasks=9191424
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=50
		CPU time spent (ms)=1820
		Physical memory (bytes) snapshot=260935680
		Virtual memory (bytes) snapshot=2946912256
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=5
18/05/29 12:36:44 INFO mapreduce.ImportJobBase: Transferred 5 bytes in 17.624 seconds (0.2837 bytes/sec)
18/05/29 12:36:44 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.482 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.821 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_eseal_cert
Table scca_ods_test.s_eseal_cert stats: [numFiles=1, totalSize=5]
OK
Time taken: 7.513 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:38:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:38:26 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:38:27 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:38:27 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:38:27 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:38:28 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:38:28 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL_CERT_EC where  (1 = 0) 
18/05/29 12:38:28 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL_CERT_EC where  (1 = 0) 
18/05/29 12:38:28 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/d002bf97198a424d72f5dd3f23eab83a/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:38:31 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/d002bf97198a424d72f5dd3f23eab83a/QueryResult.jar
18/05/29 12:38:32 INFO tool.ImportTool: Destination directory /scca/ods_check/S_ESEAL_CERT_EC deleted.
18/05/29 12:38:32 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:38:32 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:38:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:38:32 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:38:34 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:38:35 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:38:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6808
18/05/29 12:38:35 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6808
18/05/29 12:38:35 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6808/
18/05/29 12:38:35 INFO mapreduce.Job: Running job: job_1525741534203_6808
18/05/29 12:38:42 INFO mapreduce.Job: Job job_1525741534203_6808 running in uber mode : false
18/05/29 12:38:42 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:38:49 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:38:49 INFO mapreduce.Job: Job job_1525741534203_6808 completed successfully
18/05/29 12:38:50 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153036
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=5
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9026
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4513
		Total vcore-milliseconds taken by all map tasks=4513
		Total megabyte-milliseconds taken by all map tasks=9242624
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=53
		CPU time spent (ms)=1810
		Physical memory (bytes) snapshot=256987136
		Virtual memory (bytes) snapshot=2936696832
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=5
18/05/29 12:38:50 INFO mapreduce.ImportJobBase: Transferred 5 bytes in 17.3252 seconds (0.2886 bytes/sec)
18/05/29 12:38:50 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.263 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.268 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_eseal_cert_ec
Table scca_ods_test.s_eseal_cert_ec stats: [numFiles=1, totalSize=5]
OK
Time taken: 7.312 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:40:34 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:40:34 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:40:34 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:40:34 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:40:34 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:40:35 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:40:35 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL_CS where  (1 = 0) 
18/05/29 12:40:35 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL_CS where  (1 = 0) 
18/05/29 12:40:35 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/6a162b036890237fa135b8924495bc89/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:40:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/6a162b036890237fa135b8924495bc89/QueryResult.jar
18/05/29 12:40:40 INFO tool.ImportTool: Destination directory /scca/ods_check/S_ESEAL_CS deleted.
18/05/29 12:40:40 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:40:40 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:40:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:40:40 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:40:42 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:40:42 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:40:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6810
18/05/29 12:40:43 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6810
18/05/29 12:40:43 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6810/
18/05/29 12:40:43 INFO mapreduce.Job: Running job: job_1525741534203_6810
18/05/29 12:40:51 INFO mapreduce.Job: Job job_1525741534203_6810 running in uber mode : false
18/05/29 12:40:51 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:40:57 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:40:57 INFO mapreduce.Job: Job job_1525741534203_6810 completed successfully
18/05/29 12:40:57 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153016
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=2
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8458
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4229
		Total vcore-milliseconds taken by all map tasks=4229
		Total megabyte-milliseconds taken by all map tasks=8660992
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=44
		CPU time spent (ms)=1690
		Physical memory (bytes) snapshot=263041024
		Virtual memory (bytes) snapshot=2944991232
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2
18/05/29 12:40:57 INFO mapreduce.ImportJobBase: Transferred 2 bytes in 17.5396 seconds (0.114 bytes/sec)
18/05/29 12:40:57 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.437 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.148 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_eseal_cs
Table scca_ods_test.s_eseal_cs stats: [numFiles=1, totalSize=2]
OK
Time taken: 7.521 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:42:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:42:40 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/29 12:42:41 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.
18/05/29 12:42:41 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:42:41 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:42:41 INFO manager.OracleManager: Time zone has been set to GMT
18/05/29 12:42:41 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:42:42 INFO manager.SqlManager: Executing SQL statement: select count(*) from S_ESEAL where to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') <= '2018-05-28 23:59:59' and to_char(CREATE_DATE,'yyyy-mm-dd hh24:mi:ss') >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:42:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/b3ebfa92ac296e8c72b2a3dee94de2b8/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:42:44 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/b3ebfa92ac296e8c72b2a3dee94de2b8/QueryResult.jar
18/05/29 12:42:46 INFO tool.ImportTool: Destination directory /scca/ods_check/S_ESEAL deleted.
18/05/29 12:42:46 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:42:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:42:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:42:46 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:42:48 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:42:48 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:42:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6812
18/05/29 12:42:49 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6812
18/05/29 12:42:49 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6812/
18/05/29 12:42:49 INFO mapreduce.Job: Running job: job_1525741534203_6812
18/05/29 12:42:57 INFO mapreduce.Job: Job job_1525741534203_6812 running in uber mode : false
18/05/29 12:42:57 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:43:04 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:43:05 INFO mapreduce.Job: Job job_1525741534203_6812 completed successfully
18/05/29 12:43:05 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153316
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=5
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=11494
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=5747
		Total vcore-milliseconds taken by all map tasks=5747
		Total megabyte-milliseconds taken by all map tasks=11769856
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=45
		CPU time spent (ms)=1620
		Physical memory (bytes) snapshot=259682304
		Virtual memory (bytes) snapshot=2928041984
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=5
18/05/29 12:43:05 INFO mapreduce.ImportJobBase: Transferred 5 bytes in 19.7053 seconds (0.2537 bytes/sec)
18/05/29 12:43:05 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.352 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.31 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.s_eseal
Table scca_ods_test.s_eseal stats: [numFiles=1, totalSize=5]
OK
Time taken: 7.256 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:44:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:44:49 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 12:44:49 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:44:49 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:44:50 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from project where  (1 = 0) 
18/05/29 12:44:51 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from project where  (1 = 0) 
18/05/29 12:44:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/1a58916b17a40baad4661aea8d9f0db7/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:44:53 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1a58916b17a40baad4661aea8d9f0db7/QueryResult.jar
18/05/29 12:44:55 INFO tool.ImportTool: Destination directory /scca/ods_check/project deleted.
18/05/29 12:44:55 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:44:55 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:44:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:44:55 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:44:57 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:952)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:690)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:879)
18/05/29 12:44:57 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:44:57 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:44:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6814
18/05/29 12:44:58 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6814
18/05/29 12:44:58 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6814/
18/05/29 12:44:58 INFO mapreduce.Job: Running job: job_1525741534203_6814
18/05/29 12:45:06 INFO mapreduce.Job: Job job_1525741534203_6814 running in uber mode : false
18/05/29 12:45:06 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:45:12 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:45:12 INFO mapreduce.Job: Job job_1525741534203_6814 completed successfully
18/05/29 12:45:12 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=152849
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=4
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8580
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4290
		Total vcore-milliseconds taken by all map tasks=4290
		Total megabyte-milliseconds taken by all map tasks=8785920
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=53
		CPU time spent (ms)=1960
		Physical memory (bytes) snapshot=260161536
		Virtual memory (bytes) snapshot=2943635456
		Total committed heap usage (bytes)=622854144
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=4
18/05/29 12:45:12 INFO mapreduce.ImportJobBase: Transferred 4 bytes in 17.365 seconds (0.2303 bytes/sec)
18/05/29 12:45:12 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.508 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.113 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.project
Table scca_ods_test.project stats: [numFiles=1, totalSize=4]
OK
Time taken: 7.573 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:46:56 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:46:56 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 12:46:56 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:46:56 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:46:57 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from customer_info where  (1 = 0) 
18/05/29 12:46:58 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from customer_info where  (1 = 0) 
18/05/29 12:46:58 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/7ac4cd6356ae4958f4018410381eb880/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:47:01 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/7ac4cd6356ae4958f4018410381eb880/QueryResult.jar
18/05/29 12:47:02 INFO tool.ImportTool: Destination directory /scca/ods_check/customer_info deleted.
18/05/29 12:47:02 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:47:02 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:47:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:47:02 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:47:04 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:952)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:690)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:879)
18/05/29 12:47:04 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:47:04 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:47:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6816
18/05/29 12:47:05 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6816
18/05/29 12:47:05 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6816/
18/05/29 12:47:05 INFO mapreduce.Job: Running job: job_1525741534203_6816
18/05/29 12:47:13 INFO mapreduce.Job: Job job_1525741534203_6816 running in uber mode : false
18/05/29 12:47:13 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:47:28 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:47:28 INFO mapreduce.Job: Job job_1525741534203_6816 completed successfully
18/05/29 12:47:28 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=152873
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=24386
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=12193
		Total vcore-milliseconds taken by all map tasks=12193
		Total megabyte-milliseconds taken by all map tasks=24971264
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=44
		CPU time spent (ms)=1880
		Physical memory (bytes) snapshot=255021056
		Virtual memory (bytes) snapshot=2945200128
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:47:28 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 25.4497 seconds (0.2751 bytes/sec)
18/05/29 12:47:28 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.267 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.225 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.customer_info
Table scca_ods_test.customer_info stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.241 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:49:11 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:49:11 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 12:49:12 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:49:12 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:49:13 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from key_table where  (1 = 0) 
18/05/29 12:49:13 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from key_table where  (1 = 0) 
18/05/29 12:49:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/0616bc5bb9c35ce6cacae233f6e74402/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:49:16 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/0616bc5bb9c35ce6cacae233f6e74402/QueryResult.jar
18/05/29 12:49:17 INFO tool.ImportTool: Destination directory /scca/ods_check/key_table deleted.
18/05/29 12:49:17 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:49:17 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:49:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:49:17 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:49:19 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:49:20 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:49:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6818
18/05/29 12:49:20 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6818
18/05/29 12:49:20 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6818/
18/05/29 12:49:20 INFO mapreduce.Job: Running job: job_1525741534203_6818
18/05/29 12:49:27 INFO mapreduce.Job: Job job_1525741534203_6818 running in uber mode : false
18/05/29 12:49:27 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:49:40 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:49:40 INFO mapreduce.Job: Job job_1525741534203_6818 completed successfully
18/05/29 12:49:41 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=152857
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=21088
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=10544
		Total vcore-milliseconds taken by all map tasks=10544
		Total megabyte-milliseconds taken by all map tasks=21594112
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=45
		CPU time spent (ms)=1900
		Physical memory (bytes) snapshot=258789376
		Virtual memory (bytes) snapshot=2942902272
		Total committed heap usage (bytes)=493879296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:49:41 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 23.2941 seconds (0.3005 bytes/sec)
18/05/29 12:49:41 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.223 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.356 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.key_table
Table scca_ods_test.key_table stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.41 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/29 12:51:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/29 12:51:27 INFO SqlServer.MSSQLServerManagerFactory: Using Microsoft's SQL Server - Hadoop Connector
18/05/29 12:51:27 INFO manager.SqlManager: Using default fetchSize of 1000
18/05/29 12:51:27 INFO tool.CodeGenTool: Beginning code generation
18/05/29 12:51:28 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from CertUpdate where CONVERT(varchar(30),AddInTime,120) <= '2018-05-28 23:59:59' and CONVERT(varchar(30),AddInTime,120) >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:51:29 INFO manager.SqlManager: Executing SQL statement: select count(*) as num from CertUpdate where CONVERT(varchar(30),AddInTime,120) <= '2018-05-28 23:59:59' and CONVERT(varchar(30),AddInTime,120) >= '2000-01-01 00:00:00' and  (1 = 0) 
18/05/29 12:51:29 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9b46e02b19ec667c4ab0a609320d3cde/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/29 12:51:32 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9b46e02b19ec667c4ab0a609320d3cde/QueryResult.jar
18/05/29 12:51:33 INFO tool.ImportTool: Destination directory /scca/ods_check/CertUpdate deleted.
18/05/29 12:51:33 INFO mapreduce.ImportJobBase: Beginning query import.
18/05/29 12:51:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/05/29 12:51:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/05/29 12:51:34 INFO client.RMProxy: Connecting to ResourceManager at master/172.18.110.21:10012
18/05/29 12:51:36 INFO db.DBInputFormat: Using read commited transaction isolation
18/05/29 12:51:36 INFO mapreduce.JobSubmitter: number of splits:1
18/05/29 12:51:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525741534203_6820
18/05/29 12:51:36 INFO impl.YarnClientImpl: Submitted application application_1525741534203_6820
18/05/29 12:51:36 INFO mapreduce.Job: The url to track the job: http://master:10014/proxy/application_1525741534203_6820/
18/05/29 12:51:36 INFO mapreduce.Job: Running job: job_1525741534203_6820
18/05/29 12:51:43 INFO mapreduce.Job: Job job_1525741534203_6820 running in uber mode : false
18/05/29 12:51:43 INFO mapreduce.Job:  map 0% reduce 0%
18/05/29 12:53:32 INFO mapreduce.Job:  map 100% reduce 0%
18/05/29 12:53:32 INFO mapreduce.Job: Job job_1525741534203_6820 completed successfully
18/05/29 12:53:33 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=153129
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=7
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=212104
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=106052
		Total vcore-milliseconds taken by all map tasks=106052
		Total megabyte-milliseconds taken by all map tasks=217194496
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=56
		CPU time spent (ms)=2270
		Physical memory (bytes) snapshot=256155648
		Virtual memory (bytes) snapshot=2940485632
		Total committed heap usage (bytes)=487587840
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=7
18/05/29 12:53:33 INFO mapreduce.ImportJobBase: Transferred 7 bytes in 119.1231 seconds (0.0588 bytes/sec)
18/05/29 12:53:33 INFO mapreduce.ImportJobBase: Retrieved 1 records.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.327 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.096 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
Loading data to table scca_ods_test.certupdate
Table scca_ods_test.certupdate stats: [numFiles=1, totalSize=7]
OK
Time taken: 7.279 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.548 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.868 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.639 seconds
Query ID = hadoop_20180529125555_674d6f5d-1a4b-4df9-ab41-fb2e020e7b39
Total jobs = 5
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6822, Tracking URL = http://master:10014/proxy/application_1525741534203_6822/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6822
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 11
2018-05-29 12:56:00,423 Stage-1 map = 0%,  reduce = 0%
2018-05-29 12:56:10,037 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 13.68 sec
2018-05-29 12:56:11,095 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 21.0 sec
2018-05-29 12:56:12,144 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 27.34 sec
2018-05-29 12:56:14,254 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 51.43 sec
2018-05-29 12:56:15,314 Stage-1 map = 70%,  reduce = 0%, Cumulative CPU 64.19 sec
2018-05-29 12:56:18,489 Stage-1 map = 90%,  reduce = 0%, Cumulative CPU 87.04 sec
2018-05-29 12:56:19,557 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 93.73 sec
2018-05-29 12:56:24,852 Stage-1 map = 100%,  reduce = 18%, Cumulative CPU 111.59 sec
2018-05-29 12:56:29,099 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 120.84 sec
2018-05-29 12:56:34,415 Stage-1 map = 100%,  reduce = 45%, Cumulative CPU 138.67 sec
2018-05-29 12:56:39,704 Stage-1 map = 100%,  reduce = 55%, Cumulative CPU 147.75 sec
2018-05-29 12:56:43,930 Stage-1 map = 100%,  reduce = 73%, Cumulative CPU 165.46 sec
2018-05-29 12:56:50,285 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 175.1 sec
2018-05-29 12:56:53,467 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 183.95 sec
2018-05-29 12:56:54,520 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 193.29 sec
MapReduce Total cumulative CPU time: 3 minutes 13 seconds 290 msec
Ended Job = job_1525741534203_6822
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6823, Tracking URL = http://master:10014/proxy/application_1525741534203_6823/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6823
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2018-05-29 12:57:02,562 Stage-5 map = 0%,  reduce = 0%
2018-05-29 12:57:09,909 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 3.29 sec
2018-05-29 12:57:18,291 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 8.39 sec
MapReduce Total cumulative CPU time: 8 seconds 390 msec
Ended Job = job_1525741534203_6823
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6824, Tracking URL = http://master:10014/proxy/application_1525741534203_6824/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6824
Hadoop job information for Stage-6: number of mappers: 13; number of reducers: 14
2018-05-29 12:57:26,362 Stage-6 map = 0%,  reduce = 0%
2018-05-29 12:57:35,820 Stage-6 map = 8%,  reduce = 0%, Cumulative CPU 7.35 sec
2018-05-29 12:57:36,870 Stage-6 map = 23%,  reduce = 0%, Cumulative CPU 20.67 sec
2018-05-29 12:57:37,918 Stage-6 map = 46%,  reduce = 0%, Cumulative CPU 43.43 sec
2018-05-29 12:57:38,979 Stage-6 map = 62%,  reduce = 0%, Cumulative CPU 56.95 sec
2018-05-29 12:57:44,248 Stage-6 map = 69%,  reduce = 0%, Cumulative CPU 62.75 sec
2018-05-29 12:57:45,299 Stage-6 map = 77%,  reduce = 0%, Cumulative CPU 68.75 sec
2018-05-29 12:57:46,350 Stage-6 map = 92%,  reduce = 0%, Cumulative CPU 82.61 sec
2018-05-29 12:57:47,399 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 91.07 sec
2018-05-29 12:57:50,550 Stage-6 map = 100%,  reduce = 7%, Cumulative CPU 96.46 sec
2018-05-29 12:57:52,651 Stage-6 map = 100%,  reduce = 21%, Cumulative CPU 106.89 sec
2018-05-29 12:57:57,920 Stage-6 map = 100%,  reduce = 29%, Cumulative CPU 112.38 sec
2018-05-29 12:58:00,023 Stage-6 map = 100%,  reduce = 43%, Cumulative CPU 122.64 sec
2018-05-29 12:58:05,286 Stage-6 map = 100%,  reduce = 50%, Cumulative CPU 128.13 sec
2018-05-29 12:58:07,390 Stage-6 map = 100%,  reduce = 64%, Cumulative CPU 138.4 sec
2018-05-29 12:58:12,651 Stage-6 map = 100%,  reduce = 71%, Cumulative CPU 143.78 sec
2018-05-29 12:58:13,702 Stage-6 map = 100%,  reduce = 79%, Cumulative CPU 148.92 sec
2018-05-29 12:58:14,753 Stage-6 map = 100%,  reduce = 86%, Cumulative CPU 154.23 sec
2018-05-29 12:58:18,960 Stage-6 map = 100%,  reduce = 93%, Cumulative CPU 159.7 sec
2018-05-29 12:58:21,057 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 164.98 sec
MapReduce Total cumulative CPU time: 2 minutes 44 seconds 980 msec
Ended Job = job_1525741534203_6824
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6825, Tracking URL = http://master:10014/proxy/application_1525741534203_6825/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6825
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 12:58:29,162 Stage-2 map = 0%,  reduce = 0%
2018-05-29 12:58:36,569 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 3.07 sec
2018-05-29 12:58:37,629 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 8.45 sec
2018-05-29 12:58:43,945 Stage-2 map = 75%,  reduce = 0%, Cumulative CPU 22.02 sec
2018-05-29 12:58:44,997 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 37.03 sec
2018-05-29 12:58:54,524 Stage-2 map = 100%,  reduce = 58%, Cumulative CPU 64.43 sec
2018-05-29 12:58:56,643 Stage-2 map = 100%,  reduce = 62%, Cumulative CPU 66.83 sec
2018-05-29 12:58:57,707 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 70.51 sec
2018-05-29 12:59:00,873 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 87.8 sec
MapReduce Total cumulative CPU time: 1 minutes 27 seconds 800 msec
Ended Job = job_1525741534203_6825
Launching Job 5 out of 5
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6826, Tracking URL = http://master:10014/proxy/application_1525741534203_6826/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6826
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 3
2018-05-29 12:59:08,924 Stage-3 map = 0%,  reduce = 0%
2018-05-29 12:59:23,583 Stage-3 map = 33%,  reduce = 0%, Cumulative CPU 13.09 sec
2018-05-29 12:59:25,682 Stage-3 map = 67%,  reduce = 0%, Cumulative CPU 30.14 sec
2018-05-29 12:59:26,732 Stage-3 map = 93%,  reduce = 0%, Cumulative CPU 48.44 sec
2018-05-29 12:59:27,789 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 49.69 sec
2018-05-29 12:59:41,444 Stage-3 map = 100%,  reduce = 65%, Cumulative CPU 66.99 sec
2018-05-29 12:59:42,495 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 104.59 sec
MapReduce Total cumulative CPU time: 1 minutes 44 seconds 590 msec
Ended Job = job_1525741534203_6826
Loading data to table default.cert_dimension partition (ymd=2018-05-28)
Partition default.cert_dimension{ymd=2018-05-28} stats: [numFiles=3, numRows=553547, totalSize=720206051, rawDataSize=719652504]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 11   Cumulative CPU: 193.29 sec   HDFS Read: 2608943469 HDFS Write: 689878131 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 8.39 sec   HDFS Read: 8749872 HDFS Write: 8513003 SUCCESS
Stage-Stage-6: Map: 13  Reduce: 14   Cumulative CPU: 164.98 sec   HDFS Read: 3338253682 HDFS Write: 36300452 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 87.8 sec   HDFS Read: 698442819 HDFS Write: 702816864 SUCCESS
Stage-Stage-3: Map: 3  Reduce: 3   Cumulative CPU: 104.59 sec   HDFS Read: 739191676 HDFS Write: 720206363 SUCCESS
Total MapReduce CPU Time Spent: 9 minutes 19 seconds 50 msec
OK
Time taken: 235.795 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.669 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.07 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.601 seconds
Query ID = hadoop_20180529130000_746e0d5f-8db3-4af6-b828-e1626512df2b
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6827, Tracking URL = http://master:10014/proxy/application_1525741534203_6827/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6827
Hadoop job information for Stage-1: number of mappers: 15; number of reducers: 14
2018-05-29 13:00:26,952 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:00:43,986 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 16.69 sec
2018-05-29 13:00:45,044 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 65.94 sec
2018-05-29 13:00:46,114 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 100.92 sec
2018-05-29 13:00:47,169 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 139.96 sec
2018-05-29 13:00:48,223 Stage-1 map = 53%,  reduce = 0%, Cumulative CPU 149.26 sec
2018-05-29 13:00:53,498 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 150.78 sec
2018-05-29 13:01:02,992 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 167.55 sec
2018-05-29 13:01:04,044 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 216.16 sec
2018-05-29 13:01:05,110 Stage-1 map = 90%,  reduce = 1%, Cumulative CPU 242.08 sec
2018-05-29 13:01:06,172 Stage-1 map = 92%,  reduce = 1%, Cumulative CPU 244.45 sec
2018-05-29 13:01:07,230 Stage-1 map = 93%,  reduce = 1%, Cumulative CPU 246.62 sec
2018-05-29 13:01:11,455 Stage-1 map = 93%,  reduce = 2%, Cumulative CPU 247.26 sec
2018-05-29 13:01:12,514 Stage-1 map = 100%,  reduce = 2%, Cumulative CPU 252.9 sec
2018-05-29 13:01:14,632 Stage-1 map = 100%,  reduce = 9%, Cumulative CPU 257.62 sec
2018-05-29 13:01:16,748 Stage-1 map = 100%,  reduce = 12%, Cumulative CPU 261.84 sec
2018-05-29 13:01:19,920 Stage-1 map = 100%,  reduce = 19%, Cumulative CPU 264.71 sec
2018-05-29 13:01:28,361 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 290.93 sec
2018-05-29 13:01:29,422 Stage-1 map = 100%,  reduce = 34%, Cumulative CPU 307.05 sec
2018-05-29 13:01:33,657 Stage-1 map = 100%,  reduce = 42%, Cumulative CPU 311.0 sec
2018-05-29 13:01:39,983 Stage-1 map = 100%,  reduce = 49%, Cumulative CPU 323.06 sec
2018-05-29 13:01:41,035 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 330.96 sec
2018-05-29 13:01:46,323 Stage-1 map = 100%,  reduce = 56%, Cumulative CPU 352.16 sec
2018-05-29 13:01:52,727 Stage-1 map = 100%,  reduce = 64%, Cumulative CPU 364.53 sec
2018-05-29 13:01:56,943 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 376.18 sec
2018-05-29 13:02:02,188 Stage-1 map = 100%,  reduce = 74%, Cumulative CPU 392.28 sec
2018-05-29 13:02:03,239 Stage-1 map = 100%,  reduce = 76%, Cumulative CPU 404.07 sec
2018-05-29 13:02:08,501 Stage-1 map = 100%,  reduce = 83%, Cumulative CPU 408.56 sec
2018-05-29 13:02:12,692 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 425.51 sec
2018-05-29 13:02:13,740 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 427.69 sec
2018-05-29 13:02:38,912 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 462.24 sec
2018-05-29 13:03:39,720 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 529.78 sec
2018-05-29 13:04:12,115 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 566.76 sec
MapReduce Total cumulative CPU time: 9 minutes 26 seconds 760 msec
Ended Job = job_1525741534203_6827
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6828, Tracking URL = http://master:10014/proxy/application_1525741534203_6828/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6828
Hadoop job information for Stage-2: number of mappers: 20; number of reducers: 21
2018-05-29 13:04:20,289 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:04:37,232 Stage-2 map = 4%,  reduce = 0%, Cumulative CPU 36.44 sec
2018-05-29 13:04:38,287 Stage-2 map = 7%,  reduce = 0%, Cumulative CPU 89.99 sec
2018-05-29 13:04:39,334 Stage-2 map = 10%,  reduce = 0%, Cumulative CPU 126.14 sec
2018-05-29 13:04:40,383 Stage-2 map = 12%,  reduce = 0%, Cumulative CPU 144.23 sec
2018-05-29 13:04:43,536 Stage-2 map = 16%,  reduce = 0%, Cumulative CPU 158.92 sec
2018-05-29 13:04:44,583 Stage-2 map = 22%,  reduce = 0%, Cumulative CPU 190.75 sec
2018-05-29 13:04:45,632 Stage-2 map = 26%,  reduce = 0%, Cumulative CPU 207.78 sec
2018-05-29 13:04:47,735 Stage-2 map = 29%,  reduce = 0%, Cumulative CPU 215.48 sec
2018-05-29 13:04:48,790 Stage-2 map = 31%,  reduce = 0%, Cumulative CPU 218.83 sec
2018-05-29 13:04:49,841 Stage-2 map = 39%,  reduce = 0%, Cumulative CPU 241.07 sec
2018-05-29 13:04:52,985 Stage-2 map = 40%,  reduce = 0%, Cumulative CPU 245.75 sec
2018-05-29 13:04:55,081 Stage-2 map = 45%,  reduce = 0%, Cumulative CPU 252.12 sec
2018-05-29 13:04:56,129 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 258.35 sec
2018-05-29 13:04:57,177 Stage-2 map = 65%,  reduce = 0%, Cumulative CPU 279.46 sec
2018-05-29 13:04:58,234 Stage-2 map = 70%,  reduce = 0%, Cumulative CPU 286.67 sec
2018-05-29 13:05:03,477 Stage-2 map = 75%,  reduce = 0%, Cumulative CPU 293.74 sec
2018-05-29 13:05:05,578 Stage-2 map = 90%,  reduce = 0%, Cumulative CPU 314.35 sec
2018-05-29 13:05:10,824 Stage-2 map = 90%,  reduce = 1%, Cumulative CPU 315.81 sec
2018-05-29 13:05:13,974 Stage-2 map = 95%,  reduce = 1%, Cumulative CPU 322.43 sec
2018-05-29 13:05:17,128 Stage-2 map = 95%,  reduce = 2%, Cumulative CPU 322.52 sec
2018-05-29 13:05:20,292 Stage-2 map = 97%,  reduce = 3%, Cumulative CPU 340.9 sec
2018-05-29 13:05:26,591 Stage-2 map = 99%,  reduce = 3%, Cumulative CPU 348.86 sec
2018-05-29 13:05:28,687 Stage-2 map = 100%,  reduce = 3%, Cumulative CPU 350.96 sec
2018-05-29 13:05:30,786 Stage-2 map = 100%,  reduce = 6%, Cumulative CPU 353.63 sec
2018-05-29 13:05:31,852 Stage-2 map = 100%,  reduce = 7%, Cumulative CPU 357.62 sec
2018-05-29 13:05:35,002 Stage-2 map = 100%,  reduce = 9%, Cumulative CPU 368.09 sec
2018-05-29 13:05:36,049 Stage-2 map = 100%,  reduce = 10%, Cumulative CPU 378.48 sec
2018-05-29 13:05:41,269 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 395.4 sec
2018-05-29 13:05:42,321 Stage-2 map = 100%,  reduce = 12%, Cumulative CPU 402.68 sec
2018-05-29 13:05:47,557 Stage-2 map = 100%,  reduce = 13%, Cumulative CPU 416.25 sec
2018-05-29 13:05:48,602 Stage-2 map = 100%,  reduce = 14%, Cumulative CPU 429.32 sec
2018-05-29 13:06:04,301 Stage-2 map = 100%,  reduce = 18%, Cumulative CPU 447.18 sec
2018-05-29 13:06:05,341 Stage-2 map = 100%,  reduce = 26%, Cumulative CPU 482.73 sec
2018-05-29 13:06:10,570 Stage-2 map = 100%,  reduce = 27%, Cumulative CPU 489.58 sec
2018-05-29 13:06:11,615 Stage-2 map = 100%,  reduce = 28%, Cumulative CPU 501.59 sec
2018-05-29 13:06:14,742 Stage-2 map = 100%,  reduce = 29%, Cumulative CPU 508.87 sec
2018-05-29 13:06:28,322 Stage-2 map = 100%,  reduce = 33%, Cumulative CPU 526.52 sec
2018-05-29 13:06:30,408 Stage-2 map = 100%,  reduce = 40%, Cumulative CPU 562.28 sec
2018-05-29 13:06:34,586 Stage-2 map = 100%,  reduce = 41%, Cumulative CPU 568.86 sec
2018-05-29 13:06:36,677 Stage-2 map = 100%,  reduce = 43%, Cumulative CPU 583.28 sec
2018-05-29 13:06:51,321 Stage-2 map = 100%,  reduce = 47%, Cumulative CPU 601.98 sec
2018-05-29 13:06:53,414 Stage-2 map = 100%,  reduce = 54%, Cumulative CPU 637.41 sec
2018-05-29 13:06:57,607 Stage-2 map = 100%,  reduce = 55%, Cumulative CPU 643.77 sec
2018-05-29 13:06:59,696 Stage-2 map = 100%,  reduce = 57%, Cumulative CPU 659.32 sec
2018-05-29 13:07:15,373 Stage-2 map = 100%,  reduce = 61%, Cumulative CPU 681.16 sec
2018-05-29 13:07:17,457 Stage-2 map = 100%,  reduce = 65%, Cumulative CPU 699.14 sec
2018-05-29 13:07:18,499 Stage-2 map = 100%,  reduce = 69%, Cumulative CPU 715.92 sec
2018-05-29 13:07:22,678 Stage-2 map = 100%,  reduce = 70%, Cumulative CPU 724.33 sec
2018-05-29 13:07:25,806 Stage-2 map = 100%,  reduce = 71%, Cumulative CPU 741.03 sec
2018-05-29 13:07:39,361 Stage-2 map = 100%,  reduce = 75%, Cumulative CPU 772.78 sec
2018-05-29 13:07:42,494 Stage-2 map = 100%,  reduce = 79%, Cumulative CPU 797.23 sec
2018-05-29 13:07:45,620 Stage-2 map = 100%,  reduce = 80%, Cumulative CPU 806.52 sec
2018-05-29 13:07:48,760 Stage-2 map = 100%,  reduce = 81%, Cumulative CPU 815.1 sec
2018-05-29 13:08:01,299 Stage-2 map = 100%,  reduce = 85%, Cumulative CPU 834.06 sec
2018-05-29 13:08:03,384 Stage-2 map = 100%,  reduce = 89%, Cumulative CPU 851.4 sec
2018-05-29 13:08:06,521 Stage-2 map = 100%,  reduce = 93%, Cumulative CPU 869.19 sec
2018-05-29 13:08:08,610 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 877.01 sec
2018-05-29 13:08:12,786 Stage-2 map = 100%,  reduce = 95%, Cumulative CPU 891.48 sec
2018-05-29 13:08:25,320 Stage-2 map = 100%,  reduce = 99%, Cumulative CPU 912.0 sec
2018-05-29 13:08:35,762 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 924.29 sec
MapReduce Total cumulative CPU time: 15 minutes 24 seconds 290 msec
Ended Job = job_1525741534203_6828
Loading data to table default.cert_use_info partition (ymd=2018-05-28)
Partition default.cert_use_info{ymd=2018-05-28} stats: [numFiles=21, numRows=16305363, totalSize=2043331015, rawDataSize=2027025652]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 15  Reduce: 14   Cumulative CPU: 566.76 sec   HDFS Read: 3504822088 HDFS Write: 2443966151 SUCCESS
Stage-Stage-2: Map: 20  Reduce: 21   Cumulative CPU: 924.29 sec   HDFS Read: 5287523941 HDFS Write: 2043333159 SUCCESS
Total MapReduce CPU Time Spent: 24 minutes 51 seconds 50 msec
OK
Time taken: 502.262 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.594 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.922 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.541 seconds
Query ID = hadoop_20180529130909_9d02336d-842d-4aac-96a8-51fb0322b9e7
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6829, Tracking URL = http://master:10014/proxy/application_1525741534203_6829/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6829
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-05-29 13:09:17,490 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:09:24,053 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.35 sec
MapReduce Total cumulative CPU time: 2 seconds 350 msec
Ended Job = job_1525741534203_6829
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/client_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-09-08_075_7337822253707078040-1/-ext-10000
Loading data to table default.client_dimension partition (ymd=2018-05-28)
Partition default.client_dimension{ymd=2018-05-28} stats: [numFiles=1, numRows=4, totalSize=611, rawDataSize=607]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.35 sec   HDFS Read: 6489 HDFS Write: 707 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 350 msec
OK
Time taken: 17.898 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.66 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.059 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.711 seconds
Query ID = hadoop_20180529130909_49a4e514-bd04-4405-a627-2b9736adc4e2
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6830, Tracking URL = http://master:10014/proxy/application_1525741534203_6830/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6830
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
2018-05-29 13:10:06,131 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:10:19,875 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 10.95 sec
2018-05-29 13:10:23,045 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 28.04 sec
2018-05-29 13:10:26,202 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 31.3 sec
MapReduce Total cumulative CPU time: 31 seconds 300 msec
Ended Job = job_1525741534203_6830
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/company_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-09-56_547_4259051067189737162-1/-ext-10000
Loading data to table default.company_dimension partition (ymd=2018-05-28)
Partition default.company_dimension{ymd=2018-05-28} stats: [numFiles=2, numRows=1296062, totalSize=269659603, rawDataSize=268363541]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2   Cumulative CPU: 31.3 sec   HDFS Read: 306539645 HDFS Write: 269659816 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 300 msec
OK
Time taken: 31.636 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.505 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.928 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.919 seconds
Query ID = hadoop_20180529131010_a2a521ef-c385-42f1-9d75-cbce161db2bc
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6831, Tracking URL = http://master:10014/proxy/application_1525741534203_6831/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6831
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
2018-05-29 13:11:08,298 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:11:31,541 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 44.17 sec
2018-05-29 13:11:37,865 Stage-1 map = 55%,  reduce = 0%, Cumulative CPU 57.78 sec
2018-05-29 13:11:42,079 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 62.58 sec
2018-05-29 13:11:48,404 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 73.96 sec
MapReduce Total cumulative CPU time: 1 minutes 13 seconds 960 msec
Ended Job = job_1525741534203_6831
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/customer_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-10-58_579_7813754884224198720-1/-ext-10000
Loading data to table default.customer_dimension partition (ymd=2018-05-28)
Partition default.customer_dimension{ymd=2018-05-28} stats: [numFiles=2, numRows=745158, totalSize=466503624, rawDataSize=465758466]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2   Cumulative CPU: 73.96 sec   HDFS Read: 494853609 HDFS Write: 466503840 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 13 seconds 960 msec
OK
Time taken: 51.948 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.736 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.359 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.55 seconds
Query ID = hadoop_20180529131212_0d492d6c-bc89-4a87-890d-1520b0255c18
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6832, Tracking URL = http://master:10014/proxy/application_1525741534203_6832/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6832
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2018-05-29 13:12:32,676 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:12:40,093 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.01 sec
2018-05-29 13:12:47,463 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.02 sec
MapReduce Total cumulative CPU time: 7 seconds 20 msec
Ended Job = job_1525741534203_6832
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6833, Tracking URL = http://master:10014/proxy/application_1525741534203_6833/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6833
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2018-05-29 13:12:55,645 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:13:01,964 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 1.48 sec
2018-05-29 13:13:03,018 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.33 sec
2018-05-29 13:13:10,407 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.96 sec
MapReduce Total cumulative CPU time: 8 seconds 960 msec
Ended Job = job_1525741534203_6833
Loading data to table default.evidence_info partition (ymd=2018-05-28)
Partition default.evidence_info{ymd=2018-05-28} stats: [numFiles=1, numRows=1645, totalSize=4971877, rawDataSize=4970232]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 7.02 sec   HDFS Read: 8926971 HDFS Write: 5045829 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 8.96 sec   HDFS Read: 5066188 HDFS Write: 4971976 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 980 msec
OK
Time taken: 51.136 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.646 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.941 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.63 seconds
Query ID = hadoop_20180529131313_af553ded-f44e-49bc-a4b4-aed188dcfcb5
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6834, Tracking URL = http://master:10014/proxy/application_1525741534203_6834/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6834
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 0
2018-05-29 13:13:52,248 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:14:06,026 Stage-1 map = 21%,  reduce = 0%, Cumulative CPU 34.26 sec
2018-05-29 13:14:07,079 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 46.02 sec
2018-05-29 13:14:08,136 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 92.07 sec
2018-05-29 13:14:14,462 Stage-1 map = 64%,  reduce = 0%, Cumulative CPU 96.27 sec
2018-05-29 13:14:17,635 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 130.5 sec
2018-05-29 13:14:18,693 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 142.16 sec
2018-05-29 13:14:23,979 Stage-1 map = 96%,  reduce = 0%, Cumulative CPU 153.47 sec
2018-05-29 13:14:27,145 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 154.22 sec
MapReduce Total cumulative CPU time: 2 minutes 34 seconds 220 msec
Ended Job = job_1525741534203_6834
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/i_xin_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-13-42_663_3091001441682494297-1/-ext-10000
Loading data to table default.i_xin_dimension partition (ymd=2018-05-28)
Partition default.i_xin_dimension{ymd=2018-05-28} stats: [numFiles=14, numRows=16305363, totalSize=1610367926, rawDataSize=1594062563]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 14   Cumulative CPU: 154.22 sec   HDFS Read: 3504699017 HDFS Write: 1610369406 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 34 seconds 220 msec
OK
Time taken: 46.448 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.548 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.968 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.694 seconds
Query ID = hadoop_20180529131414_bc348c4c-23e6-4dcb-a506-9d07cc8d967d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6835, Tracking URL = http://master:10014/proxy/application_1525741534203_6835/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6835
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2018-05-29 13:15:11,403 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:15:17,800 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 4.99 sec
2018-05-29 13:15:18,856 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 11.04 sec
2018-05-29 13:15:25,218 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 14.12 sec
MapReduce Total cumulative CPU time: 14 seconds 120 msec
Ended Job = job_1525741534203_6835
Loading data to table default.project_dimension partition (ymd=2018-05-28)
Partition default.project_dimension{ymd=2018-05-28} stats: [numFiles=1, numRows=159, totalSize=38675, rawDataSize=38516]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 4  Reduce: 1   Cumulative CPU: 14.12 sec   HDFS Read: 110010 HDFS Write: 38775 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 120 msec
OK
Time taken: 27.499 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.204 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.901 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.514 seconds
Query ID = hadoop_20180529131515_065ae8f8-1414-45b0-80fa-994d89e83196
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6836, Tracking URL = http://master:10014/proxy/application_1525741534203_6836/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6836
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-05-29 13:16:05,669 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:16:13,095 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.96 sec
MapReduce Total cumulative CPU time: 1 seconds 960 msec
Ended Job = job_1525741534203_6836
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/s_d_qzcs_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-15-57_379_1712261242148697812-1/-ext-10000
Loading data to table default.s_d_qzcs_dimension partition (ymd=2018-05-28)
Partition default.s_d_qzcs_dimension{ymd=2018-05-28} stats: [numFiles=1, numRows=2, totalSize=52, rawDataSize=50]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.96 sec   HDFS Read: 4302 HDFS Write: 149 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 960 msec
OK
Time taken: 17.64 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.713 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 1.043 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.532 seconds
Query ID = hadoop_20180529131616_7a8142a2-14a9-4fde-91dc-e7d181f8e825
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6837, Tracking URL = http://master:10014/proxy/application_1525741534203_6837/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6837
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2018-05-29 13:16:56,936 Stage-3 map = 0%,  reduce = 0%
2018-05-29 13:17:04,361 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 10.05 sec
2018-05-29 13:17:12,789 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 14.65 sec
MapReduce Total cumulative CPU time: 14 seconds 650 msec
Ended Job = job_1525741534203_6837
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6838, Tracking URL = http://master:10014/proxy/application_1525741534203_6838/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6838
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2018-05-29 13:17:20,944 Stage-4 map = 0%,  reduce = 0%
2018-05-29 13:17:28,311 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.13 sec
2018-05-29 13:17:36,750 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 8.11 sec
MapReduce Total cumulative CPU time: 8 seconds 110 msec
Ended Job = job_1525741534203_6838
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6839, Tracking URL = http://master:10014/proxy/application_1525741534203_6839/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6839
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2018-05-29 13:17:44,825 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:17:51,142 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 1.5 sec
2018-05-29 13:17:52,203 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 4.83 sec
2018-05-29 13:17:53,260 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.67 sec
2018-05-29 13:18:00,653 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.69 sec
MapReduce Total cumulative CPU time: 15 seconds 690 msec
Ended Job = job_1525741534203_6839
Loading data to table default.s_eseal_info partition (ymd=2018-05-28)
Partition default.s_eseal_info{ymd=2018-05-28} stats: [numFiles=1, numRows=5705, totalSize=2006871, rawDataSize=2001166]
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 2  Reduce: 1   Cumulative CPU: 14.65 sec   HDFS Read: 9427839 HDFS Write: 486852 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 8.11 sec   HDFS Read: 495073 HDFS Write: 380110 SUCCESS
Stage-Stage-1: Map: 3  Reduce: 1   Cumulative CPU: 15.69 sec   HDFS Read: 69509410 HDFS Write: 2006969 SUCCESS
Total MapReduce CPU Time Spent: 38 seconds 450 msec
OK
Time taken: 77.135 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.694 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.953 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.511 seconds
Query ID = hadoop_20180529131818_d6e11f6d-acb5-41f5-94bd-941eaf2b1cd9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6840, Tracking URL = http://master:10014/proxy/application_1525741534203_6840/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6840
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2018-05-29 13:18:42,897 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:18:49,309 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.61 sec
2018-05-29 13:18:55,048 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.06 sec
2018-05-29 13:19:02,449 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 21.53 sec
MapReduce Total cumulative CPU time: 21 seconds 530 msec
Ended Job = job_1525741534203_6840
Loading data to table default.sign_info partition (ymd=2018-05-28)
Partition default.sign_info{ymd=2018-05-28} stats: [numFiles=1, numRows=90652, totalSize=16764294, rawDataSize=16673642]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 21.53 sec   HDFS Read: 219466434 HDFS Write: 16764391 SUCCESS
Total MapReduce CPU Time Spent: 21 seconds 530 msec
OK
Time taken: 36.62 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.51 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.902 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.574 seconds
Query ID = hadoop_20180529131919_973033f5-6a90-4a83-bf23-c37b9b5d0778
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6841, Tracking URL = http://master:10014/proxy/application_1525741534203_6841/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6841
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-05-29 13:19:49,269 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:19:56,691 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.47 sec
MapReduce Total cumulative CPU time: 2 seconds 470 msec
Ended Job = job_1525741534203_6841
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/sys_office_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-19-39_752_7537001717626743252-1/-ext-10000
Loading data to table default.sys_office_dimension partition (ymd=2018-05-28)
Partition default.sys_office_dimension{ymd=2018-05-28} stats: [numFiles=1, numRows=47, totalSize=8362, rawDataSize=8315]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.47 sec   HDFS Read: 14545 HDFS Write: 8463 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 470 msec
OK
Time taken: 18.957 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.474 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.925 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.604 seconds
Query ID = hadoop_20180529132020_460ced44-b4ca-4914-9c30-b7efb5bdc757
Total jobs = 5
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6842, Tracking URL = http://master:10014/proxy/application_1525741534203_6842/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6842
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2018-05-29 13:20:38,471 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:20:46,987 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 3.64 sec
2018-05-29 13:20:48,047 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 11.94 sec
2018-05-29 13:20:49,108 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 36.82 sec
2018-05-29 13:20:50,166 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 45.01 sec
2018-05-29 13:20:51,225 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 53.06 sec
2018-05-29 13:20:55,476 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 61.54 sec
2018-05-29 13:20:57,599 Stage-1 map = 100%,  reduce = 43%, Cumulative CPU 67.55 sec
2018-05-29 13:21:02,903 Stage-1 map = 100%,  reduce = 57%, Cumulative CPU 71.9 sec
2018-05-29 13:21:03,962 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 82.79 sec
2018-05-29 13:21:10,319 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 87.29 sec
MapReduce Total cumulative CPU time: 1 minutes 27 seconds 290 msec
Ended Job = job_1525741534203_6842
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6843, Tracking URL = http://master:10014/proxy/application_1525741534203_6843/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6843
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 13:21:18,496 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:21:24,817 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.43 sec
2018-05-29 13:21:32,212 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.71 sec
MapReduce Total cumulative CPU time: 3 seconds 710 msec
Ended Job = job_1525741534203_6843
Launching Job 3 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6844, Tracking URL = http://master:10014/proxy/application_1525741534203_6844/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6844
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 0
2018-05-29 13:21:40,321 Stage-3 map = 0%,  reduce = 0%
2018-05-29 13:21:47,694 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 5.27 sec
MapReduce Total cumulative CPU time: 5 seconds 270 msec
Ended Job = job_1525741534203_6844
Stage-6 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.
Stage-7 is filtered out by condition resolver.
Launching Job 5 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6845, Tracking URL = http://master:10014/proxy/application_1525741534203_6845/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6845
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 0
2018-05-29 13:21:56,749 Stage-5 map = 0%,  reduce = 0%
2018-05-29 13:22:03,065 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.96 sec
MapReduce Total cumulative CPU time: 1 seconds 960 msec
Ended Job = job_1525741534203_6845
Loading data to table default.sys_user_dimension partition (ymd=2018-05-28)
Partition default.sys_user_dimension{ymd=2018-05-28} stats: [numFiles=1, numRows=176, totalSize=27184, rawDataSize=27008]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 87.29 sec   HDFS Read: 1588750251 HDFS Write: 3005 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.71 sec   HDFS Read: 9535 HDFS Write: 4339 SUCCESS
Stage-Stage-3: Map: 2   Cumulative CPU: 5.27 sec   HDFS Read: 59012 HDFS Write: 27384 SUCCESS
Stage-Stage-5: Map: 1   Cumulative CPU: 1.96 sec   HDFS Read: 30524 HDFS Write: 27184 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 38 seconds 230 msec
OK
Time taken: 96.242 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.524 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.951 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.741 seconds
Query ID = hadoop_20180529132222_e63b6fd7-bc33-43cb-8e02-91f9977d2787
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6846, Tracking URL = http://master:10014/proxy/application_1525741534203_6846/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6846
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
2018-05-29 13:22:45,124 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:23:04,116 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 32.63 sec
2018-05-29 13:23:08,338 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 39.01 sec
MapReduce Total cumulative CPU time: 39 seconds 10 msec
Ended Job = job_1525741534203_6846
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/user_dimension/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-22-35_574_4557737105043189766-1/-ext-10000
Loading data to table default.user_dimension partition (ymd=2018-05-28)
Partition default.user_dimension{ymd=2018-05-28} stats: [numFiles=2, numRows=1596375, totalSize=161374607, rawDataSize=159778232]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2   Cumulative CPU: 39.01 sec   HDFS Read: 152389209 HDFS Write: 161374813 SUCCESS
Total MapReduce CPU Time Spent: 39 seconds 10 msec
OK
Time taken: 34.699 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.569 seconds
Dropped the partition ymd=2018-05-27
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.932 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.576 seconds
Query ID = hadoop_20180529132323_fd2336eb-ac5d-4881-aef2-83d4013878d2
Total jobs = 11
Launching Job 1 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6847, Tracking URL = http://master:10014/proxy/application_1525741534203_6847/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6847
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 1
2018-05-29 13:23:52,351 Stage-10 map = 0%,  reduce = 0%
2018-05-29 13:23:59,750 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 3.34 sec
2018-05-29 13:24:07,137 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 6.0 sec
MapReduce Total cumulative CPU time: 6 seconds 0 msec
Ended Job = job_1525741534203_6847
Launching Job 2 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6848, Tracking URL = http://master:10014/proxy/application_1525741534203_6848/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6848
Hadoop job information for Stage-11: number of mappers: 1; number of reducers: 1
2018-05-29 13:24:15,259 Stage-11 map = 0%,  reduce = 0%
2018-05-29 13:24:27,870 Stage-11 map = 100%,  reduce = 0%, Cumulative CPU 10.1 sec
2018-05-29 13:24:39,464 Stage-11 map = 100%,  reduce = 100%, Cumulative CPU 19.48 sec
MapReduce Total cumulative CPU time: 19 seconds 480 msec
Ended Job = job_1525741534203_6848
Launching Job 3 out of 11
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6849, Tracking URL = http://master:10014/proxy/application_1525741534203_6849/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6849
Hadoop job information for Stage-12: number of mappers: 3; number of reducers: 2
2018-05-29 13:24:47,497 Stage-12 map = 0%,  reduce = 0%
2018-05-29 13:24:54,844 Stage-12 map = 33%,  reduce = 0%, Cumulative CPU 3.19 sec
2018-05-29 13:25:01,167 Stage-12 map = 67%,  reduce = 0%, Cumulative CPU 13.66 sec
2018-05-29 13:25:05,381 Stage-12 map = 78%,  reduce = 0%, Cumulative CPU 28.95 sec
2018-05-29 13:25:12,744 Stage-12 map = 78%,  reduce = 22%, Cumulative CPU 36.36 sec
2018-05-29 13:25:15,893 Stage-12 map = 100%,  reduce = 22%, Cumulative CPU 42.89 sec
2018-05-29 13:25:19,063 Stage-12 map = 100%,  reduce = 83%, Cumulative CPU 50.96 sec
2018-05-29 13:25:24,334 Stage-12 map = 100%,  reduce = 95%, Cumulative CPU 60.37 sec
2018-05-29 13:25:26,437 Stage-12 map = 100%,  reduce = 100%, Cumulative CPU 63.42 sec
MapReduce Total cumulative CPU time: 1 minutes 3 seconds 420 msec
Ended Job = job_1525741534203_6849
Launching Job 4 out of 11
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6850, Tracking URL = http://master:10014/proxy/application_1525741534203_6850/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6850
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 7
2018-05-29 13:25:34,500 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:25:42,905 Stage-1 map = 14%,  reduce = 0%, Cumulative CPU 1.54 sec
2018-05-29 13:25:43,954 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 20.97 sec
2018-05-29 13:25:45,007 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 27.06 sec
2018-05-29 13:25:46,059 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 35.86 sec
2018-05-29 13:25:47,114 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 42.68 sec
2018-05-29 13:25:50,269 Stage-1 map = 100%,  reduce = 14%, Cumulative CPU 45.87 sec
2018-05-29 13:25:51,321 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 51.78 sec
2018-05-29 13:25:52,374 Stage-1 map = 100%,  reduce = 43%, Cumulative CPU 56.64 sec
2018-05-29 13:25:57,646 Stage-1 map = 100%,  reduce = 57%, Cumulative CPU 61.74 sec
2018-05-29 13:26:00,797 Stage-1 map = 100%,  reduce = 71%, Cumulative CPU 68.62 sec
2018-05-29 13:26:06,069 Stage-1 map = 100%,  reduce = 86%, Cumulative CPU 76.19 sec
2018-05-29 13:26:07,121 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 92.97 sec
2018-05-29 13:26:08,177 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 95.31 sec
MapReduce Total cumulative CPU time: 1 minutes 35 seconds 310 msec
Ended Job = job_1525741534203_6850
Launching Job 5 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6851, Tracking URL = http://master:10014/proxy/application_1525741534203_6851/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6851
Hadoop job information for Stage-13: number of mappers: 2; number of reducers: 1
2018-05-29 13:26:16,185 Stage-13 map = 0%,  reduce = 0%
2018-05-29 13:26:22,485 Stage-13 map = 50%,  reduce = 0%, Cumulative CPU 1.58 sec
2018-05-29 13:26:34,050 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 18.53 sec
2018-05-29 13:26:40,357 Stage-13 map = 100%,  reduce = 76%, Cumulative CPU 27.23 sec
2018-05-29 13:26:45,623 Stage-13 map = 100%,  reduce = 100%, Cumulative CPU 34.79 sec
MapReduce Total cumulative CPU time: 34 seconds 790 msec
Ended Job = job_1525741534203_6851
Launching Job 6 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6852, Tracking URL = http://master:10014/proxy/application_1525741534203_6852/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6852
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2018-05-29 13:26:53,731 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:27:02,112 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 5.38 sec
2018-05-29 13:27:10,487 Stage-2 map = 98%,  reduce = 0%, Cumulative CPU 23.43 sec
2018-05-29 13:27:11,548 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 23.84 sec
2018-05-29 13:27:19,951 Stage-2 map = 100%,  reduce = 71%, Cumulative CPU 38.49 sec
2018-05-29 13:27:26,246 Stage-2 map = 100%,  reduce = 78%, Cumulative CPU 45.16 sec
2018-05-29 13:27:32,531 Stage-2 map = 100%,  reduce = 86%, Cumulative CPU 51.68 sec
2018-05-29 13:27:38,823 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 58.2 sec
2018-05-29 13:27:44,060 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 63.58 sec
MapReduce Total cumulative CPU time: 1 minutes 3 seconds 580 msec
Ended Job = job_1525741534203_6852
Launching Job 7 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6853, Tracking URL = http://master:10014/proxy/application_1525741534203_6853/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6853
Hadoop job information for Stage-14: number of mappers: 2; number of reducers: 1
2018-05-29 13:27:52,073 Stage-14 map = 0%,  reduce = 0%
2018-05-29 13:27:59,480 Stage-14 map = 50%,  reduce = 0%, Cumulative CPU 3.14 sec
2018-05-29 13:28:09,970 Stage-14 map = 100%,  reduce = 0%, Cumulative CPU 20.6 sec
2018-05-29 13:28:17,332 Stage-14 map = 100%,  reduce = 78%, Cumulative CPU 31.05 sec
2018-05-29 13:28:22,589 Stage-14 map = 100%,  reduce = 100%, Cumulative CPU 38.24 sec
MapReduce Total cumulative CPU time: 38 seconds 240 msec
Ended Job = job_1525741534203_6853
Launching Job 8 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6854, Tracking URL = http://master:10014/proxy/application_1525741534203_6854/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6854
Hadoop job information for Stage-15: number of mappers: 2; number of reducers: 1
2018-05-29 13:28:30,657 Stage-15 map = 0%,  reduce = 0%
2018-05-29 13:28:36,958 Stage-15 map = 50%,  reduce = 0%, Cumulative CPU 3.14 sec
2018-05-29 13:28:48,517 Stage-15 map = 100%,  reduce = 0%, Cumulative CPU 20.88 sec
2018-05-29 13:28:54,833 Stage-15 map = 100%,  reduce = 68%, Cumulative CPU 31.84 sec
2018-05-29 13:29:01,144 Stage-15 map = 100%,  reduce = 75%, Cumulative CPU 39.91 sec
2018-05-29 13:29:06,396 Stage-15 map = 100%,  reduce = 86%, Cumulative CPU 46.64 sec
2018-05-29 13:29:18,999 Stage-15 map = 100%,  reduce = 91%, Cumulative CPU 60.88 sec
2018-05-29 13:29:25,297 Stage-15 map = 100%,  reduce = 98%, Cumulative CPU 67.78 sec
2018-05-29 13:29:27,394 Stage-15 map = 100%,  reduce = 100%, Cumulative CPU 71.21 sec
MapReduce Total cumulative CPU time: 1 minutes 11 seconds 210 msec
Ended Job = job_1525741534203_6854
Launching Job 9 out of 11
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6855, Tracking URL = http://master:10014/proxy/application_1525741534203_6855/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6855
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 0
2018-05-29 13:29:35,406 Stage-3 map = 0%,  reduce = 0%
2018-05-29 13:29:48,978 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 13.2 sec
2018-05-29 13:29:50,024 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 26.22 sec
MapReduce Total cumulative CPU time: 26 seconds 220 msec
Ended Job = job_1525741534203_6855
Stage-6 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Stage-7 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/scca/etl_hdfs/etl_db/work_deal_info/ymd=2018-05-28/.hive-staging_hive_2018-05-29_13-23-40_872_4192735266335420138-1/-ext-10000
Loading data to table default.work_deal_info partition (ymd=2018-05-28)
Partition default.work_deal_info{ymd=2018-05-28} stats: [numFiles=2, numRows=1309569, totalSize=326698793, rawDataSize=325389224]
MapReduce Jobs Launched: 
Stage-Stage-10: Map: 1  Reduce: 1   Cumulative CPU: 6.0 sec   HDFS Read: 48854 HDFS Write: 7675 SUCCESS
Stage-Stage-11: Map: 1  Reduce: 1   Cumulative CPU: 19.48 sec   HDFS Read: 67701664 HDFS Write: 6394724 SUCCESS
Stage-Stage-12: Map: 3  Reduce: 2   Cumulative CPU: 63.42 sec   HDFS Read: 363939150 HDFS Write: 117136645 SUCCESS
Stage-Stage-1: Map: 7  Reduce: 7   Cumulative CPU: 95.31 sec   HDFS Read: 1588725987 HDFS Write: 189166840 SUCCESS
Stage-Stage-13: Map: 2  Reduce: 1   Cumulative CPU: 34.79 sec   HDFS Read: 117196315 HDFS Write: 123260774 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 63.58 sec   HDFS Read: 195577205 HDFS Write: 151539307 SUCCESS
Stage-Stage-14: Map: 2  Reduce: 1   Cumulative CPU: 38.24 sec   HDFS Read: 123320580 HDFS Write: 131475156 SUCCESS
Stage-Stage-15: Map: 2  Reduce: 1   Cumulative CPU: 71.21 sec   HDFS Read: 131523680 HDFS Write: 172510985 SUCCESS
Stage-Stage-3: Map: 2   Cumulative CPU: 26.22 sec   HDFS Read: 324067970 HDFS Write: 326699001 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 58 seconds 250 msec
OK
Time taken: 371.054 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.45 seconds
Query ID = hadoop_20180529133030_c78c8a17-8ca6-43eb-a376-42433d0f7f5b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6856, Tracking URL = http://master:10014/proxy/application_1525741534203_6856/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6856
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 8
2018-05-29 13:30:16,852 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:30:26,394 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 2.36 sec
2018-05-29 13:30:29,569 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 12.93 sec
2018-05-29 13:30:30,634 Stage-1 map = 63%,  reduce = 0%, Cumulative CPU 45.14 sec
2018-05-29 13:30:31,705 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 76.63 sec
2018-05-29 13:30:37,017 Stage-1 map = 100%,  reduce = 38%, Cumulative CPU 83.87 sec
2018-05-29 13:30:42,311 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 86.16 sec
2018-05-29 13:30:43,373 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 91.05 sec
2018-05-29 13:30:48,675 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 95.89 sec
MapReduce Total cumulative CPU time: 1 minutes 35 seconds 890 msec
Ended Job = job_1525741534203_6856
Moving data to: hdfs://master:10000/user/hive/warehouse/scca_etl_test.db/cert_use_info_etl
Table scca_etl_test.cert_use_info_etl stats: [numFiles=8, numRows=1555, totalSize=103126, rawDataSize=101571]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 8   Cumulative CPU: 95.89 sec   HDFS Read: 2043577527 HDFS Write: 103843 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 35 seconds 890 msec
OK
Time taken: 44.139 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.287 seconds
Query ID = hadoop_20180529133737_cc8a0507-50d0-4dc9-adab-85d26940c6cd
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6861, Tracking URL = http://master:10014/proxy/application_1525741534203_6861/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6861
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2018-05-29 13:37:33,845 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:37:41,285 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.7 sec
2018-05-29 13:37:47,641 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.69 sec
MapReduce Total cumulative CPU time: 6 seconds 690 msec
Ended Job = job_1525741534203_6861
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6862, Tracking URL = http://master:10014/proxy/application_1525741534203_6862/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6862
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2018-05-29 13:37:58,060 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:38:04,511 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.72 sec
2018-05-29 13:38:10,845 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.06 sec
MapReduce Total cumulative CPU time: 7 seconds 60 msec
Ended Job = job_1525741534203_6862
Moving data to: hdfs://master:10000/user/hive/warehouse/scca_etl_test.db/evidence_info_etl
Table scca_etl_test.evidence_info_etl stats: [numFiles=1, numRows=159, totalSize=13245, rawDataSize=13086]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 6.69 sec   HDFS Read: 4992363 HDFS Write: 15707 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 7.06 sec   HDFS Read: 69668 HDFS Write: 13336 SUCCESS
Total MapReduce CPU Time Spent: 13 seconds 750 msec
OK
Time taken: 51.083 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.455 seconds
Query ID = hadoop_20180529134141_6a164df9-a515-4ff6-b0ed-a476c61984e3
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6869, Tracking URL = http://master:10014/proxy/application_1525741534203_6869/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6869
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 2
2018-05-29 13:41:56,969 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:42:05,449 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 5.92 sec
2018-05-29 13:42:10,726 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.78 sec
2018-05-29 13:42:12,865 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 21.74 sec
MapReduce Total cumulative CPU time: 21 seconds 740 msec
Ended Job = job_1525741534203_6869
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6870, Tracking URL = http://master:10014/proxy/application_1525741534203_6870/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6870
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2018-05-29 13:42:20,830 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:42:27,150 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 1.53 sec
2018-05-29 13:42:28,202 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.25 sec
2018-05-29 13:42:34,515 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.83 sec
MapReduce Total cumulative CPU time: 6 seconds 830 msec
Ended Job = job_1525741534203_6870
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6871, Tracking URL = http://master:10014/proxy/application_1525741534203_6871/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6871
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 2
2018-05-29 13:42:42,566 Stage-3 map = 0%,  reduce = 0%
2018-05-29 13:42:49,052 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 2.44 sec
2018-05-29 13:42:59,604 Stage-3 map = 95%,  reduce = 0%, Cumulative CPU 19.84 sec
2018-05-29 13:43:00,657 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 20.7 sec
2018-05-29 13:43:05,953 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 35.4 sec
MapReduce Total cumulative CPU time: 35 seconds 400 msec
Ended Job = job_1525741534203_6871
Moving data to: hdfs://master:10000/user/hive/warehouse/scca_etl_test.db/work_deal_info_etl
Table scca_etl_test.work_deal_info_etl stats: [numFiles=2, numRows=1322, totalSize=177689, rawDataSize=176367]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 2   Cumulative CPU: 21.74 sec   HDFS Read: 326952229 HDFS Write: 163878 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.83 sec   HDFS Read: 219013 HDFS Write: 156152 SUCCESS
Stage-Stage-3: Map: 2  Reduce: 2   Cumulative CPU: 35.4 sec   HDFS Read: 269943841 HDFS Write: 177874 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 3 seconds 970 msec
OK
Time taken: 82.171 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.512 seconds
Query ID = hadoop_20180529134848_20a5d0d5-9be9-436c-9fcb-9e4e5bc42bfb
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1525741534203_6881, Tracking URL = http://master:10014/proxy/application_1525741534203_6881/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6881
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-05-29 13:48:46,371 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:48:53,810 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.29 sec
MapReduce Total cumulative CPU time: 3 seconds 290 msec
Ended Job = job_1525741534203_6881
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://master:10000/user/hive/warehouse/scca_etl_test.db/.hive-staging_hive_2018-05-29_13-48-37_976_5097837520849946559-1/-ext-10001
Moving data to: hdfs://master:10000/user/hive/warehouse/scca_etl_test.db/s_eseal_info_etl
Table scca_etl_test.s_eseal_info_etl stats: [numFiles=1, numRows=583, totalSize=59463, rawDataSize=58880]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 3.29 sec   HDFS Read: 2012761 HDFS Write: 59553 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 290 msec
OK
Time taken: 17.356 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 7.544 seconds
Query ID = hadoop_20180529135353_61a00071-a98f-4662-924a-dedc2c353988
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6890, Tracking URL = http://master:10014/proxy/application_1525741534203_6890/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6890
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2018-05-29 13:53:27,013 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:53:34,459 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.4 sec
2018-05-29 13:53:41,932 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.38 sec
MapReduce Total cumulative CPU time: 8 seconds 380 msec
Ended Job = job_1525741534203_6890
Moving data to: hdfs://master:10000/user/hive/warehouse/scca_etl_test.db/sign_info_etl
Table scca_etl_test.sign_info_etl stats: [numFiles=1, numRows=882, totalSize=90395, rawDataSize=89513]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 8.38 sec   HDFS Read: 16823900 HDFS Write: 90482 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 380 msec
OK
Time taken: 26.458 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.535 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.821 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.613 seconds
Query ID = hadoop_20180529135757_b5823f39-3645-46e9-80a6-afd9c4453b9f
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6897, Tracking URL = http://master:10014/proxy/application_1525741534203_6897/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6897
Hadoop job information for Stage-1: number of mappers: 9; number of reducers: 11
2018-05-29 13:57:50,969 Stage-1 map = 0%,  reduce = 0%
2018-05-29 13:58:01,576 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 15.71 sec
2018-05-29 13:58:07,914 Stage-1 map = 28%,  reduce = 0%, Cumulative CPU 33.87 sec
2018-05-29 13:58:08,969 Stage-1 map = 43%,  reduce = 0%, Cumulative CPU 68.1 sec
2018-05-29 13:58:10,023 Stage-1 map = 65%,  reduce = 0%, Cumulative CPU 118.45 sec
2018-05-29 13:58:12,142 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 126.0 sec
2018-05-29 13:58:13,197 Stage-1 map = 82%,  reduce = 0%, Cumulative CPU 135.65 sec
2018-05-29 13:58:14,295 Stage-1 map = 85%,  reduce = 0%, Cumulative CPU 139.66 sec
2018-05-29 13:58:15,352 Stage-1 map = 89%,  reduce = 0%, Cumulative CPU 145.74 sec
2018-05-29 13:58:18,516 Stage-1 map = 96%,  reduce = 0%, Cumulative CPU 162.95 sec
2018-05-29 13:58:22,737 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 167.63 sec
2018-05-29 13:58:30,151 Stage-1 map = 100%,  reduce = 23%, Cumulative CPU 200.67 sec
2018-05-29 13:58:32,268 Stage-1 map = 100%,  reduce = 26%, Cumulative CPU 205.62 sec
2018-05-29 13:58:36,501 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 213.2 sec
2018-05-29 13:58:43,915 Stage-1 map = 100%,  reduce = 36%, Cumulative CPU 213.2 sec
2018-05-29 13:58:46,034 Stage-1 map = 100%,  reduce = 45%, Cumulative CPU 238.95 sec
2018-05-29 13:58:49,208 Stage-1 map = 100%,  reduce = 55%, Cumulative CPU 251.67 sec
2018-05-29 13:58:57,658 Stage-1 map = 100%,  reduce = 64%, Cumulative CPU 264.52 sec
2018-05-29 13:58:58,712 Stage-1 map = 100%,  reduce = 73%, Cumulative CPU 277.6 sec
2018-05-29 13:59:02,941 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 290.11 sec
2018-05-29 13:59:10,341 Stage-1 map = 100%,  reduce = 91%, Cumulative CPU 302.74 sec
2018-05-29 13:59:11,390 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 315.6 sec
MapReduce Total cumulative CPU time: 5 minutes 15 seconds 600 msec
Ended Job = job_1525741534203_6897
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6898, Tracking URL = http://master:10014/proxy/application_1525741534203_6898/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6898
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 5
2018-05-29 13:59:19,489 Stage-2 map = 0%,  reduce = 0%
2018-05-29 13:59:29,037 Stage-2 map = 20%,  reduce = 0%, Cumulative CPU 5.97 sec
2018-05-29 13:59:36,387 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 35.4 sec
2018-05-29 13:59:37,438 Stage-2 map = 38%,  reduce = 0%, Cumulative CPU 67.66 sec
2018-05-29 13:59:47,947 Stage-2 map = 40%,  reduce = 1%, Cumulative CPU 108.91 sec
2018-05-29 13:59:48,996 Stage-2 map = 61%,  reduce = 1%, Cumulative CPU 128.09 sec
2018-05-29 13:59:52,174 Stage-2 map = 65%,  reduce = 1%, Cumulative CPU 131.24 sec
2018-05-29 13:59:53,229 Stage-2 map = 69%,  reduce = 3%, Cumulative CPU 135.58 sec
2018-05-29 13:59:59,620 Stage-2 map = 69%,  reduce = 4%, Cumulative CPU 155.07 sec
2018-05-29 14:00:00,674 Stage-2 map = 72%,  reduce = 4%, Cumulative CPU 168.48 sec
2018-05-29 14:00:07,004 Stage-2 map = 77%,  reduce = 4%, Cumulative CPU 186.64 sec
2018-05-29 14:00:09,166 Stage-2 map = 77%,  reduce = 8%, Cumulative CPU 187.94 sec
2018-05-29 14:00:18,602 Stage-2 map = 82%,  reduce = 8%, Cumulative CPU 219.21 sec
2018-05-29 14:00:24,895 Stage-2 map = 87%,  reduce = 8%, Cumulative CPU 235.58 sec
2018-05-29 14:00:31,174 Stage-2 map = 91%,  reduce = 8%, Cumulative CPU 248.48 sec
2018-05-29 14:00:36,423 Stage-2 map = 94%,  reduce = 8%, Cumulative CPU 254.74 sec
2018-05-29 14:00:37,473 Stage-2 map = 98%,  reduce = 8%, Cumulative CPU 260.85 sec
2018-05-29 14:00:39,567 Stage-2 map = 98%,  reduce = 9%, Cumulative CPU 262.25 sec
2018-05-29 14:00:40,629 Stage-2 map = 100%,  reduce = 9%, Cumulative CPU 265.72 sec
2018-05-29 14:00:41,679 Stage-2 map = 100%,  reduce = 12%, Cumulative CPU 267.79 sec
2018-05-29 14:00:45,866 Stage-2 map = 100%,  reduce = 20%, Cumulative CPU 272.86 sec
2018-05-29 14:00:47,972 Stage-2 map = 100%,  reduce = 26%, Cumulative CPU 281.6 sec
2018-05-29 14:00:52,162 Stage-2 map = 100%,  reduce = 29%, Cumulative CPU 289.96 sec
2018-05-29 14:00:54,252 Stage-2 map = 100%,  reduce = 33%, Cumulative CPU 296.79 sec
2018-05-29 14:00:56,351 Stage-2 map = 100%,  reduce = 49%, Cumulative CPU 311.65 sec
2018-05-29 14:00:57,397 Stage-2 map = 100%,  reduce = 53%, Cumulative CPU 317.84 sec
2018-05-29 14:00:59,489 Stage-2 map = 100%,  reduce = 56%, Cumulative CPU 325.26 sec
2018-05-29 14:01:02,632 Stage-2 map = 100%,  reduce = 60%, Cumulative CPU 331.97 sec
2018-05-29 14:01:15,216 Stage-2 map = 100%,  reduce = 92%, Cumulative CPU 361.71 sec
2018-05-29 14:01:21,487 Stage-2 map = 100%,  reduce = 99%, Cumulative CPU 374.44 sec
2018-05-29 14:01:22,530 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 376.84 sec
MapReduce Total cumulative CPU time: 6 minutes 16 seconds 840 msec
Ended Job = job_1525741534203_6898
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6899, Tracking URL = http://master:10014/proxy/application_1525741534203_6899/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6899
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:01:30,567 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:01:36,849 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.73 sec
2018-05-29 14:01:44,190 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.07 sec
MapReduce Total cumulative CPU time: 4 seconds 70 msec
Ended Job = job_1525741534203_6899
Loading data to table cboard_db.cert_use_cert partition (ymd=2018-05-28)
Partition cboard_db.cert_use_cert{ymd=2018-05-28} stats: [numFiles=1, numRows=101, totalSize=3816, rawDataSize=3715]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 9  Reduce: 11   Cumulative CPU: 315.6 sec   HDFS Read: 2764043329 HDFS Write: 588306682 SUCCESS
Stage-Stage-2: Map: 5  Reduce: 5   Cumulative CPU: 376.84 sec   HDFS Read: 1155796583 HDFS Write: 26583 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 4.07 sec   HDFS Read: 33750 HDFS Write: 3913 SUCCESS
Total MapReduce CPU Time Spent: 11 minutes 36 seconds 510 msec
OK
Time taken: 246.163 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.676 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.913 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.593 seconds
Query ID = hadoop_20180529140202_b8bd3712-8761-4335-ac47-0bdcdea39603
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6900, Tracking URL = http://master:10014/proxy/application_1525741534203_6900/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6900
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 15
2018-05-29 14:02:27,893 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:02:44,795 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 35.62 sec
2018-05-29 14:02:45,850 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 106.62 sec
2018-05-29 14:02:46,905 Stage-1 map = 29%,  reduce = 0%, Cumulative CPU 139.54 sec
2018-05-29 14:02:52,194 Stage-1 map = 41%,  reduce = 0%, Cumulative CPU 183.96 sec
2018-05-29 14:02:53,252 Stage-1 map = 46%,  reduce = 0%, Cumulative CPU 191.31 sec
2018-05-29 14:02:54,307 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 194.05 sec
2018-05-29 14:02:55,363 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 197.68 sec
2018-05-29 14:02:57,473 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 223.14 sec
2018-05-29 14:03:01,690 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 232.59 sec
2018-05-29 14:03:02,750 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 237.09 sec
2018-05-29 14:03:09,093 Stage-1 map = 79%,  reduce = 0%, Cumulative CPU 271.27 sec
2018-05-29 14:03:11,207 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 306.61 sec
2018-05-29 14:03:14,372 Stage-1 map = 91%,  reduce = 0%, Cumulative CPU 313.3 sec
2018-05-29 14:03:16,487 Stage-1 map = 99%,  reduce = 0%, Cumulative CPU 326.22 sec
2018-05-29 14:03:17,551 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 326.74 sec
2018-05-29 14:03:19,677 Stage-1 map = 100%,  reduce = 4%, Cumulative CPU 330.02 sec
2018-05-29 14:03:26,044 Stage-1 map = 100%,  reduce = 12%, Cumulative CPU 350.89 sec
2018-05-29 14:03:27,101 Stage-1 map = 100%,  reduce = 13%, Cumulative CPU 352.56 sec
2018-05-29 14:03:32,379 Stage-1 map = 100%,  reduce = 20%, Cumulative CPU 369.04 sec
2018-05-29 14:03:42,906 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 397.49 sec
2018-05-29 14:03:48,178 Stage-1 map = 100%,  reduce = 40%, Cumulative CPU 412.44 sec
2018-05-29 14:03:57,678 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 441.08 sec
2018-05-29 14:04:03,994 Stage-1 map = 100%,  reduce = 60%, Cumulative CPU 455.19 sec
2018-05-29 14:04:12,419 Stage-1 map = 100%,  reduce = 73%, Cumulative CPU 483.72 sec
2018-05-29 14:04:18,719 Stage-1 map = 100%,  reduce = 80%, Cumulative CPU 497.81 sec
2018-05-29 14:04:28,187 Stage-1 map = 100%,  reduce = 93%, Cumulative CPU 526.33 sec
2018-05-29 14:04:34,479 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 541.54 sec
MapReduce Total cumulative CPU time: 9 minutes 1 seconds 540 msec
Ended Job = job_1525741534203_6900
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6901, Tracking URL = http://master:10014/proxy/application_1525741534203_6901/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6901
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 5
2018-05-29 14:04:42,585 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:04:53,061 Stage-2 map = 17%,  reduce = 0%, Cumulative CPU 7.01 sec
2018-05-29 14:05:00,418 Stage-2 map = 37%,  reduce = 0%, Cumulative CPU 52.99 sec
2018-05-29 14:05:01,468 Stage-2 map = 48%,  reduce = 0%, Cumulative CPU 85.76 sec
2018-05-29 14:05:06,713 Stage-2 map = 51%,  reduce = 0%, Cumulative CPU 104.13 sec
2018-05-29 14:05:11,969 Stage-2 map = 51%,  reduce = 2%, Cumulative CPU 120.35 sec
2018-05-29 14:05:13,012 Stage-2 map = 69%,  reduce = 2%, Cumulative CPU 145.92 sec
2018-05-29 14:05:17,193 Stage-2 map = 76%,  reduce = 2%, Cumulative CPU 154.22 sec
2018-05-29 14:05:23,491 Stage-2 map = 76%,  reduce = 4%, Cumulative CPU 173.21 sec
2018-05-29 14:05:24,540 Stage-2 map = 78%,  reduce = 4%, Cumulative CPU 179.89 sec
2018-05-29 14:05:25,590 Stage-2 map = 79%,  reduce = 4%, Cumulative CPU 186.06 sec
2018-05-29 14:05:30,837 Stage-2 map = 83%,  reduce = 4%, Cumulative CPU 205.81 sec
2018-05-29 14:05:33,990 Stage-2 map = 83%,  reduce = 9%, Cumulative CPU 207.31 sec
2018-05-29 14:05:37,134 Stage-2 map = 86%,  reduce = 9%, Cumulative CPU 219.96 sec
2018-05-29 14:05:48,674 Stage-2 map = 89%,  reduce = 9%, Cumulative CPU 246.94 sec
2018-05-29 14:05:49,722 Stage-2 map = 92%,  reduce = 9%, Cumulative CPU 253.33 sec
2018-05-29 14:05:54,958 Stage-2 map = 97%,  reduce = 9%, Cumulative CPU 265.74 sec
2018-05-29 14:05:58,104 Stage-2 map = 100%,  reduce = 9%, Cumulative CPU 271.59 sec
2018-05-29 14:06:00,194 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 272.9 sec
2018-05-29 14:06:03,335 Stage-2 map = 100%,  reduce = 19%, Cumulative CPU 278.04 sec
2018-05-29 14:06:05,425 Stage-2 map = 100%,  reduce = 26%, Cumulative CPU 285.36 sec
2018-05-29 14:06:09,625 Stage-2 map = 100%,  reduce = 29%, Cumulative CPU 293.98 sec
2018-05-29 14:06:11,727 Stage-2 map = 100%,  reduce = 32%, Cumulative CPU 301.46 sec
2018-05-29 14:06:13,831 Stage-2 map = 100%,  reduce = 48%, Cumulative CPU 316.35 sec
2018-05-29 14:06:15,926 Stage-2 map = 100%,  reduce = 51%, Cumulative CPU 322.55 sec
2018-05-29 14:06:18,029 Stage-2 map = 100%,  reduce = 54%, Cumulative CPU 328.68 sec
2018-05-29 14:06:19,076 Stage-2 map = 100%,  reduce = 56%, Cumulative CPU 334.48 sec
2018-05-29 14:06:20,121 Stage-2 map = 100%,  reduce = 59%, Cumulative CPU 340.77 sec
2018-05-29 14:06:22,212 Stage-2 map = 100%,  reduce = 60%, Cumulative CPU 340.77 sec
2018-05-29 14:06:35,838 Stage-2 map = 100%,  reduce = 91%, Cumulative CPU 374.08 sec
2018-05-29 14:06:42,112 Stage-2 map = 100%,  reduce = 97%, Cumulative CPU 386.68 sec
2018-05-29 14:06:45,245 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 394.84 sec
MapReduce Total cumulative CPU time: 6 minutes 34 seconds 840 msec
Ended Job = job_1525741534203_6901
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6902, Tracking URL = http://master:10014/proxy/application_1525741534203_6902/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6902
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:06:54,265 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:07:00,527 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 3.26 sec
2018-05-29 14:07:08,913 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.6 sec
MapReduce Total cumulative CPU time: 5 seconds 600 msec
Ended Job = job_1525741534203_6902
Loading data to table cboard_db.cert_use_i_xin partition (ymd=2018-05-28)
Partition cboard_db.cert_use_i_xin{ymd=2018-05-28} stats: [numFiles=1, numRows=123, totalSize=3468, rawDataSize=3345]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 12  Reduce: 15   Cumulative CPU: 541.54 sec   HDFS Read: 3653902568 HDFS Write: 632906332 SUCCESS
Stage-Stage-2: Map: 6  Reduce: 5   Cumulative CPU: 394.84 sec   HDFS Read: 1200401758 HDFS Write: 26274 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 5.6 sec   HDFS Read: 33496 HDFS Write: 3566 SUCCESS
Total MapReduce CPU Time Spent: 15 minutes 41 seconds 980 msec
OK
Time taken: 294.299 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.733 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.854 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.667 seconds
Query ID = hadoop_20180529140707_8f1fd779-83e0-483b-b49f-94c12d8b0736
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6903, Tracking URL = http://master:10014/proxy/application_1525741534203_6903/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6903
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 8
2018-05-29 14:07:52,273 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:08:00,792 Stage-1 map = 13%,  reduce = 0%, Cumulative CPU 1.51 sec
2018-05-29 14:08:09,241 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 19.09 sec
2018-05-29 14:08:10,299 Stage-1 map = 76%,  reduce = 0%, Cumulative CPU 100.82 sec
2018-05-29 14:08:11,356 Stage-1 map = 89%,  reduce = 0%, Cumulative CPU 119.67 sec
2018-05-29 14:08:12,415 Stage-1 map = 95%,  reduce = 0%, Cumulative CPU 128.28 sec
2018-05-29 14:08:13,482 Stage-1 map = 99%,  reduce = 0%, Cumulative CPU 131.76 sec
2018-05-29 14:08:14,538 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 133.83 sec
2018-05-29 14:08:21,948 Stage-1 map = 100%,  reduce = 13%, Cumulative CPU 142.32 sec
2018-05-29 14:08:27,246 Stage-1 map = 100%,  reduce = 25%, Cumulative CPU 158.67 sec
2018-05-29 14:08:28,309 Stage-1 map = 100%,  reduce = 35%, Cumulative CPU 173.6 sec
2018-05-29 14:08:34,667 Stage-1 map = 100%,  reduce = 49%, Cumulative CPU 186.55 sec
2018-05-29 14:08:37,825 Stage-1 map = 100%,  reduce = 61%, Cumulative CPU 202.94 sec
2018-05-29 14:08:38,876 Stage-1 map = 100%,  reduce = 63%, Cumulative CPU 208.58 sec
2018-05-29 14:08:48,447 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 231.66 sec
2018-05-29 14:08:54,775 Stage-1 map = 100%,  reduce = 96%, Cumulative CPU 243.6 sec
2018-05-29 14:09:06,383 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 259.66 sec
2018-05-29 14:09:12,683 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 266.23 sec
2018-05-29 14:09:18,999 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 271.64 sec
MapReduce Total cumulative CPU time: 4 minutes 59 seconds 0 msec
Ended Job = job_1525741534203_6903
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6904, Tracking URL = http://master:10014/proxy/application_1525741534203_6904/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6904
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 7
2018-05-29 14:09:50,159 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:10:00,675 Stage-2 map = 14%,  reduce = 0%, Cumulative CPU 7.21 sec
2018-05-29 14:10:08,033 Stage-2 map = 24%,  reduce = 0%, Cumulative CPU 91.45 sec
2018-05-29 14:10:13,287 Stage-2 map = 32%,  reduce = 0%, Cumulative CPU 133.02 sec
2018-05-29 14:10:15,380 Stage-2 map = 38%,  reduce = 0%, Cumulative CPU 160.03 sec
2018-05-29 14:10:19,576 Stage-2 map = 45%,  reduce = 0%, Cumulative CPU 181.39 sec
2018-05-29 14:10:20,629 Stage-2 map = 52%,  reduce = 0%, Cumulative CPU 194.32 sec
2018-05-29 14:10:21,688 Stage-2 map = 56%,  reduce = 0%, Cumulative CPU 200.52 sec
2018-05-29 14:10:23,793 Stage-2 map = 59%,  reduce = 0%, Cumulative CPU 204.45 sec
2018-05-29 14:10:24,843 Stage-2 map = 63%,  reduce = 0%, Cumulative CPU 209.31 sec
2018-05-29 14:10:25,893 Stage-2 map = 72%,  reduce = 0%, Cumulative CPU 233.63 sec
2018-05-29 14:10:26,943 Stage-2 map = 76%,  reduce = 0%, Cumulative CPU 240.03 sec
2018-05-29 14:10:27,994 Stage-2 map = 77%,  reduce = 0%, Cumulative CPU 240.78 sec
2018-05-29 14:10:31,149 Stage-2 map = 81%,  reduce = 0%, Cumulative CPU 247.16 sec
2018-05-29 14:10:32,199 Stage-2 map = 88%,  reduce = 0%, Cumulative CPU 261.25 sec
2018-05-29 14:10:33,263 Stage-2 map = 89%,  reduce = 0%, Cumulative CPU 262.65 sec
2018-05-29 14:10:36,416 Stage-2 map = 92%,  reduce = 0%, Cumulative CPU 267.67 sec
2018-05-29 14:10:40,610 Stage-2 map = 92%,  reduce = 4%, Cumulative CPU 278.34 sec
2018-05-29 14:10:43,761 Stage-2 map = 92%,  reduce = 8%, Cumulative CPU 280.81 sec
2018-05-29 14:10:44,812 Stage-2 map = 97%,  reduce = 8%, Cumulative CPU 287.17 sec
2018-05-29 14:10:50,069 Stage-2 map = 100%,  reduce = 12%, Cumulative CPU 294.64 sec
2018-05-29 14:10:52,171 Stage-2 map = 100%,  reduce = 15%, Cumulative CPU 296.66 sec
2018-05-29 14:10:55,323 Stage-2 map = 100%,  reduce = 20%, Cumulative CPU 302.14 sec
2018-05-29 14:10:56,374 Stage-2 map = 100%,  reduce = 26%, Cumulative CPU 310.46 sec
2018-05-29 14:10:58,477 Stage-2 map = 100%,  reduce = 30%, Cumulative CPU 318.67 sec
2018-05-29 14:11:01,636 Stage-2 map = 100%,  reduce = 33%, Cumulative CPU 326.51 sec
2018-05-29 14:11:02,688 Stage-2 map = 100%,  reduce = 36%, Cumulative CPU 333.28 sec
2018-05-29 14:11:04,780 Stage-2 map = 100%,  reduce = 41%, Cumulative CPU 343.0 sec
2018-05-29 14:11:05,829 Stage-2 map = 100%,  reduce = 43%, Cumulative CPU 347.35 sec
2018-05-29 14:11:21,599 Stage-2 map = 100%,  reduce = 68%, Cumulative CPU 377.15 sec
2018-05-29 14:11:22,651 Stage-2 map = 100%,  reduce = 80%, Cumulative CPU 391.94 sec
2018-05-29 14:11:24,753 Stage-2 map = 100%,  reduce = 84%, Cumulative CPU 399.3 sec
2018-05-29 14:11:26,849 Stage-2 map = 100%,  reduce = 86%, Cumulative CPU 403.6 sec
2018-05-29 14:11:40,456 Stage-2 map = 100%,  reduce = 98%, Cumulative CPU 418.19 sec
2018-05-29 14:11:43,592 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 421.77 sec
MapReduce Total cumulative CPU time: 7 minutes 1 seconds 770 msec
Ended Job = job_1525741534203_6904
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6905, Tracking URL = http://master:10014/proxy/application_1525741534203_6905/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6905
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:11:51,642 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:11:58,968 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 3.21 sec
2018-05-29 14:12:07,381 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.5 sec
MapReduce Total cumulative CPU time: 6 seconds 500 msec
Ended Job = job_1525741534203_6905
Loading data to table cboard_db.cert_use_project partition (ymd=2018-05-28)
Partition cboard_db.cert_use_project{ymd=2018-05-28} stats: [numFiles=1, numRows=1012, totalSize=58436, rawDataSize=57424]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 8   Cumulative CPU: 299.0 sec   HDFS Read: 2043545639 HDFS Write: 1065192708 SUCCESS
Stage-Stage-2: Map: 7  Reduce: 7   Cumulative CPU: 421.77 sec   HDFS Read: 1632708853 HDFS Write: 446033 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 6.5 sec   HDFS Read: 453741 HDFS Write: 58538 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 7 seconds 270 msec
OK
Time taken: 267.997 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.609 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.933 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.648 seconds
Query ID = hadoop_20180529141212_99e365ff-42d9-490e-833f-1c0c378f5ef1
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6906, Tracking URL = http://master:10014/proxy/application_1525741534203_6906/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6906
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 11
2018-05-29 14:12:49,828 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:13:02,546 Stage-1 map = 10%,  reduce = 0%, Cumulative CPU 5.77 sec
2018-05-29 14:13:07,824 Stage-1 map = 23%,  reduce = 0%, Cumulative CPU 56.74 sec
2018-05-29 14:13:08,881 Stage-1 map = 39%,  reduce = 0%, Cumulative CPU 107.83 sec
2018-05-29 14:13:09,938 Stage-1 map = 42%,  reduce = 0%, Cumulative CPU 124.77 sec
2018-05-29 14:13:13,111 Stage-1 map = 51%,  reduce = 0%, Cumulative CPU 136.14 sec
2018-05-29 14:13:14,166 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 160.54 sec
2018-05-29 14:13:15,223 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 162.77 sec
2018-05-29 14:13:18,392 Stage-1 map = 80%,  reduce = 0%, Cumulative CPU 188.08 sec
2018-05-29 14:13:22,620 Stage-1 map = 88%,  reduce = 0%, Cumulative CPU 199.53 sec
2018-05-29 14:13:25,791 Stage-1 map = 90%,  reduce = 0%, Cumulative CPU 203.47 sec
2018-05-29 14:13:28,937 Stage-1 map = 93%,  reduce = 0%, Cumulative CPU 221.45 sec
2018-05-29 14:13:31,050 Stage-1 map = 93%,  reduce = 3%, Cumulative CPU 222.78 sec
2018-05-29 14:13:32,104 Stage-1 map = 93%,  reduce = 5%, Cumulative CPU 223.81 sec
2018-05-29 14:13:41,612 Stage-1 map = 98%,  reduce = 5%, Cumulative CPU 238.78 sec
2018-05-29 14:13:45,845 Stage-1 map = 100%,  reduce = 5%, Cumulative CPU 243.09 sec
2018-05-29 14:13:49,011 Stage-1 map = 100%,  reduce = 9%, Cumulative CPU 246.35 sec
2018-05-29 14:13:50,065 Stage-1 map = 100%,  reduce = 12%, Cumulative CPU 251.83 sec
2018-05-29 14:13:55,344 Stage-1 map = 100%,  reduce = 18%, Cumulative CPU 266.53 sec
2018-05-29 14:14:01,664 Stage-1 map = 100%,  reduce = 27%, Cumulative CPU 281.92 sec
2018-05-29 14:14:11,174 Stage-1 map = 100%,  reduce = 45%, Cumulative CPU 309.37 sec
2018-05-29 14:14:17,510 Stage-1 map = 100%,  reduce = 55%, Cumulative CPU 323.34 sec
2018-05-29 14:14:25,919 Stage-1 map = 100%,  reduce = 73%, Cumulative CPU 350.67 sec
2018-05-29 14:14:32,217 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 365.0 sec
2018-05-29 14:14:40,638 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 391.79 sec
MapReduce Total cumulative CPU time: 6 minutes 31 seconds 790 msec
Ended Job = job_1525741534203_6906
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6907, Tracking URL = http://master:10014/proxy/application_1525741534203_6907/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6907
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 14:14:49,746 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:14:56,058 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.2 sec
2018-05-29 14:15:04,483 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.57 sec
MapReduce Total cumulative CPU time: 5 seconds 570 msec
Ended Job = job_1525741534203_6907
Loading data to table cboard_db.cert_use_type partition (ymd=2018-05-28)
Partition cboard_db.cert_use_type{ymd=2018-05-28} stats: [numFiles=1, numRows=42, totalSize=1300, rawDataSize=1258]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 10  Reduce: 11   Cumulative CPU: 391.79 sec   HDFS Read: 2610975679 HDFS Write: 21223 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 5.57 sec   HDFS Read: 29903 HDFS Write: 1396 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 37 seconds 360 msec
OK
Time taken: 145.99 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.509 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.939 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.743 seconds
Query ID = hadoop_20180529141515_eb9c0304-6108-4f32-a4d2-584f22358a75
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6908, Tracking URL = http://master:10014/proxy/application_1525741534203_6908/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6908
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 3
2018-05-29 14:15:46,234 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:15:53,675 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 1.88 sec
2018-05-29 14:15:55,792 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.85 sec
2018-05-29 14:16:03,205 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 41.73 sec
2018-05-29 14:16:09,546 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 59.39 sec
2018-05-29 14:16:11,676 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 60.54 sec
2018-05-29 14:16:15,913 Stage-1 map = 84%,  reduce = 11%, Cumulative CPU 73.27 sec
2018-05-29 14:16:21,202 Stage-1 map = 92%,  reduce = 11%, Cumulative CPU 78.73 sec
2018-05-29 14:16:22,265 Stage-1 map = 100%,  reduce = 11%, Cumulative CPU 85.65 sec
2018-05-29 14:16:23,325 Stage-1 map = 100%,  reduce = 23%, Cumulative CPU 88.06 sec
2018-05-29 14:16:29,656 Stage-1 map = 100%,  reduce = 53%, Cumulative CPU 103.62 sec
2018-05-29 14:16:33,885 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 113.19 sec
2018-05-29 14:16:38,106 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 127.62 sec
MapReduce Total cumulative CPU time: 2 minutes 7 seconds 620 msec
Ended Job = job_1525741534203_6908
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6909, Tracking URL = http://master:10014/proxy/application_1525741534203_6909/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6909
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 14:16:46,289 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:16:52,642 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec
2018-05-29 14:17:00,003 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.64 sec
MapReduce Total cumulative CPU time: 3 seconds 640 msec
Ended Job = job_1525741534203_6909
Loading data to table cboard_db.evidence_attributes partition (ymd=2018-05-28)
Partition cboard_db.evidence_attributes{ymd=2018-05-28} stats: [numFiles=1, numRows=21, totalSize=957, rawDataSize=936]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 4  Reduce: 3   Cumulative CPU: 127.62 sec   HDFS Read: 572455195 HDFS Write: 2524 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.64 sec   HDFS Read: 9311 HDFS Write: 1058 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 11 seconds 260 msec
OK
Time taken: 85.246 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.724 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.863 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.554 seconds
Query ID = hadoop_20180529141717_5e780be1-acff-44bf-868a-54cbdcca8c7d
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6910, Tracking URL = http://master:10014/proxy/application_1525741534203_6910/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6910
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3
2018-05-29 14:17:43,609 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:17:51,052 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 1.98 sec
2018-05-29 14:17:54,210 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 9.12 sec
2018-05-29 14:17:55,263 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 18.16 sec
2018-05-29 14:17:59,501 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 28.67 sec
2018-05-29 14:18:00,555 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 33.87 sec
MapReduce Total cumulative CPU time: 33 seconds 870 msec
Ended Job = job_1525741534203_6910
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6911, Tracking URL = http://master:10014/proxy/application_1525741534203_6911/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6911
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:18:08,753 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:18:16,127 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 2.43 sec
2018-05-29 14:18:19,290 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 9.68 sec
2018-05-29 14:18:26,683 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 43.49 sec
2018-05-29 14:18:33,006 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 61.61 sec
2018-05-29 14:18:34,063 Stage-2 map = 67%,  reduce = 6%, Cumulative CPU 62.23 sec
2018-05-29 14:18:35,116 Stage-2 map = 67%,  reduce = 17%, Cumulative CPU 63.38 sec
2018-05-29 14:18:38,285 Stage-2 map = 76%,  reduce = 17%, Cumulative CPU 69.78 sec
2018-05-29 14:18:39,341 Stage-2 map = 84%,  reduce = 17%, Cumulative CPU 75.88 sec
2018-05-29 14:18:43,561 Stage-2 map = 92%,  reduce = 17%, Cumulative CPU 81.66 sec
2018-05-29 14:18:44,617 Stage-2 map = 99%,  reduce = 17%, Cumulative CPU 88.11 sec
2018-05-29 14:18:45,674 Stage-2 map = 100%,  reduce = 17%, Cumulative CPU 88.59 sec
2018-05-29 14:18:46,730 Stage-2 map = 100%,  reduce = 19%, Cumulative CPU 89.03 sec
2018-05-29 14:18:47,799 Stage-2 map = 100%,  reduce = 38%, Cumulative CPU 92.77 sec
2018-05-29 14:18:52,528 Stage-2 map = 100%,  reduce = 55%, Cumulative CPU 101.02 sec
2018-05-29 14:18:53,588 Stage-2 map = 100%,  reduce = 81%, Cumulative CPU 116.83 sec
2018-05-29 14:18:56,759 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 130.45 sec
MapReduce Total cumulative CPU time: 2 minutes 10 seconds 450 msec
Ended Job = job_1525741534203_6911
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6912, Tracking URL = http://master:10014/proxy/application_1525741534203_6912/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6912
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:19:05,828 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:19:12,138 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.42 sec
2018-05-29 14:19:19,522 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.46 sec
MapReduce Total cumulative CPU time: 3 seconds 460 msec
Ended Job = job_1525741534203_6912
Loading data to table cboard_db.evidence_cert partition (ymd=2018-05-28)
Partition cboard_db.evidence_cert{ymd=2018-05-28} stats: [numFiles=1, numRows=16, totalSize=476, rawDataSize=460]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 33.87 sec   HDFS Read: 725516711 HDFS Write: 77660 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 130.45 sec   HDFS Read: 567554575 HDFS Write: 1637 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 3.46 sec   HDFS Read: 8302 HDFS Write: 571 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 47 seconds 780 msec
OK
Time taken: 109.258 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.427 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.84 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.771 seconds
Query ID = hadoop_20180529141919_ad12c64b-16d0-40ed-bfb7-af4ce08c268e
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6913, Tracking URL = http://master:10014/proxy/application_1525741534203_6913/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6913
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 3
2018-05-29 14:20:01,472 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:20:08,932 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 2.94 sec
2018-05-29 14:20:11,036 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 8.88 sec
2018-05-29 14:20:18,420 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 24.7 sec
2018-05-29 14:20:24,763 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 59.34 sec
2018-05-29 14:20:26,890 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 60.49 sec
2018-05-29 14:20:27,951 Stage-1 map = 67%,  reduce = 17%, Cumulative CPU 61.1 sec
2018-05-29 14:20:31,137 Stage-1 map = 85%,  reduce = 17%, Cumulative CPU 73.6 sec
2018-05-29 14:20:35,375 Stage-1 map = 92%,  reduce = 17%, Cumulative CPU 78.67 sec
2018-05-29 14:20:36,436 Stage-1 map = 100%,  reduce = 17%, Cumulative CPU 84.88 sec
2018-05-29 14:20:38,545 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 87.66 sec
2018-05-29 14:20:39,603 Stage-1 map = 100%,  reduce = 42%, Cumulative CPU 89.91 sec
2018-05-29 14:20:44,882 Stage-1 map = 100%,  reduce = 70%, Cumulative CPU 105.44 sec
2018-05-29 14:20:45,943 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 113.48 sec
2018-05-29 14:20:48,056 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 117.99 sec
2018-05-29 14:20:49,108 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 126.69 sec
MapReduce Total cumulative CPU time: 2 minutes 6 seconds 690 msec
Ended Job = job_1525741534203_6913
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6914, Tracking URL = http://master:10014/proxy/application_1525741534203_6914/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6914
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 14:20:57,314 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:21:04,677 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.25 sec
2018-05-29 14:21:12,069 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.6 sec
MapReduce Total cumulative CPU time: 5 seconds 600 msec
Ended Job = job_1525741534203_6914
Loading data to table cboard_db.s_eseal_attributes partition (ymd=2018-05-28)
Partition cboard_db.s_eseal_attributes{ymd=2018-05-28} stats: [numFiles=1, numRows=193, totalSize=8539, rawDataSize=8346]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 4  Reduce: 3   Cumulative CPU: 126.69 sec   HDFS Read: 569494685 HDFS Write: 27922 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 5.6 sec   HDFS Read: 35240 HDFS Write: 8641 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 12 seconds 290 msec
OK
Time taken: 82.023 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.804 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.901 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.724 seconds
Query ID = hadoop_20180529142121_94b7703a-2a32-4142-9234-5c400cc5c55c
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6915, Tracking URL = http://master:10014/proxy/application_1525741534203_6915/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6915
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3
2018-05-29 14:21:55,830 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:22:02,303 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 2.8 sec
2018-05-29 14:22:06,537 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 9.96 sec
2018-05-29 14:22:07,597 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 19.07 sec
2018-05-29 14:22:10,787 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 23.91 sec
2018-05-29 14:22:11,851 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 34.04 sec
MapReduce Total cumulative CPU time: 34 seconds 40 msec
Ended Job = job_1525741534203_6915
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6916, Tracking URL = http://master:10014/proxy/application_1525741534203_6916/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6916
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:22:20,055 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:22:27,411 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 3.24 sec
2018-05-29 14:22:29,524 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 10.49 sec
2018-05-29 14:22:37,943 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 44.95 sec
2018-05-29 14:22:46,378 Stage-2 map = 67%,  reduce = 11%, Cumulative CPU 63.69 sec
2018-05-29 14:22:49,542 Stage-2 map = 75%,  reduce = 11%, Cumulative CPU 70.07 sec
2018-05-29 14:22:50,599 Stage-2 map = 84%,  reduce = 11%, Cumulative CPU 76.2 sec
2018-05-29 14:22:54,816 Stage-2 map = 92%,  reduce = 11%, Cumulative CPU 82.0 sec
2018-05-29 14:22:56,928 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 88.64 sec
2018-05-29 14:22:57,984 Stage-2 map = 100%,  reduce = 25%, Cumulative CPU 91.75 sec
2018-05-29 14:23:04,302 Stage-2 map = 100%,  reduce = 54%, Cumulative CPU 107.45 sec
2018-05-29 14:23:08,519 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 116.76 sec
2018-05-29 14:23:10,619 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 131.26 sec
MapReduce Total cumulative CPU time: 2 minutes 11 seconds 260 msec
Ended Job = job_1525741534203_6916
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6917, Tracking URL = http://master:10014/proxy/application_1525741534203_6917/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6917
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:23:18,628 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:23:25,144 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.4 sec
2018-05-29 14:23:32,529 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.67 sec
MapReduce Total cumulative CPU time: 3 seconds 670 msec
Ended Job = job_1525741534203_6917
Loading data to table cboard_db.s_eseal_cert partition (ymd=2018-05-28)
Partition cboard_db.s_eseal_cert{ymd=2018-05-28} stats: [numFiles=1, numRows=69, totalSize=2215, rawDataSize=2146]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 34.04 sec   HDFS Read: 722553927 HDFS Write: 223467 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 131.26 sec   HDFS Read: 567700382 HDFS Write: 7988 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 3.67 sec   HDFS Read: 14645 HDFS Write: 2310 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 48 seconds 970 msec
OK
Time taken: 109.835 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.704 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.95 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.555 seconds
Query ID = hadoop_20180529142424_245fdb3e-6fdf-4688-82b4-15ec4343878d
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6918, Tracking URL = http://master:10014/proxy/application_1525741534203_6918/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6918
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 3
2018-05-29 14:24:14,789 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:24:23,344 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 5.27 sec
2018-05-29 14:24:24,403 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 11.27 sec
2018-05-29 14:24:32,843 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 45.19 sec
2018-05-29 14:24:39,168 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 62.43 sec
2018-05-29 14:24:40,232 Stage-1 map = 67%,  reduce = 6%, Cumulative CPU 63.01 sec
2018-05-29 14:24:41,291 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 63.62 sec
2018-05-29 14:24:44,466 Stage-1 map = 76%,  reduce = 11%, Cumulative CPU 70.05 sec
2018-05-29 14:24:45,526 Stage-1 map = 84%,  reduce = 11%, Cumulative CPU 76.16 sec
2018-05-29 14:24:49,761 Stage-1 map = 92%,  reduce = 11%, Cumulative CPU 81.84 sec
2018-05-29 14:24:50,821 Stage-1 map = 100%,  reduce = 11%, Cumulative CPU 88.29 sec
2018-05-29 14:24:52,932 Stage-1 map = 100%,  reduce = 18%, Cumulative CPU 91.13 sec
2018-05-29 14:24:53,990 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 95.04 sec
2018-05-29 14:24:59,266 Stage-1 map = 100%,  reduce = 45%, Cumulative CPU 102.42 sec
2018-05-29 14:25:00,321 Stage-1 map = 100%,  reduce = 55%, Cumulative CPU 109.86 sec
2018-05-29 14:25:03,482 Stage-1 map = 100%,  reduce = 59%, Cumulative CPU 114.69 sec
2018-05-29 14:25:04,535 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 121.17 sec
2018-05-29 14:25:05,586 Stage-1 map = 100%,  reduce = 98%, Cumulative CPU 135.93 sec
2018-05-29 14:25:06,653 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 137.96 sec
MapReduce Total cumulative CPU time: 2 minutes 17 seconds 960 msec
Ended Job = job_1525741534203_6918
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6919, Tracking URL = http://master:10014/proxy/application_1525741534203_6919/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6919
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 14:25:15,896 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:25:23,392 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.84 sec
2018-05-29 14:25:31,841 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.91 sec
MapReduce Total cumulative CPU time: 9 seconds 910 msec
Ended Job = job_1525741534203_6919
Loading data to table cboard_db.sign_attributes partition (ymd=2018-05-28)
Partition cboard_db.sign_attributes{ymd=2018-05-28} stats: [numFiles=1, numRows=33252, totalSize=2383994, rawDataSize=2350742]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 4  Reduce: 3   Cumulative CPU: 137.96 sec   HDFS Read: 584246810 HDFS Write: 4757460 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 9.91 sec   HDFS Read: 4764828 HDFS Write: 2384098 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 27 seconds 870 msec
OK
Time taken: 89.129 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.613 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.848 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.521 seconds
Query ID = hadoop_20180529142626_2030c8bd-07a0-4119-a2d8-402462b5ee16
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6920, Tracking URL = http://master:10014/proxy/application_1525741534203_6920/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6920
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2018-05-29 14:26:15,660 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:26:22,114 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.63 sec
2018-05-29 14:26:23,173 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.64 sec
2018-05-29 14:26:30,582 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.03 sec
MapReduce Total cumulative CPU time: 12 seconds 30 msec
Ended Job = job_1525741534203_6920
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6921, Tracking URL = http://master:10014/proxy/application_1525741534203_6921/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6921
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:26:38,811 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:26:47,211 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 6.42 sec
2018-05-29 14:26:48,269 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 12.23 sec
2018-05-29 14:26:56,651 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 46.24 sec
2018-05-29 14:27:02,955 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 63.97 sec
2018-05-29 14:27:06,124 Stage-2 map = 67%,  reduce = 11%, Cumulative CPU 65.12 sec
2018-05-29 14:27:08,236 Stage-2 map = 76%,  reduce = 11%, Cumulative CPU 71.72 sec
2018-05-29 14:27:09,291 Stage-2 map = 84%,  reduce = 11%, Cumulative CPU 77.83 sec
2018-05-29 14:27:13,519 Stage-2 map = 92%,  reduce = 11%, Cumulative CPU 83.51 sec
2018-05-29 14:27:14,578 Stage-2 map = 99%,  reduce = 11%, Cumulative CPU 89.96 sec
2018-05-29 14:27:15,632 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 90.58 sec
2018-05-29 14:27:17,740 Stage-2 map = 100%,  reduce = 23%, Cumulative CPU 93.19 sec
2018-05-29 14:27:18,794 Stage-2 map = 100%,  reduce = 40%, Cumulative CPU 97.2 sec
2018-05-29 14:27:24,081 Stage-2 map = 100%,  reduce = 51%, Cumulative CPU 105.21 sec
2018-05-29 14:27:25,133 Stage-2 map = 100%,  reduce = 59%, Cumulative CPU 113.24 sec
2018-05-29 14:27:26,189 Stage-2 map = 100%,  reduce = 62%, Cumulative CPU 115.66 sec
2018-05-29 14:27:27,241 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 119.57 sec
2018-05-29 14:27:29,353 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 134.36 sec
MapReduce Total cumulative CPU time: 2 minutes 14 seconds 360 msec
Ended Job = job_1525741534203_6921
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6922, Tracking URL = http://master:10014/proxy/application_1525741534203_6922/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6922
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:27:38,378 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:27:44,690 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
2018-05-29 14:27:51,012 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.7 sec
MapReduce Total cumulative CPU time: 3 seconds 700 msec
Ended Job = job_1525741534203_6922
Loading data to table cboard_db.sign_project partition (ymd=2018-05-28)
Partition cboard_db.sign_project{ymd=2018-05-28} stats: [numFiles=1, numRows=80, totalSize=3801, rawDataSize=3721]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 12.03 sec   HDFS Read: 16817864 HDFS Write: 5045125 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 134.36 sec   HDFS Read: 572521568 HDFS Write: 13507 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 3.7 sec   HDFS Read: 20179 HDFS Write: 3896 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 30 seconds 90 msec
OK
Time taken: 109.646 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.458 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.955 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.571 seconds
Query ID = hadoop_20180529142828_2fbfd917-28e6-47d5-ad84-dd0bd095bb64
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6923, Tracking URL = http://master:10014/proxy/application_1525741534203_6923/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6923
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 5
2018-05-29 14:28:35,558 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:28:44,072 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 5.65 sec
2018-05-29 14:28:46,189 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 20.43 sec
2018-05-29 14:28:53,593 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 36.73 sec
2018-05-29 14:28:57,837 Stage-1 map = 100%,  reduce = 60%, Cumulative CPU 62.46 sec
2018-05-29 14:29:06,324 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 78.17 sec
MapReduce Total cumulative CPU time: 1 minutes 18 seconds 170 msec
Ended Job = job_1525741534203_6923
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6924, Tracking URL = http://master:10014/proxy/application_1525741534203_6924/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6924
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:29:15,015 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:29:24,459 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 5.84 sec
2018-05-29 14:29:32,872 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 56.24 sec
2018-05-29 14:29:33,923 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 56.55 sec
2018-05-29 14:29:39,176 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 74.26 sec
2018-05-29 14:29:42,345 Stage-2 map = 67%,  reduce = 11%, Cumulative CPU 75.57 sec
2018-05-29 14:29:44,451 Stage-2 map = 75%,  reduce = 11%, Cumulative CPU 81.69 sec
2018-05-29 14:29:45,506 Stage-2 map = 84%,  reduce = 11%, Cumulative CPU 88.09 sec
2018-05-29 14:29:50,782 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 100.2 sec
2018-05-29 14:29:53,942 Stage-2 map = 100%,  reduce = 26%, Cumulative CPU 103.9 sec
2018-05-29 14:29:54,998 Stage-2 map = 100%,  reduce = 43%, Cumulative CPU 107.75 sec
2018-05-29 14:30:00,259 Stage-2 map = 100%,  reduce = 58%, Cumulative CPU 124.85 sec
2018-05-29 14:30:03,410 Stage-2 map = 100%,  reduce = 62%, Cumulative CPU 128.24 sec
2018-05-29 14:30:04,459 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 132.26 sec
2018-05-29 14:30:06,578 Stage-2 map = 100%,  reduce = 98%, Cumulative CPU 146.94 sec
2018-05-29 14:30:07,624 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 148.93 sec
MapReduce Total cumulative CPU time: 2 minutes 28 seconds 930 msec
Ended Job = job_1525741534203_6924
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6925, Tracking URL = http://master:10014/proxy/application_1525741534203_6925/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6925
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:30:15,608 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:30:22,971 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 3.92 sec
2018-05-29 14:30:30,331 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.44 sec
MapReduce Total cumulative CPU time: 6 seconds 440 msec
Ended Job = job_1525741534203_6925
Loading data to table cboard_db.work_deal_cert partition (ymd=2018-05-28)
Partition cboard_db.work_deal_cert{ymd=2018-05-28} stats: [numFiles=1, numRows=553, totalSize=19762, rawDataSize=19209]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 4  Reduce: 5   Cumulative CPU: 78.17 sec   HDFS Read: 1047465721 HDFS Write: 50694134 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 148.93 sec   HDFS Read: 618171551 HDFS Write: 82630 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 6.44 sec   HDFS Read: 89303 HDFS Write: 19861 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 53 seconds 540 msec
OK
Time taken: 128.041 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.534 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.872 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.453 seconds
Query ID = hadoop_20180529143131_4a3dd4cf-fd83-4421-93b4-98380e991f28
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6926, Tracking URL = http://master:10014/proxy/application_1525741534203_6926/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6926
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3
2018-05-29 14:31:14,203 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:31:21,628 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 4.08 sec
2018-05-29 14:31:27,960 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 14.84 sec
2018-05-29 14:31:31,120 Stage-1 map = 96%,  reduce = 0%, Cumulative CPU 31.7 sec
2018-05-29 14:31:32,172 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 32.6 sec
2018-05-29 14:31:37,459 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 41.57 sec
2018-05-29 14:31:38,515 Stage-1 map = 100%,  reduce = 95%, Cumulative CPU 61.84 sec
2018-05-29 14:31:40,632 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 63.47 sec
MapReduce Total cumulative CPU time: 1 minutes 3 seconds 470 msec
Ended Job = job_1525741534203_6926
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6927, Tracking URL = http://master:10014/proxy/application_1525741534203_6927/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6927
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:31:48,827 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:31:58,275 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 7.42 sec
2018-05-29 14:32:06,729 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 56.74 sec
2018-05-29 14:32:09,892 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 60.51 sec
2018-05-29 14:32:12,004 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 77.09 sec
2018-05-29 14:32:16,227 Stage-2 map = 67%,  reduce = 6%, Cumulative CPU 77.72 sec
2018-05-29 14:32:17,284 Stage-2 map = 67%,  reduce = 11%, Cumulative CPU 78.44 sec
2018-05-29 14:32:18,341 Stage-2 map = 85%,  reduce = 11%, Cumulative CPU 91.13 sec
2018-05-29 14:32:22,576 Stage-2 map = 92%,  reduce = 11%, Cumulative CPU 95.78 sec
2018-05-29 14:32:24,692 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 101.99 sec
2018-05-29 14:32:26,804 Stage-2 map = 100%,  reduce = 27%, Cumulative CPU 104.87 sec
2018-05-29 14:32:27,858 Stage-2 map = 100%,  reduce = 40%, Cumulative CPU 108.38 sec
2018-05-29 14:32:28,921 Stage-2 map = 100%,  reduce = 57%, Cumulative CPU 113.76 sec
2018-05-29 14:32:33,138 Stage-2 map = 100%,  reduce = 68%, Cumulative CPU 122.07 sec
2018-05-29 14:32:34,194 Stage-2 map = 100%,  reduce = 76%, Cumulative CPU 130.66 sec
2018-05-29 14:32:35,250 Stage-2 map = 100%,  reduce = 84%, Cumulative CPU 139.11 sec
2018-05-29 14:32:38,411 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 152.78 sec
2018-05-29 14:32:39,462 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 159.95 sec
MapReduce Total cumulative CPU time: 2 minutes 39 seconds 950 msec
Ended Job = job_1525741534203_6927
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6928, Tracking URL = http://master:10014/proxy/application_1525741534203_6928/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6928
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:32:47,482 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:33:01,119 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 9.93 sec
2018-05-29 14:33:12,715 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 18.47 sec
MapReduce Total cumulative CPU time: 18 seconds 470 msec
Ended Job = job_1525741534203_6928
Loading data to table cboard_db.work_deal_company partition (ymd=2018-05-28)
Partition cboard_db.work_deal_company{ymd=2018-05-28} stats: [numFiles=1, numRows=462819, totalSize=34585348, rawDataSize=34122529]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 63.47 sec   HDFS Read: 596699196 HDFS Write: 86204441 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 159.95 sec   HDFS Read: 653683480 HDFS Write: 47614240 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 18.47 sec   HDFS Read: 47621569 HDFS Write: 34585456 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 1 seconds 890 msec
OK
Time taken: 131.792 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.616 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.864 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.573 seconds
Query ID = hadoop_20180529143333_ea83a801-e35d-428c-a4ae-c69b2c22cb4c
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6929, Tracking URL = http://master:10014/proxy/application_1525741534203_6929/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6929
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 2
2018-05-29 14:33:56,521 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:34:02,929 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 1.58 sec
2018-05-29 14:34:03,988 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 5.38 sec
2018-05-29 14:34:09,282 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.66 sec
2018-05-29 14:34:12,474 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 20.85 sec
2018-05-29 14:34:18,847 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.85 sec
MapReduce Total cumulative CPU time: 34 seconds 640 msec
Ended Job = job_1525741534203_6929
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6930, Tracking URL = http://master:10014/proxy/application_1525741534203_6930/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6930
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:34:28,182 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:34:38,713 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 5.91 sec
2018-05-29 14:34:46,114 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 53.99 sec
2018-05-29 14:34:47,168 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 55.54 sec
2018-05-29 14:34:51,387 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 72.62 sec
2018-05-29 14:34:56,665 Stage-2 map = 67%,  reduce = 6%, Cumulative CPU 73.28 sec
2018-05-29 14:34:57,722 Stage-2 map = 85%,  reduce = 6%, Cumulative CPU 85.87 sec
2018-05-29 14:35:01,956 Stage-2 map = 92%,  reduce = 6%, Cumulative CPU 90.61 sec
2018-05-29 14:35:04,072 Stage-2 map = 100%,  reduce = 14%, Cumulative CPU 98.66 sec
2018-05-29 14:35:08,310 Stage-2 map = 100%,  reduce = 31%, Cumulative CPU 103.47 sec
2018-05-29 14:35:10,429 Stage-2 map = 100%,  reduce = 45%, Cumulative CPU 109.94 sec
2018-05-29 14:35:14,649 Stage-2 map = 100%,  reduce = 52%, Cumulative CPU 118.38 sec
2018-05-29 14:35:16,768 Stage-2 map = 100%,  reduce = 61%, Cumulative CPU 125.27 sec
2018-05-29 14:35:17,823 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 130.36 sec
2018-05-29 14:35:20,986 Stage-2 map = 100%,  reduce = 98%, Cumulative CPU 145.47 sec
2018-05-29 14:35:22,039 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 147.08 sec
MapReduce Total cumulative CPU time: 2 minutes 27 seconds 80 msec
Ended Job = job_1525741534203_6930
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6931, Tracking URL = http://master:10014/proxy/application_1525741534203_6931/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6931
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:35:30,108 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:35:37,464 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.8 sec
2018-05-29 14:35:44,842 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.33 sec
MapReduce Total cumulative CPU time: 5 seconds 330 msec
Ended Job = job_1525741534203_6931
Loading data to table cboard_db.work_deal_office partition (ymd=2018-05-28)
Partition cboard_db.work_deal_office{ymd=2018-05-28} stats: [numFiles=1, numRows=928, totalSize=28960, rawDataSize=28032]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 2   Cumulative CPU: 34.64 sec   HDFS Read: 326939200 HDFS Write: 51599559 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 147.08 sec   HDFS Read: 619076283 HDFS Write: 118498 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 5.33 sec   HDFS Read: 125217 HDFS Write: 29061 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 7 seconds 50 msec
OK
Time taken: 121.464 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.591 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.98 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.311 seconds
Query ID = hadoop_20180529143636_3feb84c4-c80b-45dd-97b3-20fca7561594
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6932, Tracking URL = http://master:10014/proxy/application_1525741534203_6932/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6932
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 2
2018-05-29 14:36:27,058 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:36:33,476 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 1.62 sec
2018-05-29 14:36:34,531 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 5.58 sec
2018-05-29 14:36:42,958 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 19.69 sec
2018-05-29 14:36:49,267 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 29.57 sec
2018-05-29 14:36:51,369 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 42.1 sec
MapReduce Total cumulative CPU time: 42 seconds 100 msec
Ended Job = job_1525741534203_6932
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6933, Tracking URL = http://master:10014/proxy/application_1525741534203_6933/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6933
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 3
2018-05-29 14:36:59,552 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:37:10,066 Stage-2 map = 25%,  reduce = 0%, Cumulative CPU 7.49 sec
2018-05-29 14:37:17,458 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 57.4 sec
2018-05-29 14:37:20,621 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 60.3 sec
2018-05-29 14:37:23,792 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 77.39 sec
2018-05-29 14:37:28,008 Stage-2 map = 67%,  reduce = 11%, Cumulative CPU 78.73 sec
2018-05-29 14:37:29,069 Stage-2 map = 86%,  reduce = 11%, Cumulative CPU 91.31 sec
2018-05-29 14:37:33,306 Stage-2 map = 92%,  reduce = 11%, Cumulative CPU 95.61 sec
2018-05-29 14:37:35,425 Stage-2 map = 100%,  reduce = 11%, Cumulative CPU 102.05 sec
2018-05-29 14:37:36,477 Stage-2 map = 100%,  reduce = 23%, Cumulative CPU 103.94 sec
2018-05-29 14:37:39,657 Stage-2 map = 100%,  reduce = 39%, Cumulative CPU 109.3 sec
2018-05-29 14:37:40,715 Stage-2 map = 100%,  reduce = 56%, Cumulative CPU 114.05 sec
2018-05-29 14:37:42,830 Stage-2 map = 100%,  reduce = 69%, Cumulative CPU 122.17 sec
2018-05-29 14:37:45,991 Stage-2 map = 100%,  reduce = 85%, Cumulative CPU 138.63 sec
2018-05-29 14:37:48,104 Stage-2 map = 100%,  reduce = 88%, Cumulative CPU 141.48 sec
2018-05-29 14:37:49,159 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 152.03 sec
MapReduce Total cumulative CPU time: 2 minutes 32 seconds 30 msec
Ended Job = job_1525741534203_6933
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6934, Tracking URL = http://master:10014/proxy/application_1525741534203_6934/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6934
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2018-05-29 14:37:57,192 Stage-3 map = 0%,  reduce = 0%
2018-05-29 14:38:05,604 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 5.04 sec
2018-05-29 14:38:12,993 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 8.88 sec
MapReduce Total cumulative CPU time: 8 seconds 880 msec
Ended Job = job_1525741534203_6934
Loading data to table cboard_db.work_deal_project partition (ymd=2018-05-28)
Partition cboard_db.work_deal_project{ymd=2018-05-28} stats: [numFiles=1, numRows=3521, totalSize=196502, rawDataSize=192981]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 2   Cumulative CPU: 42.1 sec   HDFS Read: 326970071 HDFS Write: 81974037 SUCCESS
Stage-Stage-2: Map: 4  Reduce: 3   Cumulative CPU: 152.03 sec   HDFS Read: 649450731 HDFS Write: 647977 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 8.88 sec   HDFS Read: 654691 HDFS Write: 196606 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 23 seconds 10 msec
OK
Time taken: 118.04 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.67 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.836 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.487 seconds
Query ID = hadoop_20180529143838_e19ea9c1-f72d-4c85-a382-018b693de23b
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6935, Tracking URL = http://master:10014/proxy/application_1525741534203_6935/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6935
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 4
2018-05-29 14:38:54,602 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:39:02,025 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 3.87 sec
2018-05-29 14:39:04,142 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 11.22 sec
2018-05-29 14:39:09,443 Stage-1 map = 60%,  reduce = 0%, Cumulative CPU 22.6 sec
2018-05-29 14:39:11,554 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 38.71 sec
2018-05-29 14:39:12,609 Stage-1 map = 73%,  reduce = 0%, Cumulative CPU 56.51 sec
2018-05-29 14:39:20,002 Stage-1 map = 73%,  reduce = 5%, Cumulative CPU 75.34 sec
2018-05-29 14:39:21,061 Stage-1 map = 73%,  reduce = 10%, Cumulative CPU 76.02 sec
2018-05-29 14:39:23,182 Stage-1 map = 81%,  reduce = 10%, Cumulative CPU 82.42 sec
2018-05-29 14:39:24,240 Stage-1 map = 88%,  reduce = 10%, Cumulative CPU 88.75 sec
2018-05-29 14:39:25,299 Stage-1 map = 88%,  reduce = 15%, Cumulative CPU 89.43 sec
2018-05-29 14:39:28,473 Stage-1 map = 94%,  reduce = 15%, Cumulative CPU 95.05 sec
2018-05-29 14:39:29,534 Stage-1 map = 100%,  reduce = 15%, Cumulative CPU 100.37 sec
2018-05-29 14:39:31,644 Stage-1 map = 100%,  reduce = 21%, Cumulative CPU 102.02 sec
2018-05-29 14:39:32,698 Stage-1 map = 100%,  reduce = 44%, Cumulative CPU 108.79 sec
2018-05-29 14:39:36,918 Stage-1 map = 100%,  reduce = 54%, Cumulative CPU 117.35 sec
2018-05-29 14:39:37,974 Stage-1 map = 100%,  reduce = 61%, Cumulative CPU 125.69 sec
2018-05-29 14:39:39,029 Stage-1 map = 100%,  reduce = 68%, Cumulative CPU 133.82 sec
2018-05-29 14:39:40,085 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 140.24 sec
2018-05-29 14:39:54,892 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 153.95 sec
MapReduce Total cumulative CPU time: 2 minutes 33 seconds 950 msec
Ended Job = job_1525741534203_6935
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6936, Tracking URL = http://master:10014/proxy/application_1525741534203_6936/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6936
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 14:40:03,070 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:40:11,481 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.03 sec
2018-05-29 14:40:17,811 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.58 sec
MapReduce Total cumulative CPU time: 6 seconds 580 msec
Ended Job = job_1525741534203_6936
Loading data to table cboard_db.work_deal_status partition (ymd=2018-05-28)
Partition cboard_db.work_deal_status{ymd=2018-05-28} stats: [numFiles=1, numRows=572, totalSize=18577, rawDataSize=18005]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 5  Reduce: 4   Cumulative CPU: 153.95 sec   HDFS Read: 894400000 HDFS Write: 94113 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 6.58 sec   HDFS Read: 101085 HDFS Write: 18678 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 40 seconds 530 msec
OK
Time taken: 94.629 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.72 seconds
Dropped the partition ymd=2018-05-28
OK
Time taken: 0.903 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.

Logging initialized using configuration in file:/usr/hadoop/hive/conf/hive-log4j.properties
OK
Time taken: 6.948 seconds
Query ID = hadoop_20180529144040_06c524ed-c692-4789-a559-42e1f3ddfb1d
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6937, Tracking URL = http://master:10014/proxy/application_1525741534203_6937/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6937
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 4
2018-05-29 14:40:59,183 Stage-1 map = 0%,  reduce = 0%
2018-05-29 14:41:09,749 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 12.97 sec
2018-05-29 14:41:17,121 Stage-1 map = 59%,  reduce = 0%, Cumulative CPU 64.32 sec
2018-05-29 14:41:23,436 Stage-1 map = 61%,  reduce = 0%, Cumulative CPU 91.3 sec
2018-05-29 14:41:27,667 Stage-1 map = 61%,  reduce = 7%, Cumulative CPU 92.51 sec
2018-05-29 14:41:28,727 Stage-1 map = 68%,  reduce = 7%, Cumulative CPU 98.92 sec
2018-05-29 14:41:29,783 Stage-1 map = 80%,  reduce = 7%, Cumulative CPU 110.99 sec
2018-05-29 14:41:34,008 Stage-1 map = 87%,  reduce = 7%, Cumulative CPU 117.32 sec
2018-05-29 14:41:35,062 Stage-1 map = 96%,  reduce = 7%, Cumulative CPU 130.15 sec
2018-05-29 14:41:36,115 Stage-1 map = 97%,  reduce = 7%, Cumulative CPU 130.71 sec
2018-05-29 14:41:39,276 Stage-1 map = 100%,  reduce = 13%, Cumulative CPU 135.52 sec
2018-05-29 14:41:45,590 Stage-1 map = 100%,  reduce = 35%, Cumulative CPU 154.7 sec
2018-05-29 14:41:50,836 Stage-1 map = 100%,  reduce = 57%, Cumulative CPU 169.64 sec
2018-05-29 14:41:51,884 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 183.41 sec
2018-05-29 14:41:52,939 Stage-1 map = 100%,  reduce = 70%, Cumulative CPU 184.61 sec
2018-05-29 14:41:53,991 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 191.64 sec
2018-05-29 14:42:08,762 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 207.57 sec
2018-05-29 14:42:11,908 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 210.85 sec
MapReduce Total cumulative CPU time: 3 minutes 30 seconds 850 msec
Ended Job = job_1525741534203_6937
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1525741534203_6938, Tracking URL = http://master:10014/proxy/application_1525741534203_6938/
Kill Command = /usr/hadoop/hadoop/bin/hadoop job  -kill job_1525741534203_6938
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2018-05-29 14:42:20,176 Stage-2 map = 0%,  reduce = 0%
2018-05-29 14:42:27,547 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.81 sec
2018-05-29 14:42:35,967 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.73 sec
MapReduce Total cumulative CPU time: 5 seconds 730 msec
Ended Job = job_1525741534203_6938
Loading data to table cboard_db.work_deal_type partition (ymd=2018-05-28)
Partition cboard_db.work_deal_type{ymd=2018-05-28} stats: [numFiles=1, numRows=1004, totalSize=31974, rawDataSize=30970]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 5  Reduce: 4   Cumulative CPU: 210.85 sec   HDFS Read: 894421298 HDFS Write: 178338 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 5.73 sec   HDFS Read: 185287 HDFS Write: 32074 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 36 seconds 580 msec
OK
Time taken: 107.201 seconds
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
Warning: /usr/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
18/05/30 00:00:15 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.12.0
18/05/30 00:00:15 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/05/30 00:00:17 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
18/05/30 00:00:17 INFO tool.CodeGenTool: Beginning code generation
18/05/30 00:00:19 INFO manager.SqlManager: Executing SQL statement: select * from sign_project where  (1 = 0) 
18/05/30 00:00:19 INFO manager.SqlManager: Executing SQL statement: select * from sign_project where  (1 = 0) 
18/05/30 00:00:19 INFO manager.SqlManager: Executing SQL statement: select * from sign_project where  (1 = 0) 
18/05/30 00:00:19 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hadoop/hadoop
Note: /tmp/sqoop-hadoop/compile/9c6a7df335cc12412e33671a93c9e095/QueryResult.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/05/30 00:00:37 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9c6a7df335cc12412e33671a93c9e095/QueryResult.jar
18/05/30 00:00:40 WARN ipc.Client: Failed to connect to server: master/172.18.110.21:10000: try once and fail.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:648)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:744)
	at org.apache.hadoop.ipc.Client$Connection.access$3000(Client.java:396)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1555)
	at org.apache.hadoop.ipc.Client.call(Client.java:1478)
	at org.apache.hadoop.ipc.Client.call(Client.java:1439)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:260)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2126)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1262)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1258)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1258)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1418)
	at org.apache.sqoop.tool.ImportTool.deleteTargetDir(ImportTool.java:545)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:509)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:621)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
18/05/30 00:00:40 ERROR tool.ImportTool: Import failed: java.net.ConnectException: Call From master1/172.18.110.31 to master:10000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1506)
	at org.apache.hadoop.ipc.Client.call(Client.java:1439)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:260)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2126)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1262)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1258)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1258)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1418)
	at org.apache.sqoop.tool.ImportTool.deleteTargetDir(ImportTool.java:545)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:509)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:621)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:648)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:744)
	at org.apache.hadoop.ipc.Client$Connection.access$3000(Client.java:396)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1555)
	at org.apache.hadoop.ipc.Client.call(Client.java:1478)
	... 26 more

